#!/usr/bin/env python3
"""
HVDC Ontology Mapper v2.6 - DataFrame â†’ RDF ë³€í™˜

mapping_rules_v2.6.json ê¸°ë°˜ìœ¼ë¡œ DataFrameì„ RDFë¡œ ë³€í™˜í•˜ëŠ” ìµœì‹  ì‹¤ì „ ì˜ˆì œ
"""

import pandas as pd
from rdflib import Graph, Namespace, Literal, RDF, RDFS, XSD
import json
from pathlib import Path
import logging
from datetime import datetime

# ğŸ†• NEW: mapping_utilsì—ì„œ ìƒˆë¡œìš´ í•¨ìˆ˜ë“¤ import
from mapping_utils import normalize_code_num, codes_match, is_valid_hvdc_vendor, is_warehouse_code

logger = logging.getLogger(__name__)

# ìµœì‹  mapping_rules ë¶ˆëŸ¬ì˜¤ê¸°
try:
    with open('mapping_rules_v2.6.json', encoding='utf-8') as f:
        RULES = json.load(f)
    NS = {k: Namespace(v) for k, v in RULES["namespaces"].items()}
    FIELD_MAP = RULES["field_map"]
    PROPERTY_MAPPINGS = RULES["property_mappings"]
    CLASS_MAPPINGS = RULES["class_mappings"]
    # ğŸ†• NEW: ìƒˆë¡œìš´ ì„¤ì •ë“¤ ë¡œë“œ
    HVDC_CODE3_VALID = RULES.get('hvdc_code3_valid', ['HE', 'SIM'])
    WAREHOUSE_CODES = RULES.get('warehouse_codes', ['DSV Outdoor', 'DSV Indoor', 'DSV Al Markaz', 'DSV MZP'])
    MONTH_MATCHING = RULES.get('month_matching', 'operation_month_eq_eta_month')
except Exception as e:
    logger.warning(f"mapping_rules_v2.6.json ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {e}")
    NS = {
        "ex": Namespace("http://samsung.com/project-logistics#"),
        "rdf": Namespace("http://www.w3.org/1999/02/22-rdf-syntax-ns#"),
        "rdfs": Namespace("http://www.w3.org/2000/01/rdf-schema#"),
        "xsd": Namespace("http://www.w3.org/2001/XMLSchema#")
    }
    FIELD_MAP = {}
    PROPERTY_MAPPINGS = {}
    CLASS_MAPPINGS = {}
    HVDC_CODE3_VALID = ['HE', 'SIM']
    WAREHOUSE_CODES = ['DSV Outdoor', 'DSV Indoor', 'DSV Al Markaz', 'DSV MZP']
    MONTH_MATCHING = 'operation_month_eq_eta_month'

def apply_hvdc_filters_to_rdf(df: pd.DataFrame) -> pd.DataFrame:
    """
    ğŸ†• NEW: RDF ë³€í™˜ ì „ HVDC í•„í„° ì ìš©
    
    Args:
        df: ì›ë³¸ DataFrame
        
    Returns:
        pd.DataFrame: í•„í„°ë§ëœ DataFrame
    """
    print("ğŸ”§ RDF ë³€í™˜ ì „ HVDC í•„í„° ì ìš© ì¤‘...")
    
    # A. HVDC CODE ì •ê·œí™” ì ìš©
    if 'HVDC CODE' in df.columns and 'HVDC CODE 4' in df.columns:
        df['HVDC_CODE_NORMALIZED'] = df['HVDC CODE'].apply(normalize_code_num)
        df['HVDC_CODE4_NORMALIZED'] = df['HVDC CODE 4'].apply(normalize_code_num)
        
        # ì½”ë“œ ë§¤ì¹­ ê²€ì¦
        df['CODE_MATCH'] = df.apply(
            lambda row: codes_match(row['HVDC CODE'], row['HVDC CODE 4']), axis=1
        )
        
        # ë§¤ì¹­ë˜ì§€ ì•ŠëŠ” í–‰ í•„í„°ë§
        original_count = len(df)
        df = df[df['CODE_MATCH'] == True]
        filtered_count = len(df)
        print(f"  âœ… HVDC CODE ë§¤ì¹­: {original_count} â†’ {filtered_count} (í•„í„°ë§: {original_count - filtered_count}ê±´)")
    
    # B. CODE 3 í•„í„° (HE, SIMë§Œ ì²˜ë¦¬)
    if 'HVDC CODE 3' in df.columns:
        original_count = len(df)
        df = df[df['HVDC CODE 3'].apply(lambda x: is_valid_hvdc_vendor(x, HVDC_CODE3_VALID))]
        filtered_count = len(df)
        print(f"  âœ… ë²¤ë” í•„í„° (HE/SIM): {original_count} â†’ {filtered_count} (í•„í„°ë§: {original_count - filtered_count}ê±´)")
    
    # C. ì°½ê³ ëª…(ì„ëŒ€ë£Œ) í•„í„° & SQM ì ìš©
    if 'HVDC CODE' in df.columns:
        warehouse_mask = df['HVDC CODE'].apply(lambda x: is_warehouse_code(x, WAREHOUSE_CODES))
        warehouse_df = df[warehouse_mask].copy()
        
        if 'SQM' in warehouse_df.columns:
            warehouse_df['SQM'] = warehouse_df['SQM'].apply(lambda x: float(x) if pd.notna(x) else 0)
            print(f"  âœ… ì°½ê³  ì„ëŒ€ë£Œ ì§‘ê³„: {len(warehouse_df)}ê±´ (SQM í¬í•¨)")
    
    # D. Operation Month(ì›”) ë§¤ì¹­
    if 'Operation Month' in df.columns and 'ETA' in df.columns:
        # INVOICE ë°ì´í„°: invoice_month
        # WAREHOUSE ë°ì´í„°: warehouse_month (ETA)
        df['INVOICE_MONTH'] = pd.to_datetime(df['Operation Month'], errors='coerce').dt.strftime('%Y-%m')
        df['WAREHOUSE_MONTH'] = pd.to_datetime(df['ETA'], errors='coerce').dt.strftime('%Y-%m')
        
        original_count = len(df)
        df = df[df['INVOICE_MONTH'] == df['WAREHOUSE_MONTH']]
        filtered_count = len(df)
        print(f"  âœ… ì›” ë§¤ì¹­: {original_count} â†’ {filtered_count} (í•„í„°ë§: {original_count - filtered_count}ê±´)")
    
    # E. Handling IN/OUT í•„ë“œ ì§‘ê³„
    handling_fields = ['Handling In freight ton', 'Handling out Freight Ton']
    for field in handling_fields:
        if field in df.columns:
            df[field] = df[field].apply(lambda x: float(x) if pd.notna(x) else 0)
            print(f"  âœ… {field} ì²˜ë¦¬ ì™„ë£Œ")
    
    return df

def dataframe_to_rdf(df: pd.DataFrame, output_path="rdf_output/output.ttl"):
    """
    DataFrameì„ RDFë¡œ ë³€í™˜ (mapping_rules ê¸°ë°˜ + ğŸ†• NEW: HVDC í•„í„° ì ìš©)
    
    Args:
        df: ë³€í™˜í•  DataFrame
        output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
        
    Returns:
        str: ìƒì„±ëœ RDF íŒŒì¼ ê²½ë¡œ
    """
    print(f"ğŸ”— DataFrameì„ RDFë¡œ ë³€í™˜ ì¤‘: {output_path}")
    
    # ğŸ†• NEW: HVDC í•„í„° ì ìš©
    df = apply_hvdc_filters_to_rdf(df)
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir = Path(output_path).parent
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # RDF ê·¸ë˜í”„ ìƒì„±
    g = Graph()
    
    # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë°”ì¸ë”©
    for prefix, ns in NS.items():
        g.bind(prefix, ns)
    
    # ê° í–‰ì„ RDF íŠ¸ë¦¬í”Œë¡œ ë³€í™˜
    for idx, row in df.iterrows():
        # TransportEvent URI ìƒì„±
        event_uri = NS["ex"][f"TransportEvent_{idx+1:05d}"]
        g.add((event_uri, RDF.type, NS["ex"].TransportEvent))
        
        # ê° ì»¬ëŸ¼ì„ RDF í”„ë¡œí¼í‹°ë¡œ ë³€í™˜
        for col, val in row.items():
            if pd.isna(val) or col not in FIELD_MAP:
                continue
                
            prop = NS["ex"][FIELD_MAP[col]]
            
            # ğŸ†• NEW: property_mappingsì—ì„œ ë°ì´í„° íƒ€ì… í™•ì¸
            datatype = PROPERTY_MAPPINGS.get(col, {}).get('datatype', XSD.decimal)
            
            # ë°ì´í„° íƒ€ì…ì— ë”°ë¥¸ Literal ìƒì„±
            if isinstance(val, (int, float)):
                lit = Literal(val, datatype=datatype)
            elif isinstance(val, str):
                # ë‚ ì§œ ë¬¸ìì—´ì¸ì§€ í™•ì¸
                try:
                    date_val = pd.to_datetime(val)
                    lit = Literal(date_val.date(), datatype=XSD.date)
                except Exception:
                    lit = Literal(str(val))
            else:
                lit = Literal(str(val))
                
            g.add((event_uri, prop, lit))
    
    # RDF íŒŒì¼ ì €ì¥
    g.serialize(destination=output_path, format="turtle")
    print(f"âœ… RDF ë³€í™˜ ì™„ë£Œ: {output_path}")
    
    return output_path

def create_enhanced_rdf(df: pd.DataFrame, output_path="rdf_output/enhanced_output.ttl"):
    """
    í–¥ìƒëœ RDF ë³€í™˜ (ì¶”ê°€ ë©”íƒ€ë°ì´í„° í¬í•¨ + ğŸ†• NEW: HVDC í•„í„° ì ìš©)
    
    Args:
        df: ë³€í™˜í•  DataFrame
        output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
        
    Returns:
        str: ìƒì„±ëœ RDF íŒŒì¼ ê²½ë¡œ
    """
    print(f"ğŸ”— í–¥ìƒëœ RDF ë³€í™˜ ì¤‘: {output_path}")
    
    # ğŸ†• NEW: HVDC í•„í„° ì ìš©
    df = apply_hvdc_filters_to_rdf(df)
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir = Path(output_path).parent
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # RDF ê·¸ë˜í”„ ìƒì„±
    g = Graph()
    
    # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë°”ì¸ë”©
    for prefix, ns in NS.items():
        g.bind(prefix, ns)
    
    # ë°ì´í„°ì…‹ ë©”íƒ€ë°ì´í„° ì¶”ê°€
    dataset_uri = NS["ex"]["Dataset_001"]
    g.add((dataset_uri, RDF.type, NS["ex"].Dataset))
    g.add((dataset_uri, NS["ex"].hasCreationDate, Literal(datetime.now().date(), datatype=XSD.date)))
    g.add((dataset_uri, NS["ex"].hasRecordCount, Literal(len(df), datatype=XSD.integer)))
    
    # ê° í–‰ì„ RDF íŠ¸ë¦¬í”Œë¡œ ë³€í™˜
    for idx, row in df.iterrows():
        # TransportEvent URI ìƒì„±
        event_uri = NS["ex"][f"TransportEvent_{idx+1:05d}"]
        g.add((event_uri, RDF.type, NS["ex"].TransportEvent))
        g.add((event_uri, NS["ex"].belongsToDataset, dataset_uri))
        
        # ê° ì»¬ëŸ¼ì„ RDF í”„ë¡œí¼í‹°ë¡œ ë³€í™˜
        for col, val in row.items():
            if pd.isna(val) or col not in FIELD_MAP:
                continue
                
            prop = NS["ex"][FIELD_MAP[col]]
            
            # ğŸ†• NEW: property_mappingsì—ì„œ ë°ì´í„° íƒ€ì… í™•ì¸
            datatype = PROPERTY_MAPPINGS.get(col, {}).get('datatype', XSD.decimal)
            
            # ë°ì´í„° íƒ€ì…ì— ë”°ë¥¸ Literal ìƒì„±
            if isinstance(val, (int, float)):
                lit = Literal(val, datatype=datatype)
            elif isinstance(val, str):
                # ë‚ ì§œ ë¬¸ìì—´ì¸ì§€ í™•ì¸
                try:
                    date_val = pd.to_datetime(val)
                    lit = Literal(date_val.date(), datatype=XSD.date)
                except Exception:
                    lit = Literal(str(val))
            else:
                lit = Literal(str(val))
                
            g.add((event_uri, prop, lit))
    
    # RDF íŒŒì¼ ì €ì¥
    g.serialize(destination=output_path, format="turtle")
    print(f"âœ… í–¥ìƒëœ RDF ë³€í™˜ ì™„ë£Œ: {output_path}")
    
    return output_path

def generate_sparql_queries(output_dir="sparql_queries"):
    """
    mapping_rules ê¸°ë°˜ SPARQL ì¿¼ë¦¬ ìƒì„±
    
    Args:
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        
    Returns:
        str: ìƒì„±ëœ SPARQL íŒŒì¼ ê²½ë¡œ
    """
    print(f"ğŸ” SPARQL ì¿¼ë¦¬ ìƒì„± ì¤‘: {output_dir}")
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # ê¸°ë³¸ ì¿¼ë¦¬ í…œí”Œë¦¿ë“¤
    queries = [
        {
            'name': 'monthly_warehouse_summary',
            'description': 'ì›”ë³„ ì°½ê³ ë³„ ì§‘ê³„',
            'query': f"""
PREFIX ex: <{NS["ex"]}>
SELECT ?month ?warehouse (SUM(?amount) AS ?totalAmount) (SUM(?qty) AS ?totalQty)
WHERE {{
    ?event rdf:type ex:TransportEvent ;
           ex:hasLocation ?warehouse ;
           ex:hasDate ?date ;
           ex:hasAmount ?amount ;
           ex:hasQuantity ?qty .
    BIND(SUBSTR(STR(?date), 1, 7) AS ?month)
}}
GROUP BY ?month ?warehouse
ORDER BY ?month ?warehouse
"""
        },
        {
            'name': 'vendor_analysis',
            'description': 'ë²¤ë”ë³„ ë¶„ì„',
            'query': f"""
PREFIX ex: <{NS["ex"]}>
SELECT ?vendor (SUM(?amount) AS ?totalAmount) (COUNT(?event) AS ?eventCount)
WHERE {{
    ?event rdf:type ex:TransportEvent ;
           ex:hasVendor ?vendor ;
           ex:hasAmount ?amount .
}}
GROUP BY ?vendor
ORDER BY DESC(?totalAmount)
"""
        },
        {
            'name': 'container_summary',
            'description': 'ì»¨í…Œì´ë„ˆ ìš”ì•½',
            'query': f"""
PREFIX ex: <{NS["ex"]}>
SELECT ?warehouse (SUM(?container20) AS ?total20FT) (SUM(?container40) AS ?total40FT)
WHERE {{
    ?event rdf:type ex:TransportEvent ;
           ex:hasLocation ?warehouse ;
           ex:has20FTContainer ?container20 ;
           ex:has40FTContainer ?container40 .
}}
GROUP BY ?warehouse
ORDER BY ?warehouse
"""
        }
    ]
    
    # mapping_rules ê¸°ë°˜ ë™ì  ì¿¼ë¦¬ ìƒì„±
    numeric_fields = [field for field, props in PROPERTY_MAPPINGS.items() 
                     if props.get('datatype') in ['xsd:decimal', 'xsd:integer']]
    
    # Handling Fee íŠ¹ë³„ ì¿¼ë¦¬
    if 'Handling Fee' in numeric_fields:
        handling_fee_query = {
            'name': 'handling_fee_monthly_warehouse',
            'description': 'ì›”ë³„ ì°½ê³ ë³„ Handling Fee ì§‘ê³„',
            'query': f"""
PREFIX ex: <{NS["ex"]}>
SELECT ?month ?warehouse (SUM(?handlingFee) AS ?totalHandlingFee)
WHERE {{
    ?event rdf:type ex:TransportEvent ;
           ex:hasLocation ?warehouse ;
           ex:hasDate ?date ;
           ex:hasHandlingFee ?handlingFee .
    BIND(SUBSTR(STR(?date), 1, 7) AS ?month)
}}
GROUP BY ?month ?warehouse
ORDER BY ?month ?warehouse
"""
        }
        queries.append(handling_fee_query)
    
    # ì¿¼ë¦¬ íŒŒì¼ ì €ì¥
    sparql_file = f"{output_dir}/generated_queries_{timestamp}.sparql"
    with open(sparql_file, 'w', encoding='utf-8') as f:
        for query_info in queries:
            f.write(f"# {query_info['description']}\n")
            f.write(f"# Query: {query_info['name']}\n")
            f.write(query_info['query'])
            f.write("\n\n")
    
    print(f"âœ… SPARQL ì¿¼ë¦¬ ìƒì„± ì™„ë£Œ: {sparql_file}")
    return sparql_file

def validate_rdf_conversion(df: pd.DataFrame) -> dict:
    """
    RDF ë³€í™˜ ê²€ì¦
    
    Args:
        df: ê²€ì¦í•  DataFrame
        
    Returns:
        dict: ê²€ì¦ ê²°ê³¼
    """
    validation_result = {
        'total_records': len(df),
        'mappable_fields': 0,
        'unmappable_fields': [],
        'missing_mappings': []
    }
    
    # ë§¤í•‘ ê°€ëŠ¥í•œ í•„ë“œ í™•ì¸
    for col in df.columns:
        if col in FIELD_MAP:
            validation_result['mappable_fields'] += 1
        else:
            validation_result['unmappable_fields'].append(col)
    
    # mapping_rulesì— ì •ì˜ëœ í•„ë“œê°€ DataFrameì— ì—†ëŠ”ì§€ í™•ì¸
    for field in FIELD_MAP.keys():
        if field not in df.columns:
            validation_result['missing_mappings'].append(field)
    
    return validation_result

def create_rdf_schema(output_path="rdf_output/schema.ttl"):
    """
    RDF ìŠ¤í‚¤ë§ˆ ìƒì„± (mapping_rules ê¸°ë°˜)
    
    Args:
        output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
        
    Returns:
        str: ìƒì„±ëœ ìŠ¤í‚¤ë§ˆ íŒŒì¼ ê²½ë¡œ
    """
    print(f"ğŸ“‹ RDF ìŠ¤í‚¤ë§ˆ ìƒì„± ì¤‘: {output_path}")
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir = Path(output_path).parent
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # RDF ê·¸ë˜í”„ ìƒì„±
    g = Graph()
    
    # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë°”ì¸ë”©
    for prefix, ns in NS.items():
        g.bind(prefix, ns)
    
    # í´ë˜ìŠ¤ ì •ì˜
    for class_name, class_uri in CLASS_MAPPINGS.items():
        class_ns = NS["ex"][class_uri]
        g.add((class_ns, RDF.type, RDFS.Class))
        g.add((class_ns, RDFS.label, Literal(class_name)))
    
    # í”„ë¡œí¼í‹° ì •ì˜
    for field_name, predicate in FIELD_MAP.items():
        prop_ns = NS["ex"][predicate]
        g.add((prop_ns, RDF.type, RDF.Property))
        g.add((prop_ns, RDFS.label, Literal(field_name)))
        
        # ë°ì´í„° íƒ€ì… ì •ë³´ ì¶”ê°€
        if field_name in PROPERTY_MAPPINGS:
            datatype = PROPERTY_MAPPINGS[field_name].get('datatype', 'xsd:string')
            g.add((prop_ns, RDFS.range, NS["xsd"][datatype.split(':')[-1]]))
    
    # ìŠ¤í‚¤ë§ˆ íŒŒì¼ ì €ì¥
    g.serialize(destination=output_path, format="turtle")
    print(f"âœ… RDF ìŠ¤í‚¤ë§ˆ ìƒì„± ì™„ë£Œ: {output_path}")
    
    return output_path

# í¸ì˜ í•¨ìˆ˜ë“¤
def quick_rdf_convert(df: pd.DataFrame, output_dir="rdf_output"):
    """
    ë¹ ë¥¸ RDF ë³€í™˜ (ê¸°ë³¸ ì„¤ì •)
    
    Args:
        df: ë³€í™˜í•  DataFrame
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        
    Returns:
        tuple: (rdf_path, sparql_path, schema_path)
    """
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # RDF ë³€í™˜
    rdf_path = f"{output_dir}/hvdc_data_{timestamp}.ttl"
    dataframe_to_rdf(df, rdf_path)
    
    # SPARQL ì¿¼ë¦¬ ìƒì„±
    sparql_path = generate_sparql_queries(output_dir)
    
    # ìŠ¤í‚¤ë§ˆ ìƒì„±
    schema_path = f"{output_dir}/schema_{timestamp}.ttl"
    create_rdf_schema(schema_path)
    
    return rdf_path, sparql_path, schema_path

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë°ì´í„°
    test_data = {
        'Case_No': ['CASE001', 'CASE002'],
        'Date': ['2024-01-01', '2024-01-02'],
        'Location': ['DSV Indoor', 'DSV Outdoor'],
        'Qty': [100, 200],
        'Amount': [1000.0, 2000.0],
        'Handling Fee': [50.0, 100.0]
    }
    
    df = pd.DataFrame(test_data)
    
    # RDF ë³€í™˜ í…ŒìŠ¤íŠ¸
    rdf_path, sparql_path, schema_path = quick_rdf_convert(df)
    
    print(f"âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ:")
    print(f"  RDF: {rdf_path}")
    print(f"  SPARQL: {sparql_path}")
    print(f"  Schema: {schema_path}") 