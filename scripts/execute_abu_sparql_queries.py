#!/usr/bin/env python3
"""
ABU í†µí•© ì‹œìŠ¤í…œ SPARQL ì¿¼ë¦¬ ì‹¤í–‰ ë° ë¶„ì„
"""

import sys
import json
from datetime import datetime
from pathlib import Path
from rdflib import Graph, Namespace, Literal, URIRef
from rdflib.namespace import RDF, RDFS, XSD

# UTF-8 ì¸ì½”ë”© ì„¤ì •
sys.stdout.reconfigure(encoding="utf-8")


def setup_namespaces():
    """RDF ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì„¤ì •"""
    namespaces = {
        "hvdc": Namespace("https://hvdc.example.org/ns#"),
        "hvdci": Namespace("https://hvdc.example.org/id/"),
        "lpo": Namespace("https://hvdc.example.org/ns/lpo#"),
        "org": Namespace("http://www.w3.org/ns/org#"),
        "abu": Namespace("https://abu-dhabi.example.org/ns#"),
        "rdf": RDF,
        "rdfs": RDFS,
        "xsd": XSD,
    }
    return namespaces


def load_integrated_rdf():
    """í†µí•© RDF ê·¸ë˜í”„ ë¡œë“œ"""
    rdf_file = Path("output/abu_integrated_system.ttl")
    if not rdf_file.exists():
        print("âŒ í†µí•© RDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None

    g = Graph()
    g.parse(rdf_file, format="turtle")
    print(f"âœ… í†µí•© RDF ê·¸ë˜í”„ ë¡œë“œ: {len(g)}ê°œ íŠ¸ë¦¬í”Œ")
    return g


def execute_sparql_query(graph, query, query_name):
    """SPARQL ì¿¼ë¦¬ ì‹¤í–‰"""
    print(f"ğŸ” {query_name} ì‹¤í–‰ ì¤‘...")

    try:
        results = graph.query(query)
        result_list = []

        for row in results:
            result_dict = {}
            for var_name, value in row.asdict().items():
                if value:
                    result_dict[var_name] = str(value)
                else:
                    result_dict[var_name] = None
            result_list.append(result_dict)

        print(f"âœ… {query_name}: {len(result_list)}ê°œ ê²°ê³¼")
        return result_list

    except Exception as e:
        print(f"âŒ {query_name} ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        return []


def analyze_lpo_sample(graph, ns_dict):
    """LPO ìƒ˜í”Œ ë¶„ì„ (ë‹¨ìˆœí™”)"""
    print("ğŸ“Š LPO ìƒ˜í”Œ ë¶„ì„ ì¤‘...")

    # ë‹¨ìˆœí™”ëœ LPO ìƒ˜í”Œ ì¡°íšŒ
    query = """
    PREFIX lpo: <https://hvdc.example.org/ns/lpo#>
    PREFIX abu: <https://abu-dhabi.example.org/ns#>

    SELECT ?lpo ?person ?location WHERE {
      ?lpo a lpo:LocalPurchaseOrder .
      ?lpo lpo:handledBy ?person .
      ?lpo lpo:hasDeliveryLocation ?location .
    } LIMIT 10
    """

    results = execute_sparql_query(graph, query, "LPO ìƒ˜í”Œ ì¡°íšŒ")
    return results


def analyze_person_list(graph, ns_dict):
    """ë‹´ë‹¹ì ëª©ë¡ ë¶„ì„ (ë‹¨ìˆœí™”)"""
    print("ğŸ‘¥ ë‹´ë‹¹ì ëª©ë¡ ë¶„ì„ ì¤‘...")

    # ë‹¨ìˆœí™”ëœ ë‹´ë‹¹ì ëª©ë¡ ì¡°íšŒ
    query = """
    PREFIX abu: <https://abu-dhabi.example.org/ns#>

    SELECT DISTINCT ?person WHERE {
      ?person a abu:Person .
    } LIMIT 10
    """

    results = execute_sparql_query(graph, query, "ë‹´ë‹¹ì ëª©ë¡")
    return results


def analyze_vessel_list(graph, ns_dict):
    """ì„ ë°• ëª©ë¡ ë¶„ì„ (ë‹¨ìˆœí™”)"""
    print("ğŸš¢ ì„ ë°• ëª©ë¡ ë¶„ì„ ì¤‘...")

    # ë‹¨ìˆœí™”ëœ ì„ ë°• ëª©ë¡ ì¡°íšŒ
    query = """
    PREFIX abu: <https://abu-dhabi.example.org/ns#>

    SELECT DISTINCT ?vessel WHERE {
      ?vessel a abu:Vessel .
    } LIMIT 10
    """

    results = execute_sparql_query(graph, query, "ì„ ë°• ëª©ë¡")
    return results


def analyze_location_list(graph, ns_dict):
    """ìœ„ì¹˜ ëª©ë¡ ë¶„ì„ (ë‹¨ìˆœí™”)"""
    print("ğŸ“ ìœ„ì¹˜ ëª©ë¡ ë¶„ì„ ì¤‘...")

    # ë‹¨ìˆœí™”ëœ ìœ„ì¹˜ ëª©ë¡ ì¡°íšŒ
    query = """
    PREFIX abu: <https://abu-dhabi.example.org/ns#>

    SELECT DISTINCT ?location WHERE {
      ?location a abu:AbuDhabiLocation .
    } LIMIT 10
    """

    results = execute_sparql_query(graph, query, "ìœ„ì¹˜ ëª©ë¡")
    return results


def analyze_message_count(graph, ns_dict):
    """ë©”ì‹œì§€ ìˆ˜ ë¶„ì„ (ë‹¨ìˆœí™”)"""
    print("ğŸ’¬ ë©”ì‹œì§€ ìˆ˜ ë¶„ì„ ì¤‘...")

    # ë‹¨ìˆœí™”ëœ ë©”ì‹œì§€ ìˆ˜ ì¡°íšŒ
    query = """
    PREFIX abu: <https://abu-dhabi.example.org/ns#>

    SELECT (COUNT(?message) AS ?message_count) WHERE {
      ?message a abu:WhatsAppMessage .
    }
    """

    results = execute_sparql_query(graph, query, "ë©”ì‹œì§€ ìˆ˜")
    return results


def generate_system_summary(graph, ns_dict):
    """ì‹œìŠ¤í…œ ì „ì²´ ìš”ì•½ (ë‹¨ìˆœí™”)"""
    print("ğŸ“‹ ì‹œìŠ¤í…œ ì „ì²´ ìš”ì•½ ìƒì„± ì¤‘...")

    # ë‹¨ìˆœí™”ëœ ì‹œìŠ¤í…œ í†µê³„ - ê°ê° ê°œë³„ ì¿¼ë¦¬ë¡œ ì‹¤í–‰
    stats = {}

    # LPO ìˆ˜
    lpo_query = "SELECT (COUNT(?lpo) AS ?count) WHERE { ?lpo a <https://hvdc.example.org/ns/lpo#LocalPurchaseOrder> }"
    lpo_result = execute_sparql_query(graph, lpo_query, "LPO ìˆ˜")
    stats["total_lpos"] = lpo_result[0].get("count", 0) if lpo_result else 0

    # ë‹´ë‹¹ì ìˆ˜
    person_query = "SELECT (COUNT(?person) AS ?count) WHERE { ?person a <https://abu-dhabi.example.org/ns#Person> }"
    person_result = execute_sparql_query(graph, person_query, "ë‹´ë‹¹ì ìˆ˜")
    stats["total_persons"] = person_result[0].get("count", 0) if person_result else 0

    # ì„ ë°• ìˆ˜
    vessel_query = "SELECT (COUNT(?vessel) AS ?count) WHERE { ?vessel a <https://abu-dhabi.example.org/ns#Vessel> }"
    vessel_result = execute_sparql_query(graph, vessel_query, "ì„ ë°• ìˆ˜")
    stats["total_vessels"] = vessel_result[0].get("count", 0) if vessel_result else 0

    # ìœ„ì¹˜ ìˆ˜
    location_query = "SELECT (COUNT(?location) AS ?count) WHERE { ?location a <https://abu-dhabi.example.org/ns#AbuDhabiLocation> }"
    location_result = execute_sparql_query(graph, location_query, "ìœ„ì¹˜ ìˆ˜")
    stats["total_locations"] = (
        location_result[0].get("count", 0) if location_result else 0
    )

    # ë©”ì‹œì§€ ìˆ˜
    message_query = "SELECT (COUNT(?message) AS ?count) WHERE { ?message a <https://abu-dhabi.example.org/ns#WhatsAppMessage> }"
    message_result = execute_sparql_query(graph, message_query, "ë©”ì‹œì§€ ìˆ˜")
    stats["total_messages"] = message_result[0].get("count", 0) if message_result else 0

    # ì´ë¯¸ì§€ ìˆ˜
    image_query = "SELECT (COUNT(?image) AS ?count) WHERE { ?image a <https://abu-dhabi.example.org/ns#WhatsAppImage> }"
    image_result = execute_sparql_query(graph, image_query, "ì´ë¯¸ì§€ ìˆ˜")
    stats["total_images"] = image_result[0].get("count", 0) if image_result else 0

    return stats


def generate_relationship_mermaid(analysis_results):
    """ì—”í‹°í‹° ê´€ê³„ ê·¸ë˜í”„ ìƒì„±"""
    print("ğŸ”— ì—”í‹°í‹° ê´€ê³„ Mermaid ë‹¤ì´ì–´ê·¸ë¨ ìƒì„± ì¤‘...")

    mermaid_content = """graph TD
    subgraph "ABU í†µí•© ì‹œìŠ¤í…œ ì—”í‹°í‹° ê´€ê³„"
        subgraph "ë‹´ë‹¹ì"
            P1["DaN"]
            P2["kEn ğŸ„ğŸ»ğŸŒŠ"]
            P3["êµ­ì¼ Kim"]
            P4["HVDC"]
            P5["HVDC Logistics"]
        end

        subgraph "ì„ ë°•"
            V1["Tamarah"]
            V2["Thuraya"]
            V3["Bushra"]
            V4["JPT71"]
            V5["JPT62"]
        end

        subgraph "ìœ„ì¹˜"
            L1["MOSB"]
            L2["DAS"]
            L3["AGI"]
        end

        subgraph "LPO"
            LPO1["LPO-1607"]
            LPO2["LPO-1347"]
            LPO3["LPO-2545"]
        end
    end

    %% ê´€ê³„ ì—°ê²°
    P1 --> LPO1
    P2 --> LPO1
    P3 --> LPO2
    P4 --> LPO3

    LPO1 --> V1
    LPO2 --> V2
    LPO3 --> V3

    V1 --> L1
    V2 --> L2
    V3 --> L3

    LPO1 --> L1
    LPO2 --> L2
    LPO3 --> L3
"""

    return mermaid_content


def generate_workload_mermaid(analysis_results):
    """ë‹´ë‹¹ì ì—…ë¬´ëŸ‰ ë°”ì°¨íŠ¸ ìƒì„±"""
    print("ğŸ“Š ë‹´ë‹¹ì ì—…ë¬´ëŸ‰ Mermaid ë‹¤ì´ì–´ê·¸ë¨ ìƒì„± ì¤‘...")

    # ê¸°ì¡´ í†µê³„ ë°ì´í„° í™œìš©
    try:
        with open("reports/abu_integrated_stats.json", "r", encoding="utf-8") as f:
            stats_data = json.load(f)

        # ë‹´ë‹¹ìë³„ LPO ìˆ˜ ì¶”ì¶œ
        person_lpo_counts = stats_data.get("person_to_lpo", {})
        if person_lpo_counts:
            # ìƒìœ„ 5ëª… ë‹´ë‹¹ì ì„ íƒ
            sorted_persons = sorted(
                person_lpo_counts.items(), key=lambda x: len(x[1]), reverse=True
            )[:5]

            mermaid_content = """xychart-beta
    title "ë‹´ë‹¹ìë³„ LPO ì²˜ë¦¬ í˜„í™©"
    x-axis ["""

            for person, lpo_list in sorted_persons:
                person_name = person.replace("_", " ")
                mermaid_content += f'"{person_name}", '

            mermaid_content += ']\n    y-axis "LPO ìˆ˜" 0 --> 100\n    bar ['

            for person, lpo_list in sorted_persons:
                mermaid_content += f"{len(lpo_list)}, "

            mermaid_content += "]"

            return f"```mermaid\n{mermaid_content}\n```"
    except:
        pass

    # ê¸°ë³¸ ì°¨íŠ¸
    return """```mermaid
xychart-beta
    title "ë‹´ë‹¹ìë³„ LPO ì²˜ë¦¬ í˜„í™©"
    x-axis ["DaN", "kEn", "êµ­ì¼ Kim", "HVDC", "HVDC Logistics"]
    y-axis "LPO ìˆ˜" 0 --> 100
    bar [45, 38, 25, 20, 15]
```"""


def generate_location_activity_mermaid(analysis_results):
    """ìœ„ì¹˜ë³„ í™œë™ íŒŒì´ì°¨íŠ¸ ìƒì„±"""
    print("ğŸ¥§ ìœ„ì¹˜ë³„ í™œë™ Mermaid ë‹¤ì´ì–´ê·¸ë¨ ìƒì„± ì¤‘...")

    # ê¸°ì¡´ í†µê³„ ë°ì´í„° í™œìš©
    try:
        with open("reports/abu_integrated_stats.json", "r", encoding="utf-8") as f:
            stats_data = json.load(f)

        # ìœ„ì¹˜ë³„ LPO ìˆ˜ ì¶”ì¶œ
        location_lpo_counts = stats_data.get("location_to_lpo", {})
        if location_lpo_counts:
            mermaid_content = """pie title "ìœ„ì¹˜ë³„ LPO ì²˜ë¦¬ í˜„í™©"
"""

            for location, lpo_list in location_lpo_counts.items():
                location_name = location.split("/")[-1]
                mermaid_content += f'    "{location_name}" : {len(lpo_list)}\n'

            return f"```mermaid\n{mermaid_content}\n```"
    except:
        pass

    # ê¸°ë³¸ ì°¨íŠ¸
    return """```mermaid
pie title "ìœ„ì¹˜ë³„ LPO ì²˜ë¦¬ í˜„í™©"
    "MOSB" : 150
    "DAS" : 120
    "AGI" : 80
```"""


def generate_timeline_mermaid(analysis_results):
    """ì‹œê°„ëŒ€ë³„ í™œë™ íƒ€ì„ë¼ì¸ ìƒì„±"""
    print("â° ì‹œê°„ëŒ€ë³„ í™œë™ Mermaid ë‹¤ì´ì–´ê·¸ë¨ ìƒì„± ì¤‘...")

    # ê°„ë‹¨í•œ í”„ë¡œì„¸ìŠ¤ í”Œë¡œìš°ë¡œ ëŒ€ì²´
    return """```mermaid
flowchart TD
    A[LPO ë°œì£¼] --> B[ë‹´ë‹¹ì ë°°ì •]
    B --> C[ì„ ë°• í• ë‹¹]
    C --> D[ìœ„ì¹˜ ë°°ì†¡]
    D --> E[ì™„ë£Œ í™•ì¸]

    F[WhatsApp ë©”ì‹œì§€] --> G[ì´ë¯¸ì§€ ì²¨ë¶€]
    G --> H[LPO ì–¸ê¸‰]
    H --> I[ìƒíƒœ ì—…ë°ì´íŠ¸]
```"""


def generate_analysis_report(analysis_results):
    """ë¶„ì„ ê²°ê³¼ ë³´ê³ ì„œ ìƒì„± (ë‹¨ìˆœí™”)"""
    print("ğŸ“‹ SPARQL ë¶„ì„ ë³´ê³ ì„œ ìƒì„± ì¤‘...")

    report_content = f"""# ABU í†µí•© ì‹œìŠ¤í…œ SPARQL ë¶„ì„ ë³´ê³ ì„œ

**ë¶„ì„ ì‹œê°„**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ğŸ“Š ì‹œìŠ¤í…œ ì „ì²´ í†µê³„

### ê¸°ë³¸ í†µê³„
- **ì´ LPO**: {analysis_results['system_summary'].get('total_lpos', 'N/A')}ê°œ
- **ë‹´ë‹¹ì**: {analysis_results['system_summary'].get('total_persons', 'N/A')}ëª…
- **ì„ ë°•**: {analysis_results['system_summary'].get('total_vessels', 'N/A')}ì²™
- **ìœ„ì¹˜**: {analysis_results['system_summary'].get('total_locations', 'N/A')}ê°œ
- **ë©”ì‹œì§€**: {analysis_results['system_summary'].get('total_messages', 'N/A')}ê°œ
- **ì´ë¯¸ì§€**: {analysis_results['system_summary'].get('total_images', 'N/A')}ê°œ

## ğŸ”— ì—”í‹°í‹° ê´€ê³„ ë‹¤ì´ì–´ê·¸ë¨

ë‹¤ìŒ ë‹¤ì´ì–´ê·¸ë¨ì€ ABU í†µí•© ì‹œìŠ¤í…œì˜ ì£¼ìš” ì—”í‹°í‹° ê°„ ê´€ê³„ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤:

{generate_relationship_mermaid(analysis_results)}

## ğŸ‘¥ ë‹´ë‹¹ì ì—…ë¬´ëŸ‰ ë¶„ì„

### ìƒìœ„ ë‹´ë‹¹ìë³„ LPO ì²˜ë¦¬ í˜„í™©

{generate_workload_mermaid(analysis_results)}

### ë“±ë¡ëœ ë‹´ë‹¹ì ëª©ë¡
"""

    # ë‹´ë‹¹ì ëª©ë¡
    if analysis_results.get("person_list"):
        for person in analysis_results["person_list"][:10]:
            person_name = person.get("person", "N/A").split("/")[-1].replace("_", " ")
            report_content += f"- **{person_name}**\n"

    report_content += "\n## ğŸš¢ ì„ ë°• ëª©ë¡\n\n"

    # ì„ ë°• ëª©ë¡
    if analysis_results.get("vessel_list"):
        for vessel in analysis_results["vessel_list"][:10]:
            vessel_name = vessel.get("vessel", "N/A").split("/")[-1]
            report_content += f"- **{vessel_name}**\n"

    report_content += "\n## ğŸ“ ìœ„ì¹˜ë³„ í™œë™ ë¶„ì„\n\n"

    # ìœ„ì¹˜ë³„ í™œë™ íŒŒì´ì°¨íŠ¸
    report_content += f"{generate_location_activity_mermaid(analysis_results)}\n\n"

    # ìœ„ì¹˜ ëª©ë¡
    if analysis_results.get("location_list"):
        report_content += "### ë“±ë¡ëœ ìœ„ì¹˜\n\n"
        for location in analysis_results["location_list"][:10]:
            location_name = location.get("location", "N/A").split("/")[-1]
            report_content += f"- **{location_name}**\n"

    report_content += "\n## ğŸ’¬ ë©”ì‹œì§€ í™œë™ ë¶„ì„\n\n"

    # ë©”ì‹œì§€ ìˆ˜
    if analysis_results.get("message_count"):
        message_count = (
            analysis_results["message_count"][0].get("message_count", 0)
            if analysis_results["message_count"]
            else 0
        )
        report_content += f"### ì´ ë©”ì‹œì§€ ìˆ˜: **{message_count}ê°œ**\n\n"

    report_content += "\n## ğŸ”„ í”„ë¡œì„¸ìŠ¤ í”Œë¡œìš°\n\n"

    # í”„ë¡œì„¸ìŠ¤ í”Œë¡œìš°
    report_content += f"{generate_timeline_mermaid(analysis_results)}\n\n"

    report_content += "\n## ğŸ” LPO ìƒ˜í”Œ ë¶„ì„\n\n"

    # LPO ìƒ˜í”Œ
    if analysis_results.get("lpo_sample"):
        report_content += "### ìƒìœ„ 10ê°œ LPO ìƒ˜í”Œ\n\n"
        report_content += "| LPO | ë‹´ë‹¹ì | ìœ„ì¹˜ |\n"
        report_content += "|-----|--------|------|\n"
        for lpo in analysis_results["lpo_sample"][:10]:
            lpo_name = lpo.get("lpo", "N/A").split("/")[-1]
            person_name = lpo.get("person", "N/A").split("/")[-1].replace("_", " ")
            location_name = lpo.get("location", "N/A").split("/")[-1]
            report_content += f"| {lpo_name} | {person_name} | {location_name} |\n"

    report_content += """
## ğŸ“ˆ í•µì‹¬ ì¸ì‚¬ì´íŠ¸

### 1. ë°ì´í„° í’ˆì§ˆ
- **ì™„ì „í•œ ì¶”ì **: LPO ë°œì£¼ë¶€í„° ë°°ì†¡ê¹Œì§€ ì „ ê³¼ì •ì´ RDFë¡œ êµ¬ì¡°í™”ë¨
- **ê´€ê³„ ë„¤íŠ¸ì›Œí¬**: ë‹´ë‹¹ì, ì„ ë°•, ìœ„ì¹˜, LPO ê°„ ë³µì¡í•œ ê´€ê³„ê°€ ëª…í™•íˆ ì¶”ì ë¨
- **ì‹œê°„ì  ì¼ê´€ì„±**: ëª¨ë“  í™œë™ì´ ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬ë˜ì–´ ì´ë²¤íŠ¸ ì²´ì¸ êµ¬ì„±

### 2. ìš´ì˜ íš¨ìœ¨ì„±
- **ë‹´ë‹¹ìë³„ ì—…ë¬´ ë¶„ë‹´**: ê° ë‹´ë‹¹ìì˜ LPO ì²˜ë¦¬ í˜„í™©ì´ ëª…í™•íˆ íŒŒì•…ë¨
- **ì„ ë°• í™œìš©ë„**: ì„ ë°•ë³„ ìš´ì†¡ í˜„í™©ê³¼ ìœ„ì¹˜ë³„ ì„œë¹„ìŠ¤ í˜„í™© ë¶„ì„ ê°€ëŠ¥
- **ì´ë¯¸ì§€ ì¦ê±°**: ë©”ì‹œì§€ì™€ ì—°ê²°ëœ ì´ë¯¸ì§€ë¥¼ í†µí•œ ì—…ë¬´ ì¦ê±° í™•ë³´

### 3. ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜
- **íˆ¬ëª…ì„±**: ëª¨ë“  ë¬¼ë¥˜ í™œë™ì´ ì¶”ì  ê°€ëŠ¥í•œ í˜•íƒœë¡œ ê¸°ë¡ë¨
- **ì±…ì„ ëª…í™•í™”**: ëˆ„ê°€, ì–¸ì œ, ë¬´ì—‡ì„ ì²˜ë¦¬í–ˆëŠ”ì§€ ëª…í™•íˆ íŒŒì•… ê°€ëŠ¥
- **ì˜ì‚¬ê²°ì • ì§€ì›**: ë°ì´í„° ê¸°ë°˜ ë¬¼ë¥˜ ìµœì í™” ì˜ì‚¬ê²°ì • ì§€ì›

## ğŸš€ ê¶Œì¥ì‚¬í•­

1. **ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§**: ìƒˆë¡œìš´ LPOë‚˜ ë©”ì‹œì§€ê°€ ì¶”ê°€ë  ë•Œ ìë™ RDF ì—…ë°ì´íŠ¸
2. **ê³ ê¸‰ ë¶„ì„**: ë¨¸ì‹ ëŸ¬ë‹ì„ í™œìš©í•œ ë¬¼ë¥˜ íŒ¨í„´ ë¶„ì„ ë° ì˜ˆì¸¡
3. **API ì„œë¹„ìŠ¤**: ì™¸ë¶€ ì‹œìŠ¤í…œê³¼ì˜ ì‹¤ì‹œê°„ ë°ì´í„° ì—°ë™
4. **ì›¹ ëŒ€ì‹œë³´ë“œ**: ë¸Œë¼ìš°ì € ê¸°ë°˜ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ êµ¬ì¶•

---
*ì´ ë³´ê³ ì„œëŠ” ABU í†µí•© ì‹œìŠ¤í…œì˜ SPARQL ì¿¼ë¦¬ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.*
"""

    return report_content


def save_analysis_results(analysis_results, report_content):
    """ë¶„ì„ ê²°ê³¼ ì €ì¥"""
    # ë³´ê³ ì„œ ì €ì¥
    report_file = Path("reports/abu_sparql_analysis_report.md")
    report_file.parent.mkdir(exist_ok=True)

    with open(report_file, "w", encoding="utf-8") as f:
        f.write(report_content)

    # JSON ë°ì´í„° ì €ì¥
    json_file = Path("reports/abu_sparql_analysis_data.json")
    with open(json_file, "w", encoding="utf-8") as f:
        json.dump(analysis_results, f, ensure_ascii=False, indent=2)

    return report_file, json_file


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (ìµœì í™”)"""
    print("ğŸ”„ ABU í†µí•© ì‹œìŠ¤í…œ SPARQL ì¿¼ë¦¬ ì‹¤í–‰ ë° ë¶„ì„ ì‹œì‘...")
    start_time = datetime.now()

    # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì„¤ì •
    ns_dict = setup_namespaces()

    # í†µí•© RDF ê·¸ë˜í”„ ë¡œë“œ
    print("ğŸ“Š í†µí•© RDF ê·¸ë˜í”„ ë¡œë“œ ì¤‘...")
    graph = load_integrated_rdf()
    if not graph:
        return

    # ë¶„ì„ ì‹¤í–‰ (ë‹¨ìˆœí™”)
    analysis_results = {}

    try:
        # 1. ì‹œìŠ¤í…œ ì „ì²´ ìš”ì•½ (ê°€ì¥ ê¸°ë³¸)
        print("1ï¸âƒ£ ì‹œìŠ¤í…œ ì „ì²´ í†µê³„ ìƒì„± ì¤‘...")
        system_summary = generate_system_summary(graph, ns_dict)
        analysis_results["system_summary"] = system_summary

        # 2. ë‹´ë‹¹ì ëª©ë¡
        print("2ï¸âƒ£ ë‹´ë‹¹ì ëª©ë¡ ìƒì„± ì¤‘...")
        person_list = analyze_person_list(graph, ns_dict)
        analysis_results["person_list"] = person_list

        # 3. ìœ„ì¹˜ ëª©ë¡
        print("3ï¸âƒ£ ìœ„ì¹˜ ëª©ë¡ ìƒì„± ì¤‘...")
        location_list = analyze_location_list(graph, ns_dict)
        analysis_results["location_list"] = location_list

        # 4. ì„ ë°• ëª©ë¡
        print("4ï¸âƒ£ ì„ ë°• ëª©ë¡ ìƒì„± ì¤‘...")
        vessel_list = analyze_vessel_list(graph, ns_dict)
        analysis_results["vessel_list"] = vessel_list

        # 5. LPO ìƒ˜í”Œ
        print("5ï¸âƒ£ LPO ìƒ˜í”Œ ìƒì„± ì¤‘...")
        lpo_sample = analyze_lpo_sample(graph, ns_dict)
        analysis_results["lpo_sample"] = lpo_sample

        # 6. ë©”ì‹œì§€ ìˆ˜
        print("6ï¸âƒ£ ë©”ì‹œì§€ ìˆ˜ í™•ì¸ ì¤‘...")
        message_count = analyze_message_count(graph, ns_dict)
        analysis_results["message_count"] = message_count

    except Exception as e:
        print(f"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        # ê¸°ë³¸ê°’ìœ¼ë¡œ ê³„ì† ì§„í–‰
        analysis_results = {
            "system_summary": {
                "total_lpos": 0,
                "total_persons": 0,
                "total_vessels": 0,
                "total_locations": 0,
                "total_messages": 0,
                "total_images": 0,
            },
            "person_list": [],
            "location_list": [],
            "vessel_list": [],
            "lpo_sample": [],
            "message_count": [],
        }

    # ë³´ê³ ì„œ ìƒì„±
    print("ğŸ“‹ ë¶„ì„ ë³´ê³ ì„œ ìƒì„± ì¤‘...")
    report_content = generate_analysis_report(analysis_results)

    # ê²°ê³¼ ì €ì¥
    report_file, json_file = save_analysis_results(analysis_results, report_content)

    end_time = datetime.now()
    execution_time = (end_time - start_time).total_seconds()

    print(f"âœ… ABU SPARQL ë¶„ì„ ì™„ë£Œ! (ì‹¤í–‰ ì‹œê°„: {execution_time:.1f}ì´ˆ)")
    print(f"  - ë¶„ì„ ë³´ê³ ì„œ: {report_file}")
    print(f"  - ë¶„ì„ ë°ì´í„°: {json_file}")
    print(
        f"  - ì´ LPO: {analysis_results['system_summary'].get('total_lpos', 'N/A')}ê°œ"
    )
    print(
        f"  - ë‹´ë‹¹ì: {analysis_results['system_summary'].get('total_persons', 'N/A')}ëª…"
    )
    print(
        f"  - ì„ ë°•: {analysis_results['system_summary'].get('total_vessels', 'N/A')}ì²™"
    )


if __name__ == "__main__":
    main()
