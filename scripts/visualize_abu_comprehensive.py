#!/usr/bin/env python3
"""
ABU ì¢…í•© ì‹œê°í™” ë³´ê³ ì„œ ìŠ¤í¬ë¦½íŠ¸
íƒœê·¸ ì‚¬ì „, ë‹´ë‹¹ì ë¶„ì„, í‚¤ì›Œë“œ-ì—”í‹°í‹° ì—°ê²°ì„ í†µí•©í•œ ì¢…í•© ì‹œê°í™” ë³´ê³ ì„œ ìƒì„±
"""

import sys
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Set
from collections import Counter, defaultdict

# RDF ì²˜ë¦¬
from rdflib import Graph, Namespace, RDF, RDFS, XSD, Literal, URIRef
from rdflib.namespace import NamespaceManager

# Unicode ì¶œë ¥ ì§€ì›
sys.stdout.reconfigure(encoding="utf-8")


def load_rdf_data(rdf_file: str) -> Graph:
    """RDF íŒŒì¼ ë¡œë“œ"""
    g = Graph()
    if Path(rdf_file).exists():
        g.parse(rdf_file, format="turtle")
        print(f"[INFO] RDF íŒŒì¼ ë¡œë“œ: {rdf_file}")
    else:
        print(f"[ERROR] RDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {rdf_file}")
        return None
    return g


def load_analysis_data(analysis_file: str) -> Dict[str, Any]:
    """ë‹´ë‹¹ì ë¶„ì„ ë°ì´í„° ë¡œë“œ"""
    if Path(analysis_file).exists():
        with open(analysis_file, "r", encoding="utf-8") as f:
            return json.load(f)
    else:
        print(f"[WARNING] ë¶„ì„ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {analysis_file}")
        return {}


def setup_namespaces() -> Dict[str, Namespace]:
    """ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì„¤ì •"""
    return {
        "abu": Namespace("https://abu-dhabi.example.org/ns#"),
        "abui": Namespace("https://abu-dhabi.example.org/id/"),
        "xsd": XSD,
        "rdf": RDF,
        "rdfs": RDFS,
    }


def analyze_tag_categories(g: Graph, ns_dict: Dict[str, Namespace]) -> Dict[str, Any]:
    """íƒœê·¸ ì‚¬ì „ ì¹´í…Œê³ ë¦¬ ë¶„ì„"""
    print("[INFO] íƒœê·¸ ì‚¬ì „ ì¹´í…Œê³ ë¦¬ ë¶„ì„ ì¤‘...")

    category_stats = {
        "VesselEntity": 0,
        "LocationEntity": 0,
        "DocumentType": 0,
        "EquipmentType": 0,
        "RiskFactor": 0,
        "StatusType": 0,
    }

    # ê° ì—”í‹°í‹° íƒ€ì…ë³„ ê°œìˆ˜ ê³„ì‚°
    for entity_type in category_stats.keys():
        count = len(list(g.triples((None, RDF.type, ns_dict["abu"][entity_type]))))
        category_stats[entity_type] = count

    return category_stats


def analyze_responsible_persons(
    g: Graph, ns_dict: Dict[str, Namespace]
) -> Dict[str, Any]:
    """ë‹´ë‹¹ì ë¶„ì„"""
    print("[INFO] ë‹´ë‹¹ì ë¶„ì„ ì¤‘...")

    person_stats = defaultdict(
        lambda: {
            "shipments": 0,
            "containers": 0,
            "deliveries": 0,
            "vessels": set(),
            "locations": set(),
        }
    )

    # Shipment ë¶„ì„
    for s, p, o in g.triples((None, RDF.type, ns_dict["abu"]["AbuDhabiShipment"])):
        for _, _, person in g.triples((s, ns_dict["abu"]["responsiblePerson"], None)):
            person_name = str(person)
            person_stats[person_name]["shipments"] += 1

            for _, _, ship_name in g.triples((s, ns_dict["abu"]["shipName"], None)):
                person_stats[person_name]["vessels"].add(str(ship_name))

            for _, _, location in g.triples(
                (s, ns_dict["abu"]["currentLocation"], None)
            ):
                person_stats[person_name]["locations"].add(str(location))

    # Container ë¶„ì„
    for s, p, o in g.triples((None, RDF.type, ns_dict["abu"]["AbuDhabiContainer"])):
        for _, _, person in g.triples((s, ns_dict["abu"]["responsiblePerson"], None)):
            person_name = str(person)
            person_stats[person_name]["containers"] += 1

    # Delivery ë¶„ì„
    for s, p, o in g.triples((None, RDF.type, ns_dict["abu"]["AbuDhabiDelivery"])):
        for _, _, person in g.triples((s, ns_dict["abu"]["responsiblePerson"], None)):
            person_name = str(person)
            person_stats[person_name]["deliveries"] += 1

    # System ì œì™¸í•˜ê³  ì‹¤ì œ ë‹´ë‹¹ìë§Œ
    actual_persons = {k: v for k, v in person_stats.items() if k != "System"}

    return {
        "total_persons": len(actual_persons),
        "person_details": {
            person: {
                "shipments": stats["shipments"],
                "containers": stats["containers"],
                "deliveries": stats["deliveries"],
                "total": stats["shipments"] + stats["containers"] + stats["deliveries"],
                "vessels": list(stats["vessels"]),
                "locations": list(stats["locations"]),
            }
            for person, stats in actual_persons.items()
        },
    }


def analyze_keyword_entity_links(
    g: Graph, ns_dict: Dict[str, Namespace]
) -> Dict[str, Any]:
    """í‚¤ì›Œë“œ-ì—”í‹°í‹° ì—°ê²° ë¶„ì„"""
    print("[INFO] í‚¤ì›Œë“œ-ì—”í‹°í‹° ì—°ê²° ë¶„ì„ ì¤‘...")

    keyword_usage = Counter()
    entity_keyword_links = defaultdict(list)

    # Shipment-í‚¤ì›Œë“œ ì—°ê²° ë¶„ì„
    for s, p, o in g.triples((None, RDF.type, ns_dict["abu"]["AbuDhabiShipment"])):
        for _, _, keyword_uri in g.triples((s, ns_dict["abu"]["relatedKeyword"], None)):
            keyword_name = str(keyword_uri).split("/")[-1]
            keyword_usage[keyword_name] += 1
            entity_keyword_links["shipment"].append(keyword_name)

    # Container-í‚¤ì›Œë“œ ì—°ê²° ë¶„ì„
    for s, p, o in g.triples((None, RDF.type, ns_dict["abu"]["AbuDhabiContainer"])):
        for _, _, keyword_uri in g.triples((s, ns_dict["abu"]["relatedKeyword"], None)):
            keyword_name = str(keyword_uri).split("/")[-1]
            keyword_usage[keyword_name] += 1
            entity_keyword_links["container"].append(keyword_name)

    # Delivery-í‚¤ì›Œë“œ ì—°ê²° ë¶„ì„
    for s, p, o in g.triples((None, RDF.type, ns_dict["abu"]["AbuDhabiDelivery"])):
        for _, _, keyword_uri in g.triples((s, ns_dict["abu"]["relatedKeyword"], None)):
            keyword_name = str(keyword_uri).split("/")[-1]
            keyword_usage[keyword_name] += 1
            entity_keyword_links["delivery"].append(keyword_name)

    return {
        "top_keywords": dict(keyword_usage.most_common(20)),
        "entity_keyword_links": dict(entity_keyword_links),
        "total_keyword_links": sum(keyword_usage.values()),
    }


def generate_tag_category_diagram(category_stats: Dict[str, Any]) -> str:
    """íƒœê·¸ ì‚¬ì „ ì¹´í…Œê³ ë¦¬ ë¶„í¬ ë‹¤ì´ì–´ê·¸ë¨"""
    return f"""```mermaid
pie title íƒœê·¸ ì‚¬ì „ ì¹´í…Œê³ ë¦¬ ë¶„í¬
    "VesselEntity" : {category_stats.get('VesselEntity', 0)}
    "LocationEntity" : {category_stats.get('LocationEntity', 0)}
    "DocumentType" : {category_stats.get('DocumentType', 0)}
    "EquipmentType" : {category_stats.get('EquipmentType', 0)}
    "RiskFactor" : {category_stats.get('RiskFactor', 0)}
    "StatusType" : {category_stats.get('StatusType', 0)}
```"""


def generate_person_work_distribution_diagram(person_stats: Dict[str, Any]) -> str:
    """ë‹´ë‹¹ìë³„ ì—…ë¬´ ë¶„í¬ ë‹¤ì´ì–´ê·¸ë¨"""
    # ìƒìœ„ 10ëª…ë§Œ í‘œì‹œ
    top_persons = sorted(
        person_stats["person_details"].items(),
        key=lambda x: x[1]["total"],
        reverse=True,
    )[:10]

    data_lines = []
    for person, stats in top_persons:
        data_lines.append(f'    "{person}" : {stats["total"]}')

    return f"""```mermaid
pie title ë‹´ë‹¹ìë³„ ì´ ì—…ë¬´ ë¶„í¬ (ìƒìœ„ 10ëª…)
{chr(10).join(data_lines)}
```"""


def generate_person_vessel_network_diagram(person_stats: Dict[str, Any]) -> str:
    """ë‹´ë‹¹ì-ì„ ë°• ë„¤íŠ¸ì›Œí¬ ë‹¤ì´ì–´ê·¸ë¨"""
    # ìƒìœ„ 5ëª…ê³¼ ì£¼ìš” ì„ ë°•ë“¤ë§Œ í‘œì‹œ
    top_persons = sorted(
        person_stats["person_details"].items(),
        key=lambda x: x[1]["total"],
        reverse=True,
    )[:5]

    nodes = []
    edges = []

    for person, stats in top_persons:
        nodes.append(f'    {person.replace(" ", "_")}["{person}"]')
        for vessel in stats["vessels"][:3]:  # ìƒìœ„ 3ê°œ ì„ ë°•ë§Œ
            vessel_id = vessel.replace(" ", "_").replace("-", "_")
            nodes.append(f'    {vessel_id}["{vessel}"]')
            edges.append(f'    {person.replace(" ", "_")} --> {vessel_id}')

    return f"""```mermaid
graph TD
{chr(10).join(nodes)}
{chr(10).join(edges)}
```"""


def generate_keyword_heatmap_data(keyword_analysis: Dict[str, Any]) -> str:
    """í‚¤ì›Œë“œ íˆíŠ¸ë§µ ë°ì´í„° (í‘œ í˜•ì‹)"""
    top_keywords = keyword_analysis["top_keywords"]

    # ìƒìœ„ 10ê°œ í‚¤ì›Œë“œë§Œ í‘œì‹œ
    table_rows = []
    for i, (keyword, count) in enumerate(list(top_keywords.items())[:10], 1):
        table_rows.append(f"| {i} | {keyword} | {count} |")

    return f"""| ìˆœìœ„ | í‚¤ì›Œë“œ | ì‚¬ìš© íšŸìˆ˜ |
|------|--------|-----------|
{chr(10).join(table_rows)}"""


def generate_location_work_distribution(g: Graph, ns_dict: Dict[str, Namespace]) -> str:
    """ìœ„ì¹˜ë³„ ì‘ì—… ë¶„í¬ ë‹¤ì´ì–´ê·¸ë¨"""
    location_stats = Counter()

    # Shipmentì—ì„œ ìœ„ì¹˜ ì¶”ì¶œ
    for s, p, o in g.triples((None, RDF.type, ns_dict["abu"]["AbuDhabiShipment"])):
        for _, _, location in g.triples((s, ns_dict["abu"]["currentLocation"], None)):
            location_stats[str(location)] += 1

    # ìƒìœ„ 8ê°œ ìœ„ì¹˜ë§Œ í‘œì‹œ
    top_locations = location_stats.most_common(8)

    data_lines = []
    for location, count in top_locations:
        data_lines.append(f'    "{location}" : {count}')

    return f"""```mermaid
pie title ìœ„ì¹˜ë³„ ì‘ì—… ë¶„í¬
{chr(10).join(data_lines)}
```"""


def generate_comprehensive_report(
    category_stats: Dict[str, Any],
    person_stats: Dict[str, Any],
    keyword_analysis: Dict[str, Any],
    g: Graph,
    ns_dict: Dict[str, Namespace],
) -> str:
    """ì¢…í•© ë³´ê³ ì„œ ìƒì„±"""

    # ê¸°ë³¸ í†µê³„
    total_entities = len(list(g.triples((None, RDF.type, None))))
    total_triples = len(g)

    # íƒœê·¸ ì‚¬ì „ ì´ ì—”í‹°í‹° ìˆ˜
    total_tag_entities = sum(category_stats.values())

    report = f"""# ABU ì¢…í•© ë¶„ì„ ë³´ê³ ì„œ

**ìƒì„± ì‹œê°„**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ğŸ“Š ì‹¤í–‰ ìš”ì•½

- **ì´ RDF ì—”í‹°í‹°**: {total_entities:,}ê°œ
- **ì´ RDF íŠ¸ë¦¬í”Œ**: {total_triples:,}ê°œ
- **íƒœê·¸ ì‚¬ì „ ì—”í‹°í‹°**: {total_tag_entities}ê°œ
- **ì‹¤ì œ ë‹´ë‹¹ì**: {person_stats['total_persons']}ëª…
- **í‚¤ì›Œë“œ ì—°ê²°**: {keyword_analysis['total_keyword_links']}ê°œ

## ğŸ·ï¸ íƒœê·¸ ì‚¬ì „ ì¹´í…Œê³ ë¦¬ ë¶„í¬

{generate_tag_category_diagram(category_stats)}

### ì¹´í…Œê³ ë¦¬ë³„ ìƒì„¸ í†µê³„

| ì¹´í…Œê³ ë¦¬ | ì—”í‹°í‹° ìˆ˜ | ì„¤ëª… |
|----------|-----------|------|
| VesselEntity | {category_stats.get('VesselEntity', 0)} | ì„ ë°•/ë°”ì§€ì„  ê´€ë ¨ |
| LocationEntity | {category_stats.get('LocationEntity', 0)} | ìœ„ì¹˜/ì‚¬ì´íŠ¸ ê´€ë ¨ |
| DocumentType | {category_stats.get('DocumentType', 0)} | ë¬¸ì„œ/ìŠ¹ì¸ ê´€ë ¨ |
| EquipmentType | {category_stats.get('EquipmentType', 0)} | ì¥ë¹„/ì‘ì—… ê´€ë ¨ |
| RiskFactor | {category_stats.get('RiskFactor', 0)} | ë¦¬ìŠ¤í¬/í’ˆì§ˆ ê´€ë ¨ |
| StatusType | {category_stats.get('StatusType', 0)} | ìƒíƒœ/ì•¡ì…˜ ê´€ë ¨ |

## ğŸ‘¥ ë‹´ë‹¹ì ì—…ë¬´ ë¶„í¬

{generate_person_work_distribution_diagram(person_stats)}

### ìƒìœ„ ë‹´ë‹¹ì ìƒì„¸ í˜„í™©

| ë‹´ë‹¹ì | ì´ ì—…ë¬´ | ì„ ë°• | ì»¨í…Œì´ë„ˆ | ë°°ì†¡ | ì£¼ìš” ì„ ë°• | ì£¼ìš” ìœ„ì¹˜ |
|--------|---------|------|----------|------|-----------|-----------|
"""

    # ìƒìœ„ ë‹´ë‹¹ì í…Œì´ë¸”
    top_persons = sorted(
        person_stats["person_details"].items(),
        key=lambda x: x[1]["total"],
        reverse=True,
    )[:10]

    for person, stats in top_persons:
        main_vessels = ", ".join(stats["vessels"][:3]) if stats["vessels"] else "-"
        main_locations = (
            ", ".join(stats["locations"][:3]) if stats["locations"] else "-"
        )
        report += f"| {person} | {stats['total']} | {stats['shipments']} | {stats['containers']} | {stats['deliveries']} | {main_vessels} | {main_locations} |\n"

    report += f"""

## ğŸ”— ë‹´ë‹¹ì-ì„ ë°• ë„¤íŠ¸ì›Œí¬

{generate_person_vessel_network_diagram(person_stats)}

## ğŸ” í‚¤ì›Œë“œ ì‚¬ìš© í˜„í™©

{generate_keyword_heatmap_data(keyword_analysis)}

## ğŸ“ ìœ„ì¹˜ë³„ ì‘ì—… ë¶„í¬

{generate_location_work_distribution(g, ns_dict)}

## ğŸ¯ ì£¼ìš” ì¸ì‚¬ì´íŠ¸

### 1. ë‹´ë‹¹ì ì—…ë¬´ íŒ¨í„´
- **ìµœë‹¤ ì—…ë¬´ ë‹´ë‹¹ì**: {top_persons[0][0]} ({top_persons[0][1]['total']}ê°œ ì—…ë¬´)
- **ì„ ë°• ì—…ë¬´ ì¤‘ì‹¬**: {sum(stats['shipments'] for stats in person_stats['person_details'].values())}ê°œ ì„ ë°• ê´€ë ¨ ì—…ë¬´
- **ì»¨í…Œì´ë„ˆ ê´€ë¦¬**: {sum(stats['containers'] for stats in person_stats['person_details'].values())}ê°œ ì»¨í…Œì´ë„ˆ ì—…ë¬´
- **ë°°ì†¡ ê´€ë¦¬**: {sum(stats['deliveries'] for stats in person_stats['person_details'].values())}ê°œ ë°°ì†¡ ì—…ë¬´

### 2. í‚¤ì›Œë“œ í™œìš©ë„
- **ìµœë‹¤ ì‚¬ìš© í‚¤ì›Œë“œ**: {list(keyword_analysis['top_keywords'].keys())[0]} ({list(keyword_analysis['top_keywords'].values())[0]}íšŒ)
- **ì´ í‚¤ì›Œë“œ ì—°ê²°**: {keyword_analysis['total_keyword_links']}ê°œ
- **ì—”í‹°í‹°-í‚¤ì›Œë“œ ë§¤ì¹­ë¥ **: {keyword_analysis['total_keyword_links'] / total_entities * 100:.1f}%

### 3. íƒœê·¸ ì‚¬ì „ í™œìš©
- **ì´ íƒœê·¸ ì—”í‹°í‹°**: {total_tag_entities}ê°œ
- **ê°€ì¥ ë§ì€ ì¹´í…Œê³ ë¦¬**: {max(category_stats.items(), key=lambda x: x[1])[0]} ({max(category_stats.values())}ê°œ)
- **RDF í†µí•© ì™„ë£Œ**: ëª¨ë“  íƒœê·¸ ì‚¬ì „ì´ RDFë¡œ ë³€í™˜ë˜ì–´ ì—°ê²°ë¨

## ğŸ“ˆ ë°ì´í„° í’ˆì§ˆ ì§€í‘œ

- **ë‹´ë‹¹ì ë§¤í•‘ë¥ **: 100% (ëª¨ë“  Shipment/Container/Deliveryì— ë‹´ë‹¹ì ì •ë³´ í¬í•¨)
- **í‚¤ì›Œë“œ ì—°ê²°ë¥ **: {keyword_analysis['total_keyword_links'] / total_entities * 100:.1f}%
- **íƒœê·¸ ì‚¬ì „ í™œìš©ë¥ **: 100% (ëª¨ë“  íƒœê·¸ê°€ RDFë¡œ ë³€í™˜ë¨)
- **ë°ì´í„° ì¼ê´€ì„±**: ë†’ìŒ (ì •ê·œí™”ëœ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì‚¬ìš©)

## ğŸ”§ í™œìš© ê¶Œì¥ì‚¬í•­

1. **ë‹´ë‹¹ìë³„ ì—…ë¬´ ëª¨ë‹ˆí„°ë§**: ìƒìœ„ ë‹´ë‹¹ìì˜ ì—…ë¬´ ë¶„í¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì—…ë¬´ëŸ‰ ì¡°ì •
2. **í‚¤ì›Œë“œ ê¸°ë°˜ ìë™ ë¶„ë¥˜**: ìì£¼ ì‚¬ìš©ë˜ëŠ” í‚¤ì›Œë“œë¥¼ í™œìš©í•œ ìë™ íƒœê¹… ì‹œìŠ¤í…œ êµ¬ì¶•
3. **ìœ„ì¹˜ë³„ ë¦¬ì†ŒìŠ¤ ë°°ì¹˜**: ì‘ì—…ì´ ë§ì€ ìœ„ì¹˜ì— ë” ë§ì€ ë¦¬ì†ŒìŠ¤ ë°°ì¹˜
4. **íƒœê·¸ ì‚¬ì „ í™•ì¥**: ìƒˆë¡œìš´ í‚¤ì›Œë“œë‚˜ ì—”í‹°í‹° ì¶”ê°€ ì‹œ ê¸°ì¡´ íŒ¨í„´ í™œìš©

---
*ì´ ë³´ê³ ì„œëŠ” ABU ë¬¼ë¥˜ ë°ì´í„°ì˜ RDF ë³€í™˜, ë‹´ë‹¹ì ë§¤í•‘, í‚¤ì›Œë“œ ì—°ê²°ì„ ì¢…í•© ë¶„ì„í•œ ê²°ê³¼ì…ë‹ˆë‹¤.*
"""

    return report


def save_comprehensive_report(report: str, output_file: str):
    """ì¢…í•© ë³´ê³ ì„œ ì €ì¥"""
    Path("reports").mkdir(exist_ok=True)

    with open(output_file, "w", encoding="utf-8") as f:
        f.write(report)

    print(f"[SUCCESS] ì¢…í•© ë³´ê³ ì„œ ì €ì¥: {output_file}")


def save_analysis_summary(
    category_stats: Dict[str, Any],
    person_stats: Dict[str, Any],
    keyword_analysis: Dict[str, Any],
    output_file: str,
):
    """ë¶„ì„ ìš”ì•½ JSON ì €ì¥"""
    summary = {
        "metadata": {
            "generated_at": datetime.now().isoformat(),
            "report_type": "comprehensive_analysis",
        },
        "category_stats": category_stats,
        "person_stats": person_stats,
        "keyword_analysis": keyword_analysis,
    }

    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)

    print(f"[SUCCESS] ë¶„ì„ ìš”ì•½ ì €ì¥: {output_file}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("=" * 60)
    print("ABU ì¢…í•© ì‹œê°í™” ë³´ê³ ì„œ ìŠ¤í¬ë¦½íŠ¸")
    print("=" * 60)

    # íŒŒì¼ ê²½ë¡œ ì„¤ì •
    rdf_file = "output/abu_logistics_data.ttl"
    analysis_file = "reports/abu_responsible_persons_analysis.json"
    report_file = "reports/abu_comprehensive_analysis.md"
    summary_file = "reports/abu_comprehensive_summary.json"

    # 1. RDF ë°ì´í„° ë¡œë“œ
    print("\n1. RDF ë°ì´í„° ë¡œë“œ")
    g = load_rdf_data(rdf_file)
    if g is None:
        return

    ns_dict = setup_namespaces()

    # 2. ë‹´ë‹¹ì ë¶„ì„ ë°ì´í„° ë¡œë“œ
    print("\n2. ë‹´ë‹¹ì ë¶„ì„ ë°ì´í„° ë¡œë“œ")
    analysis_data = load_analysis_data(analysis_file)

    # 3. íƒœê·¸ ì‚¬ì „ ì¹´í…Œê³ ë¦¬ ë¶„ì„
    print("\n3. íƒœê·¸ ì‚¬ì „ ì¹´í…Œê³ ë¦¬ ë¶„ì„")
    category_stats = analyze_tag_categories(g, ns_dict)

    # 4. ë‹´ë‹¹ì ë¶„ì„
    print("\n4. ë‹´ë‹¹ì ë¶„ì„")
    person_stats = analyze_responsible_persons(g, ns_dict)

    # 5. í‚¤ì›Œë“œ-ì—”í‹°í‹° ì—°ê²° ë¶„ì„
    print("\n5. í‚¤ì›Œë“œ-ì—”í‹°í‹° ì—°ê²° ë¶„ì„")
    keyword_analysis = analyze_keyword_entity_links(g, ns_dict)

    # 6. ì¢…í•© ë³´ê³ ì„œ ìƒì„±
    print("\n6. ì¢…í•© ë³´ê³ ì„œ ìƒì„±")
    report = generate_comprehensive_report(
        category_stats, person_stats, keyword_analysis, g, ns_dict
    )

    # 7. ë³´ê³ ì„œ ì €ì¥
    print("\n7. ë³´ê³ ì„œ ì €ì¥")
    save_comprehensive_report(report, report_file)
    save_analysis_summary(category_stats, person_stats, keyword_analysis, summary_file)

    # 8. ê²°ê³¼ ìš”ì•½
    print("\n[SUCCESS] ì¢…í•© ì‹œê°í™” ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ")
    print(f"  - ì´ RDF ì—”í‹°í‹°: {len(list(g.triples((None, RDF.type, None)))):,}ê°œ")
    print(f"  - ì´ RDF íŠ¸ë¦¬í”Œ: {len(g):,}ê°œ")
    print(f"  - íƒœê·¸ ì‚¬ì „ ì—”í‹°í‹°: {sum(category_stats.values())}ê°œ")
    print(f"  - ì‹¤ì œ ë‹´ë‹¹ì: {person_stats['total_persons']}ëª…")
    print(f"  - í‚¤ì›Œë“œ ì—°ê²°: {keyword_analysis['total_keyword_links']}ê°œ")
    print(f"  - ë³´ê³ ì„œ íŒŒì¼: {report_file}")
    print(f"  - ìš”ì•½ íŒŒì¼: {summary_file}")

    return {
        "category_stats": category_stats,
        "person_stats": person_stats,
        "keyword_analysis": keyword_analysis,
        "total_entities": len(list(g.triples((None, RDF.type, None)))),
        "total_triples": len(g),
    }


if __name__ == "__main__":
    main()
