#!/usr/bin/env python3
"""
Flow Code v3.5 Calculator - CLI Script
ì´ë²¤íŠ¸ ê¸°ë°˜ ê´€ì¸¡ê°’ìœ¼ë¡œ Flow Code 0~5 ê³„ì‚° + AGI/DAS ë„ë©”ì¸ ë£° ì ìš©

Usage:
    python scripts/core/flow_code_calc.py --input data.xlsx --output result.csv
    python scripts/core/flow_code_calc.py --input data.xlsx --output result.json --format json
"""

from __future__ import annotations
import argparse
import sys
from pathlib import Path
import logging

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ PYTHONPATHì— ì¶”ê°€
ROOT = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(ROOT))

import pandas as pd
import numpy as np
from typing import List, Optional
import json

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# ============================================================================
# Core Functions (from logiontology/src/ingest/flow_code_calculator.py)
# ============================================================================

def normalize_column_names(df: pd.DataFrame) -> pd.DataFrame:
    """
    ì»¬ëŸ¼ëª… ì •ê·œí™”: ê°œí–‰ ë¬¸ì ì œê±°, ê³µë°± ì •ë¦¬

    'DSV\n Indoor' â†’ 'DSV Indoor'
    """
    df = df.copy()
    # ê°œí–‰ ë¬¸ì ì œê±° â†’ ê³µë°± ì •ê·œí™” (ì—°ì† ê³µë°±ì„ 1ê°œë¡œ)
    df.columns = df.columns.str.replace('\n', ' ')
    df.columns = df.columns.str.replace(r'\s+', ' ', regex=True).str.strip()
    return df


def extract_final_location(row: pd.Series, site_cols: List[str]) -> Optional[str]:
    """
    Site ì»¬ëŸ¼ì—ì„œ ìµœì¢… ìœ„ì¹˜ ì¶”ì¶œ

    Logic:
        1. Site ì»¬ëŸ¼ë“¤ ì¤‘ ë‚ ì§œê°€ ìˆëŠ” ê²ƒ í•„í„°ë§
        2. ê°€ì¥ ìµœê·¼ ë‚ ì§œë¥¼ ê°€ì§„ ì»¬ëŸ¼ëª… ë°˜í™˜
        3. ë‚ ì§œê°€ ì—†ìœ¼ë©´ None

    Example:
        SHU=2024-01-10, MIR=NaN, DAS=2024-01-15, AGI=NaN
        â†’ Final_Location = "DAS"
    """
    dates_dict = {}
    for col in site_cols:
        val = row.get(col)
        if pd.notna(val):
            try:
                date_val = pd.to_datetime(val, errors='coerce')
                if pd.notna(date_val):
                    dates_dict[col] = date_val
            except:
                pass

    if dates_dict:
        # ìµœê·¼ ë‚ ì§œ ê¸°ì¤€ ì •ë ¬
        latest_site = max(dates_dict, key=dates_dict.get)
        return latest_site
    return None


def is_pre_arrival(row: pd.Series, all_date_cols: List[str], has_ata_col: bool = False) -> bool:
    """
    Pre Arrival íŒë³„

    Conditions:
        1. ATA (ì‹¤ì œ ë„ì°©ì¼) ì»¬ëŸ¼ì´ NaN
        2. ë˜ëŠ” ëª¨ë“  ì°½ê³ /ì‚¬ì´íŠ¸ ì»¬ëŸ¼ì´ NaN
    """
    # Option 1: ATA ì»¬ëŸ¼ í™•ì¸
    if has_ata_col:
        ata = row.get('ATA')
        if pd.isna(ata):
            return True
        # ATAê°€ ìˆê³  ë‚ ì§œ ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ Pre Arrival ì•„ë‹˜
        if all_date_cols:
            has_any_date = any(pd.notna(row.get(col)) for col in all_date_cols)
            if has_any_date:
                return False

    # Option 2: ëª¨ë“  ë‚ ì§œ ì»¬ëŸ¼ í™•ì¸
    if all_date_cols:
        has_any_date = any(pd.notna(row.get(col)) for col in all_date_cols)
        return not has_any_date

    # ê¸°ë³¸ê°’: False
    return False


def calculate_flow_code_v35(
    df: pd.DataFrame,
    warehouse_columns: List[str],
    site_columns: List[str]
) -> pd.DataFrame:
    """
    Flow Code v3.5 ê³„ì‚°

    Args:
        df: ì…ë ¥ DataFrame
        warehouse_columns: ì°½ê³  ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ (MOSB í¬í•¨)
        site_columns: ì‚¬ì´íŠ¸ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸

    Returns:
        DataFrame with added columns:
        - FLOW_CODE (0~5)
        - FLOW_DESCRIPTION
        - FLOW_CODE_ORIG (ì˜¤ë²„ë¼ì´ë“œ ì „ ì›ë³¸)
        - FLOW_OVERRIDE_REASON (ì˜¤ë²„ë¼ì´ë“œ ì‚¬ìœ )
        - Final_Location (ìë™ ì¶”ì¶œ)
        - is_pre_arrival (Pre Arrival ì—¬ë¶€)

    Algorithm:
        1. í•„ë“œ ê²€ì¦ ë° ì „ì²˜ë¦¬
        2. ê´€ì¸¡ê°’ ê³„ì‚° (is_pre_arrival, wh_cnt, has_mosb, has_site)
        3. ê¸°ë³¸ Flow Code ê³„ì‚° (0~4)
        4. AGI/DAS ë„ë©”ì¸ ì˜¤ë²„ë¼ì´ë“œ
        5. í˜¼í•© ì¼€ì´ìŠ¤ ì²˜ë¦¬ (Flow 5)
        6. ìµœì¢… ë°˜ì˜ ë° ê²€ì¦
    """
    df = df.copy()

    # Step 1: ì»¬ëŸ¼ëª… ì •ê·œí™”
    df = normalize_column_names(df)

    # ì‹¤ì œ ì»¬ëŸ¼ ì°¾ê¸° (ì •ê·œí™” í›„ - ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´)
    df_columns_lower = {col.lower(): col for col in df.columns}

    WH_COLS = []
    MOSB_COLS = []
    for wh_key in warehouse_columns:
        wh_key_lower = wh_key.lower().strip()
        # ì •í™•í•œ ë§¤ì¹­ ìš°ì„ , ì—†ìœ¼ë©´ ë¶€ë¶„ ë§¤ì¹­
        if wh_key_lower in df_columns_lower:
            col_orig = df_columns_lower[wh_key_lower]
        else:
            # ë¶€ë¶„ ë§¤ì¹­ ì‹œë„
            col_orig = None
            for col_lower, col in df_columns_lower.items():
                if wh_key_lower in col_lower:
                    col_orig = col
                    break

        if col_orig:
            if wh_key_lower == 'mosb':
                MOSB_COLS.append(col_orig)
            else:
                WH_COLS.append(col_orig)

    SITE_COLS = []
    for site_key in site_columns:
        site_key_lower = site_key.lower().strip()
        # ì •í™•í•œ ë§¤ì¹­ë§Œ
        if site_key_lower in df_columns_lower:
            SITE_COLS.append(df_columns_lower[site_key_lower])

    logger.info(f"Found columns: WH={len(WH_COLS)}, MOSB={len(MOSB_COLS)}, SITE={len(SITE_COLS)}")

    # Step 2: Final_Location ì¶”ì¶œ (ìƒˆ ì»¬ëŸ¼ ìƒì„± - ê¸°ì¡´ ê°’ì´ ì—†ì„ ë•Œë§Œ)
    if 'Final_Location' not in df.columns:
        df['Final_Location'] = df.apply(
            lambda row: extract_final_location(row, SITE_COLS),
            axis=1
        )
    else:
        logger.info("Final_Location ì»¬ëŸ¼ì´ ì´ë¯¸ ì¡´ì¬ - ìë™ ì¶”ì¶œ ê±´ë„ˆëœ€")

    # Step 3: Pre Arrival íŒë³„
    all_date_cols = WH_COLS + MOSB_COLS + SITE_COLS
    has_ata_col = 'ATA' in df.columns
    df['is_pre_arrival'] = df.apply(
        lambda row: is_pre_arrival(row, all_date_cols, has_ata_col),
        axis=1
    )

    # Step 4: ê´€ì¸¡ê°’ ê³„ì‚°
    # 4.1 ë°ì´í„° ì •ê·œí™” (0, "" â†’ NaN)
    for col in WH_COLS + MOSB_COLS:
        if col in df.columns:
            df[col] = df[col].replace({0: np.nan, "": np.nan})

    # 4.2 ì°½ê³  ê°œìˆ˜ ê³„ì‚°
    wh_cnt = df[WH_COLS].notna().sum(axis=1) if WH_COLS else pd.Series(0, index=df.index)

    # 4.3 MOSB ì¡´ì¬ ì—¬ë¶€
    has_mosb = df[MOSB_COLS].notna().any(axis=1) if MOSB_COLS else pd.Series(False, index=df.index)

    # 4.4 Site ì¡´ì¬ ì—¬ë¶€
    has_site = df[SITE_COLS].notna().any(axis=1) if SITE_COLS else pd.Series(True, index=df.index)

    # Step 5: ê¸°ë³¸ Flow Code ê³„ì‚° (0~4)
    flow = pd.Series(0, index=df.index, dtype="int64")
    flow_desc = pd.Series("", index=df.index, dtype="object")

    # Flow 0: Pre Arrival
    flow[df['is_pre_arrival']] = 0
    flow_desc[df['is_pre_arrival']] = "Flow 0: Pre Arrival"

    # ë‚˜ë¨¸ì§€ Flow ê³„ì‚° (Pre Arrival ì•„ë‹˜)
    not_pre = ~df['is_pre_arrival']

    # Flow 1: Port â†’ Site (WH=0, MOSB=0)
    mask_1 = not_pre & (wh_cnt == 0) & (~has_mosb)
    flow[mask_1] = 1
    flow_desc[mask_1] = "Flow 1: Port â†’ Site"

    # Flow 2: Port â†’ WH â†’ Site (WHâ‰¥1, MOSB=0)
    mask_2 = not_pre & (wh_cnt >= 1) & (~has_mosb)
    flow[mask_2] = 2
    flow_desc[mask_2] = "Flow 2: Port â†’ WH â†’ Site"

    # Flow 3: Port â†’ MOSB â†’ Site (WH=0, MOSB=1)
    mask_3 = not_pre & (wh_cnt == 0) & has_mosb
    flow[mask_3] = 3
    flow_desc[mask_3] = "Flow 3: Port â†’ MOSB â†’ Site"

    # Flow 4: Port â†’ WH â†’ MOSB â†’ Site (WHâ‰¥1, MOSB=1)
    mask_4 = not_pre & (wh_cnt >= 1) & has_mosb
    flow[mask_4] = 4
    flow_desc[mask_4] = "Flow 4: Port â†’ WH â†’ MOSB â†’ Site"

    # Step 6: AGI/DAS ë„ë©”ì¸ ì˜¤ë²„ë¼ì´ë“œ
    df["FLOW_CODE_ORIG"] = flow.copy()
    df["FLOW_OVERRIDE_REASON"] = np.nan

    # Final_Location ì»¬ëŸ¼ ì°¾ê¸° (ì´ë¯¸ Step 2ì—ì„œ ìƒì„±í–ˆê±°ë‚˜ ì…ë ¥ ë°ì´í„°ì— ì¡´ì¬)
    final_col = 'Final_Location'

    if final_col in df.columns:
        final_location = df[final_col].astype(str).str.upper()
        is_agi_das = final_location.isin(["AGI", "DAS"])

        # AGI/DASê°€ 0/1/2ì¸ ê²½ìš° ê°•ì œ 3 ìŠ¹ê¸‰
        need_force = is_agi_das & flow.isin([0, 1, 2])
        flow[need_force] = 3
        flow_desc[need_force] = "Flow 3: Port â†’ MOSB â†’ Site (AGI/DAS forced)"
        df["FLOW_OVERRIDE_REASON"] = df["FLOW_OVERRIDE_REASON"].astype(object)
        df.loc[need_force, "FLOW_OVERRIDE_REASON"] = "AGI/DAS requires MOSB leg"

        if need_force.sum() > 0:
            logger.info(f" AGI/DAS ê°•ì œ ìŠ¹ê¸‰: {need_force.sum()}ê±´ (0/1/2 â†’ 3)")
    else:
        logger.warning("Final_Location ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ - AGI/DAS ê°•ì œ ìŠ¹ê¸‰ ë¶ˆê°€")

    # Step 7: í˜¼í•© ì¼€ì´ìŠ¤ ì²˜ë¦¬ (Flow 5)
    # ì¡°ê±´ 1: MOSB ìˆìœ¼ë‚˜ Site ì—†ìŒ
    cond_mosb_no_site = has_mosb & (~has_site)

    # ì¡°ê±´ 2: WH 2ê°œ ì´ìƒ + MOSB ì—†ìŒ
    cond_weird_wh = (wh_cnt >= 2) & (~has_mosb) & (~df['is_pre_arrival'])

    need_5 = cond_mosb_no_site | cond_weird_wh
    flow[need_5] = 5
    flow_desc[need_5] = "Flow 5: Mixed / Waiting / Incomplete leg"

    # Step 8: ìµœì¢… ë°˜ì˜
    df["FLOW_CODE"] = flow.astype("int64")
    df["FLOW_DESCRIPTION"] = flow_desc

    # ê²€ì¦ ë° ë¡œê¹…
    dist = df["FLOW_CODE"].value_counts().sort_index()
    logger.info(f"[FlowCode v3.5] ë¶„í¬: {dict(dist)}")
    logger.info(f" Pre Arrival: {df['is_pre_arrival'].sum()}ê±´")
    logger.info(" Flow Code ì¬ê³„ì‚° ì™„ë£Œ (v3.5: 0~5 í™•ì¥)")

    # ë²”ìœ„ ê²€ì¦
    invalid_codes = df[~df["FLOW_CODE"].isin([0, 1, 2, 3, 4, 5])]
    if len(invalid_codes) > 0:
        logger.error(f"âš ï¸ ì˜ëª»ëœ Flow Code ë°œê²¬: {invalid_codes['FLOW_CODE'].unique()}")

    return df


# ============================================================================
# CLI Interface
# ============================================================================

# ê¸°ë³¸ ì°½ê³ /ì‚¬ì´íŠ¸ ì»¬ëŸ¼ (HVDC í”„ë¡œì íŠ¸ í‘œì¤€)
DEFAULT_WAREHOUSE_COLS = [
    "DSV Indoor", "DSV Outdoor", "DSV MZD", "DSV MZP", "DSV Al Markaz",
    "JDN MZD", "JDN Waterfront",
    "MOSB",
    "AAA Storage", "Hauler DG Storage", "DHL WH", "Hauler Indoor",
    "Shifting", "ZENER (WH)", "Vijay Tanks"
]

DEFAULT_SITE_COLS = ["SHU", "MIR", "DAS", "AGI"]


def main():
    parser = argparse.ArgumentParser(
        description="Flow Code v3.5 Calculator - Calculate logistics flow codes from Excel data",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # CSV ì¶œë ¥ (ê¸°ë³¸)
  python scripts/core/flow_code_calc.py --input data/HVDC_STATUS.xlsx --output output/flow_codes.csv

  # JSON ì¶œë ¥
  python scripts/core/flow_code_calc.py --input data/HVDC_STATUS.xlsx --output output/flow_codes.json --format json

  # ì»¤ìŠ¤í…€ ì°½ê³  ì»¬ëŸ¼ ì§€ì •
  python scripts/core/flow_code_calc.py --input data.xlsx --output result.csv --warehouses "WH1,WH2,MOSB"

  # í†µê³„ë§Œ ì¶œë ¥ (íŒŒì¼ ì €ì¥ ì•ˆ í•¨)
  python scripts/core/flow_code_calc.py --input data.xlsx --stats-only
        """
    )

    parser.add_argument(
        '--input', '-i',
        required=True,
        help='Input Excel file path (e.g., data/HVDC_STATUS.xlsx)'
    )
    parser.add_argument(
        '--output', '-o',
        help='Output file path (CSV or JSON)'
    )
    parser.add_argument(
        '--format', '-f',
        choices=['csv', 'json'],
        default='csv',
        help='Output format (default: csv)'
    )
    parser.add_argument(
        '--warehouses',
        help='Comma-separated warehouse column names (default: HVDC standard)'
    )
    parser.add_argument(
        '--sites',
        help='Comma-separated site column names (default: SHU,MIR,DAS,AGI)'
    )
    parser.add_argument(
        '--sheet',
        default=0,
        help='Excel sheet name or index (default: 0 = first sheet)'
    )
    parser.add_argument(
        '--stats-only',
        action='store_true',
        help='Print statistics only, do not save file'
    )
    parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        help='Verbose output'
    )

    args = parser.parse_args()

    # ë¡œê¹… ë ˆë²¨ ì„¤ì •
    if args.verbose:
        logger.setLevel(logging.DEBUG)

    # íŒŒì¼ í™•ì¸
    input_path = Path(args.input)
    if not input_path.exists():
        logger.error(f"âŒ Input file not found: {input_path}")
        sys.exit(1)

    # ì°½ê³ /ì‚¬ì´íŠ¸ ì»¬ëŸ¼ íŒŒì‹±
    warehouse_cols = DEFAULT_WAREHOUSE_COLS
    if args.warehouses:
        warehouse_cols = [w.strip() for w in args.warehouses.split(',')]

    site_cols = DEFAULT_SITE_COLS
    if args.sites:
        site_cols = [s.strip() for s in args.sites.split(',')]

    logger.info(f"ğŸ“‚ Loading: {input_path}")
    logger.info(f"   Warehouse columns: {len(warehouse_cols)} configured")
    logger.info(f"   Site columns: {site_cols}")

    # Excel ë¡œë“œ
    try:
        if isinstance(args.sheet, int) or args.sheet.isdigit():
            sheet = int(args.sheet)
        else:
            sheet = args.sheet
        df = pd.read_excel(input_path, sheet_name=sheet)
        logger.info(f"âœ… Loaded {len(df)} rows, {len(df.columns)} columns")
    except Exception as e:
        logger.error(f"âŒ Failed to load Excel: {e}")
        sys.exit(1)

    # Flow Code ê³„ì‚°
    try:
        logger.info("ğŸ”„ Calculating Flow Code v3.5...")
        df_result = calculate_flow_code_v35(df, warehouse_cols, site_cols)
        logger.info("âœ… Calculation complete")
    except Exception as e:
        logger.error(f"âŒ Flow Code calculation failed: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

    # í†µê³„ ì¶œë ¥
    print("\n" + "="*60)
    print("ğŸ“Š Flow Code v3.5 Distribution")
    print("="*60)
    dist = df_result['FLOW_CODE'].value_counts().sort_index()
    for code, count in dist.items():
        pct = count / len(df_result) * 100
        desc = df_result[df_result['FLOW_CODE'] == code]['FLOW_DESCRIPTION'].iloc[0]
        print(f"  Flow {code}: {count:5d} cases ({pct:5.1f}%)  {desc}")

    print(f"\n  Pre Arrival: {df_result['is_pre_arrival'].sum()} cases")

    override_count = df_result['FLOW_OVERRIDE_REASON'].notna().sum()
    if override_count > 0:
        print(f"  Overrides:   {override_count} cases (AGI/DAS forced)")

    print("="*60 + "\n")

    # íŒŒì¼ ì €ì¥
    if not args.stats_only:
        if not args.output:
            logger.error("âŒ --output required (or use --stats-only)")
            sys.exit(1)

        output_path = Path(args.output)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        try:
            if args.format == 'json':
                # JSON ì¶œë ¥ (ë ˆì½”ë“œ í˜•ì‹)
                result_json = df_result.to_dict(orient='records')
                with open(output_path, 'w', encoding='utf-8') as f:
                    json.dump(result_json, f, indent=2, ensure_ascii=False, default=str)
                logger.info(f"âœ… Saved to: {output_path} (JSON format)")
            else:
                # CSV ì¶œë ¥
                df_result.to_csv(output_path, index=False, encoding='utf-8-sig')
                logger.info(f"âœ… Saved to: {output_path} (CSV format)")
        except Exception as e:
            logger.error(f"âŒ Failed to save output: {e}")
            sys.exit(1)

    logger.info("ğŸ‰ Flow Code calculation complete!")


if __name__ == "__main__":
    main()

