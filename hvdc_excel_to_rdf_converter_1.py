#!/usr/bin/env python3
"""
HVDC Excel to RDF Converter
Excel íŒŒì¼ì„ RDF/TTL í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""

import pandas as pd
import numpy as np
from rdflib import Graph, Namespace, URIRef, Literal, RDF, RDFS, OWL, XSD
from rdflib.plugins.sparql import prepareQuery
import json
import os
from pathlib import Path
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì •ì˜
EX = Namespace("http://samsung.com/project-logistics#")
ns = {
    "ex": "http://samsung.com/project-logistics#",
    "rdf": "http://www.w3.org/1999/02/22-rdf-syntax-ns#",
    "rdfs": "http://www.w3.org/2000/01/rdf-schema#",
    "owl": "http://www.w3.org/2002/07/owl#",
    "xsd": "http://www.w3.org/2001/XMLSchema#"
}

# ë§¤í•‘ ê·œì¹™ ì •ì˜
FIELD_MAPPINGS = {
    "Case No.": "hasCase",
    "Case_No": "hasCase",
    "no.": "hasSequenceNumber",
    "Shipment Invoice No.": "hasShipmentInvoiceNumber",
    "HVDC CODE": "hasHVDCCode",
    "HVDC CODE 1": "hasHVDCCode1",
    "HVDC CODE 2": "hasHVDCCode2",
    "HVDC CODE 3": "hasHVDCCode3",
    "HVDC CODE 4": "hasHVDCCode4",
    "HVDC CODE 5": "hasHVDCCode5",
    "EQ No": "hasEquipmentNumber",
    "Pkg": "hasPackageCount",
    "Storage": "hasStorageType",
    "Description": "hasDescription",
    "L(CM)": "hasLength",
    "W(CM)": "hasWidth",
    "H(CM)": "hasHeight",
    "CBM": "hasCubicMeter",
    "N.W(kgs)": "hasNetWeight",
    "G.W(kgs)": "hasGrossWeight",
    "Stack": "hasStackInfo",
    "ETA": "hasETA",
    "ETD": "hasETD",
    "Date": "hasDate",
    "Operation Month": "hasOperationMonth",
    "DHL Warehouse": "hasDHLWarehouse",
    "DSV Indoor": "hasDSVIndoor",
    "DSV Al Markaz": "hasDSVAlMarkaz",
    "DSV Outdoor": "hasDSVOutdoor",
    "AAA  Storage": "hasAAAStorage",
    "Hauler Indoor": "hasHaulerIndoor",
    "DSV MZP": "hasDSVMZP",
    "MOSB": "hasMOSB",
    "Shifting": "hasShifting",
    "DAS": "hasDAS",
    "AGI": "hasAGI",
    "SHU": "hasSHU",
    "MIR": "hasMIR",
    "Logistics Flow Code": "hasLogisticsFlowCode",
    "Flow_Code": "hasFlowCode",
    "Status": "hasStatus",
    "Status_Location": "hasStatusLocation",
    "Status_Location_Date": "hasStatusLocationDate",
    "Status_Current": "hasCurrentStatus",
    "Status_Storage": "hasStorageStatus",
    "wh handling": "hasWHHandling"
}

def load_ontology_schema():
    """ì˜¨í†¨ë¡œì§€ ìŠ¤í‚¤ë§ˆ ë¡œë“œ"""
    g = Graph()
    
    # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë°”ì¸ë”©
    for prefix, uri in ns.items():
        g.bind(prefix, Namespace(uri))
    
    # ìŠ¤í‚¤ë§ˆ íŒŒì¼ì´ ìˆë‹¤ë©´ ë¡œë“œ
    if Path("hvdc_integrated_ontology_schema.ttl").exists():
        try:
            g.parse("hvdc_integrated_ontology_schema.ttl", format="turtle")
            print(f"âœ… ì˜¨í†¨ë¡œì§€ ìŠ¤í‚¤ë§ˆ ë¡œë“œ ì™„ë£Œ: {len(g)} íŠ¸ë¦¬í”Œ")
        except Exception as e:
            print(f"âš ï¸ ìŠ¤í‚¤ë§ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    else:
        print("âš ï¸ ìŠ¤í‚¤ë§ˆ íŒŒì¼ ì—†ìŒ, ê¸°ë³¸ ìŠ¤í‚¤ë§ˆ ìƒì„±")
        create_default_schema(g)
    
    return g

def create_default_schema(g):
    """ê¸°ë³¸ ì˜¨í†¨ë¡œì§€ ìŠ¤í‚¤ë§ˆ ìƒì„±"""
    # ê¸°ë³¸ í´ë˜ìŠ¤ ì •ì˜
    classes = [
        (EX.TransportEvent, "ìš´ì†¡ ì´ë²¤íŠ¸"),
        (EX.Warehouse, "ì°½ê³ "),
        (EX.Site, "í˜„ì¥"),
        (EX.HitachiCargo, "íˆíƒ€ì¹˜ í™”ë¬¼"),
        (EX.SiemensCargo, "ì§€ë©˜ìŠ¤ í™”ë¬¼"),
        (EX.Case, "ì¼€ì´ìŠ¤"),
        (EX.Item, "ì•„ì´í…œ")
    ]
    
    for class_uri, label in classes:
        g.add((class_uri, RDF.type, OWL.Class))
        g.add((class_uri, RDFS.label, Literal(label, lang='ko')))
    
    # ê¸°ë³¸ ì†ì„± ì •ì˜
    properties = [
        (EX.hasCase, "ì¼€ì´ìŠ¤ ë²ˆí˜¸"),
        (EX.hasDate, "ë‚ ì§œ"),
        (EX.hasLocation, "ìœ„ì¹˜"),
        (EX.hasQuantity, "ìˆ˜ëŸ‰"),
        (EX.hasAmount, "ê¸ˆì•¡"),
        (EX.hasCubicMeter, "ì²´ì "),
        (EX.hasWeight, "ì¤‘ëŸ‰"),
        (EX.hasHVDCCode, "HVDC ì½”ë“œ"),
        (EX.hasDescription, "ì„¤ëª…")
    ]
    
    for prop_uri, label in properties:
        g.add((prop_uri, RDF.type, OWL.DatatypeProperty))
        g.add((prop_uri, RDFS.label, Literal(label, lang='ko')))

def normalize_code_num(code):
    """ì½”ë“œ ë²ˆí˜¸ ì •ê·œí™”"""
    if pd.isna(code):
        return None
    return str(code).strip().lstrip('0') or '0'

def is_valid_hvdc_vendor(code, valid_codes=['HE', 'SIM']):
    """ìœ íš¨í•œ HVDC ë²¤ë” ì½”ë“œì¸ì§€ í™•ì¸"""
    if pd.isna(code):
        return False
    return str(code).strip().upper() in valid_codes

def preprocess_dataframe(df, source_file):
    """ë°ì´í„°í”„ë ˆì„ ì „ì²˜ë¦¬"""
    print(f"ğŸ“Š ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘: {source_file}")
    
    original_count = len(df)
    
    # 1. ì—´ ì´ë¦„ ì •ê·œí™”
    df.columns = df.columns.str.strip()
    
    # 2. ë‚ ì§œ ì»¬ëŸ¼ ë³€í™˜
    date_columns = [col for col in df.columns if 'Date' in col or 'ETA' in col or 'ETD' in col]
    for col in date_columns:
        if col in df.columns:
            df[col] = pd.to_datetime(df[col], errors='coerce')
    
    # 3. ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ë³€í™˜
    numeric_columns = ['CBM', 'N.W(kgs)', 'G.W(kgs)', 'L(CM)', 'W(CM)', 'H(CM)', 'Pkg']
    for col in numeric_columns:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')
    
    # 4. CBM ì–‘ìˆ˜ ë³´ì •
    if 'CBM' in df.columns:
        cbm_violations = (df['CBM'] <= 0).sum()
        if cbm_violations > 0:
            mean_cbm = df[df['CBM'] > 0]['CBM'].mean()
            df.loc[df['CBM'] <= 0, 'CBM'] = mean_cbm
            print(f"  âœ… CBM ìœ„ë°˜ {cbm_violations}ê±´ â†’ í‰ê· ê°’ {mean_cbm:.2f}ë¡œ ë³´ì •")
    
    # 5. íŒ¨í‚¤ì§€ ìˆ˜ ë³´ì •
    if 'Pkg' in df.columns:
        df['Pkg'] = df['Pkg'].fillna(1)
        df.loc[df['Pkg'] <= 0, 'Pkg'] = 1
    
    # 6. HVDC CODE 3 í•„í„°ë§
    if 'HVDC CODE 3' in df.columns:
        valid_vendor_mask = df['HVDC CODE 3'].apply(is_valid_hvdc_vendor)
        df = df[valid_vendor_mask]
        filtered_count = len(df)
        print(f"  âœ… ë²¤ë” í•„í„°ë§: {original_count} â†’ {filtered_count}")
    
    # 7. ì¤‘ë³µ ì œê±°
    if 'Case No.' in df.columns:
        df = df.drop_duplicates(subset=['Case No.'])
        final_count = len(df)
        print(f"  âœ… ì¤‘ë³µ ì œê±°: {filtered_count if 'HVDC CODE 3' in df.columns else original_count} â†’ {final_count}")
    
    # 8. ë°ì´í„° ì†ŒìŠ¤ ì¶”ê°€
    df['data_source'] = source_file.replace('.xlsx', '')
    
    print(f"ğŸ“Š ì „ì²˜ë¦¬ ì™„ë£Œ: {original_count} â†’ {len(df)} ({len(df)/original_count*100:.1f}%)")
    
    return df

def create_rdf_graph(df, source_file):
    """DataFrameì„ RDF ê·¸ë˜í”„ë¡œ ë³€í™˜"""
    print(f"ğŸ”— RDF ê·¸ë˜í”„ ìƒì„± ì‹œì‘: {source_file}")
    
    # ì˜¨í†¨ë¡œì§€ ìŠ¤í‚¤ë§ˆ ë¡œë“œ
    g = load_ontology_schema()
    
    # ë°ì´í„° ë³€í™˜
    for idx, row in df.iterrows():
        # TransportEvent URI ìƒì„±
        case_no = str(row.get('Case No.', f"case_{idx+1}")).replace(' ', '_')
        event_uri = EX[f"TransportEvent_{case_no}"]
        
        # ê¸°ë³¸ í´ë˜ìŠ¤ ì¶”ê°€
        g.add((event_uri, RDF.type, EX.TransportEvent))
        
        # ë²¤ë”ë³„ í™”ë¬¼ í´ë˜ìŠ¤ ì¶”ê°€
        if 'HVDC CODE 3' in row and pd.notna(row['HVDC CODE 3']):
            vendor = str(row['HVDC CODE 3']).strip().upper()
            if vendor == 'HE':
                g.add((event_uri, RDF.type, EX.HitachiCargo))
            elif vendor == 'SIM':
                g.add((event_uri, RDF.type, EX.SiemensCargo))
        
        # ë°ì´í„° ì†ŒìŠ¤ ì¶”ê°€
        g.add((event_uri, EX.hasDataSource, Literal(source_file)))
        
        # ê° ì»¬ëŸ¼ì„ RDF ì†ì„±ìœ¼ë¡œ ë³€í™˜
        for col, value in row.items():
            if pd.isna(value) or col not in FIELD_MAPPINGS:
                continue
            
            property_uri = EX[FIELD_MAPPINGS[col]]
            
            # ë°ì´í„° íƒ€ì…ì— ë”°ë¥¸ Literal ìƒì„±
            if isinstance(value, (int, np.integer)):
                literal = Literal(int(value), datatype=XSD.integer)
            elif isinstance(value, (float, np.floating)):
                literal = Literal(float(value), datatype=XSD.decimal)
            elif isinstance(value, datetime):
                literal = Literal(value.date(), datatype=XSD.date)
            elif isinstance(value, pd.Timestamp):
                literal = Literal(value.date(), datatype=XSD.date)
            else:
                literal = Literal(str(value))
            
            g.add((event_uri, property_uri, literal))
    
    print(f"ğŸ”— RDF ê·¸ë˜í”„ ìƒì„± ì™„ë£Œ: {len(df)} ë ˆì½”ë“œ, {len(g)} íŠ¸ë¦¬í”Œ")
    
    return g

def save_rdf_file(graph, output_file):
    """RDF ê·¸ë˜í”„ë¥¼ TTL íŒŒì¼ë¡œ ì €ì¥"""
    print(f"ğŸ’¾ RDF íŒŒì¼ ì €ì¥: {output_file}")
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir = Path(output_file).parent
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # TTL íŒŒì¼ë¡œ ì €ì¥
    graph.serialize(destination=output_file, format="turtle")
    
    file_size = Path(output_file).stat().st_size
    print(f"âœ… RDF íŒŒì¼ ì €ì¥ ì™„ë£Œ: {file_size:,} bytes")

def validate_rdf_graph(graph):
    """RDF ê·¸ë˜í”„ ìœ íš¨ì„± ê²€ì¦"""
    print("ğŸ” RDF ê·¸ë˜í”„ ê²€ì¦ ì‹œì‘...")
    
    # ê¸°ë³¸ í†µê³„
    total_triples = len(graph)
    
    # í´ë˜ìŠ¤ë³„ ì¸ìŠ¤í„´ìŠ¤ ìˆ˜
    query = """
    SELECT ?class (COUNT(?instance) AS ?count)
    WHERE {
        ?instance rdf:type ?class .
        FILTER(STRSTARTS(STR(?class), "http://samsung.com/project-logistics#"))
    }
    GROUP BY ?class
    ORDER BY DESC(?count)
    """
    
    results = graph.query(query)
    
    print(f"ğŸ“Š ê²€ì¦ ê²°ê³¼:")
    print(f"  - ì´ íŠ¸ë¦¬í”Œ ìˆ˜: {total_triples:,}")
    print(f"  - í´ë˜ìŠ¤ë³„ ì¸ìŠ¤í„´ìŠ¤:")
    
    for row in results:
        class_name = str(row[0]).split('#')[-1]
        count = int(row[1])
        print(f"    â€¢ {class_name}: {count:,}ê°œ")
    
    # ë°ì´í„° í’ˆì§ˆ ê²€ì¦
    quality_checks = [
        ("CBM > 0", "?event ex:hasCubicMeter ?cbm . FILTER(?cbm > 0)"),
        ("íŒ¨í‚¤ì§€ ìˆ˜ > 0", "?event ex:hasPackageCount ?pkg . FILTER(?pkg > 0)"),
        ("ì¼€ì´ìŠ¤ ë²ˆí˜¸ ì¡´ì¬", "?event ex:hasCase ?case . FILTER(STRLEN(?case) > 0)"),
        ("ë°ì´í„° ì†ŒìŠ¤ ì¡´ì¬", "?event ex:hasDataSource ?source . FILTER(STRLEN(?source) > 0)")
    ]
    
    print(f"  - í’ˆì§ˆ ê²€ì¦:")
    for check_name, check_query in quality_checks:
        count_query = f"SELECT (COUNT(*) AS ?count) WHERE {{ {check_query} }}"
        result = list(graph.query(count_query))
        if result:
            count = int(result[0][0])
            print(f"    â€¢ {check_name}: {count:,}ê°œ")
    
    return total_triples

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ HVDC Excel to RDF ë³€í™˜ ì‹œì‘")
    print("=" * 50)
    
    # ì…ë ¥ íŒŒì¼ ì •ì˜
    input_files = [
        "data/HVDC WAREHOUSE_HITACHI(HE).xlsx",
        "data/HVDC WAREHOUSE_SIMENSE(SIM).xlsx"
    ]
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir = Path("rdf_output")
    output_dir.mkdir(exist_ok=True)
    
    # í†µí•© ê·¸ë˜í”„ ìƒì„±
    combined_graph = Graph()
    
    # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë°”ì¸ë”©
    for prefix, uri in ns.items():
        combined_graph.bind(prefix, Namespace(uri))
    
    total_records = 0
    
    # ê° íŒŒì¼ ì²˜ë¦¬
    for input_file in input_files:
        if not Path(input_file).exists():
            print(f"âŒ íŒŒì¼ ì—†ìŒ: {input_file}")
            continue
        
        print(f"\nğŸ“ íŒŒì¼ ì²˜ë¦¬: {input_file}")
        
        # Excel íŒŒì¼ ì½ê¸°
        try:
            df = pd.read_excel(input_file, sheet_name='Case List')
            print(f"ğŸ“Š ë°ì´í„° ë¡œë“œ: {len(df)} ë ˆì½”ë“œ")
        except Exception as e:
            print(f"âŒ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
            continue
        
        # ë°ì´í„° ì „ì²˜ë¦¬
        df = preprocess_dataframe(df, Path(input_file).stem)
        
        # RDF ê·¸ë˜í”„ ìƒì„±
        graph = create_rdf_graph(df, Path(input_file).stem)
        
        # ê°œë³„ íŒŒì¼ ì €ì¥
        output_file = output_dir / f"{Path(input_file).stem}.ttl"
        save_rdf_file(graph, str(output_file))
        
        # í†µí•© ê·¸ë˜í”„ì— ì¶”ê°€
        combined_graph += graph
        
        total_records += len(df)
        
        print(f"âœ… {input_file} ì²˜ë¦¬ ì™„ë£Œ")
    
    # í†µí•© íŒŒì¼ ì €ì¥
    combined_output = output_dir / "HVDC_COMBINED.ttl"
    save_rdf_file(combined_graph, str(combined_output))
    
    # ìµœì¢… ê²€ì¦
    print(f"\nğŸ” ìµœì¢… ê²€ì¦")
    print("=" * 50)
    validate_rdf_graph(combined_graph)
    
    print(f"\nğŸ‰ ë³€í™˜ ì™„ë£Œ!")
    print(f"ğŸ“Š ì´ ì²˜ë¦¬ ë ˆì½”ë“œ: {total_records:,}")
    print(f"ğŸ’¾ ì¶œë ¥ íŒŒì¼:")
    print(f"  - í†µí•© íŒŒì¼: {combined_output}")
    for input_file in input_files:
        if Path(input_file).exists():
            output_file = output_dir / f"{Path(input_file).stem}.ttl"
            print(f"  - ê°œë³„ íŒŒì¼: {output_file}")
    
    print(f"\nğŸ”§ ì¶”ì²œ ëª…ë ¹ì–´:")
    print(f"  /validate-data comprehensive --sparql-rules")
    print(f"  /semantic-search --query='RDF conversion'")

if __name__ == "__main__":
    main() 