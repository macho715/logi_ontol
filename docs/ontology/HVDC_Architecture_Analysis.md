1. ì˜¨í†¨ë¡œì§€ ì•„í‚¤í…ì²˜ ë¶„ì„
1.1 ê³„ì¸µ êµ¬ì¡° (4-Level Hierarchy)
python# ==================== L0: Root Node ====================
HVDC_Project (ë£¨íŠ¸)
â”œâ”€ type: "root"
â”œâ”€ ontology_class: "Project"
â””â”€ color: #ff0000 (ë¹¨ê°• - ìµœìƒìœ„ ë…¸ë“œ)

# ==================== L1: System Layer ====================
â”œâ”€ JPT71_System (ê¸°ì¡´ ë¬¼ë¥˜ ì‹œìŠ¤í…œ)
â”‚  â”œâ”€ ontology_class: "System"
â”‚  â”œâ”€ color: #ff6b6b
â”‚  â””â”€ belongs_to â†’ HVDC_Project
â”‚
â”œâ”€ ABU_System (Abu Dhabi ì‹œìŠ¤í…œ)
â”‚  â”œâ”€ ontology_class: "System"
â”‚  â”œâ”€ color: #51cf66
â”‚  â””â”€ belongs_to â†’ HVDC_Project
â”‚
â””â”€ HVDC_Infrastructure (ì‹ ê·œ HVDC ì¸í”„ë¼)
   â”œâ”€ ontology_class: "System"
   â”œâ”€ color: #339af0
   â””â”€ belongs_to â†’ HVDC_Project

# ==================== L2: Location Nodes (8ê°œ) ====================
HVDC_Infrastructure
â”œâ”€ Ports (3ê°œ)
â”‚  â”œâ”€ ZAYED_PORT
â”‚  â”‚  â”œâ”€ type: "port"
â”‚  â”‚  â”œâ”€ ontology_class: "Location"
â”‚  â”‚  â”œâ”€ customs_code: "ADNOC 47150"
â”‚  â”‚  â””â”€ description: "ì¤‘ëŸ‰/ë²Œí¬ í™”ë¬¼ ì²˜ë¦¬í•­"
â”‚  â”‚
â”‚  â”œâ”€ KHALIFA_PORT
â”‚  â”‚  â”œâ”€ type: "port"
â”‚  â”‚  â”œâ”€ description: "ì»¨í…Œì´ë„ˆ ì „ìš©"
â”‚  â”‚  â””â”€ customs_code: ""
â”‚  â”‚
â”‚  â””â”€ JEBEL_ALI_PORT
â”‚     â”œâ”€ type: "port"
â”‚     â”œâ”€ description: "Free Zone"
â”‚     â””â”€ customs_code: "ADOPT"
â”‚
â”œâ”€ Hub (1ê°œ)
â”‚  â””â”€ MOSB
â”‚     â”œâ”€ type: "hub"
â”‚     â”œâ”€ ontology_class: "Location"
â”‚     â”œâ”€ operator: "ADNOC L&S"
â”‚     â”œâ”€ area_sqm: 20000
â”‚     â””â”€ sct_team: "SCT ë¬¼ë¥˜ë³¸ë¶€ ìƒì£¼"
â”‚
â”œâ”€ Onshore Sites (2ê°œ)
â”‚  â”œâ”€ MIR (Mirfa Site)
â”‚  â”‚  â”œâ”€ site_type: "onshore"
â”‚  â”‚  â””â”€ laydown_area_sqm: 35000
â”‚  â”‚
â”‚  â””â”€ SHU (Shuweihat Site)
â”‚     â”œâ”€ site_type: "onshore"
â”‚     â””â”€ laydown_area_sqm: 10500
â”‚
â””â”€ Offshore Sites (2ê°œ)
   â”œâ”€ DAS (Das Island)
   â”‚  â”œâ”€ site_type: "offshore"
   â”‚  â””â”€ voyage_from_mosb_hours: 20
   â”‚
   â””â”€ AGI (Al Ghallan Island)
      â”œâ”€ site_type: "offshore"
      â””â”€ voyage_from_mosb_hours: 10

# ==================== L3: Entity Nodes ====================
â”œâ”€ Assets (Vessels, Cargo, Equipment)
â”‚  â””â”€ ontology_class: "Asset"
â”‚
â”œâ”€ Parties (Persons, Organizations)
â”‚  â”œâ”€ SCT_Logistics_Team
â”‚  â”‚  â”œâ”€ ontology_class: "Party"
â”‚  â”‚  â”œâ”€ organization: "Samsung C&T"
â”‚  â”‚  â””â”€ location: "MOSB"
â”‚  â”‚
â”‚  â””â”€ ADNOC_LS
â”‚     â”œâ”€ ontology_class: "Party"
â”‚     â””â”€ organization: "ADNOC"
â”‚
â”œâ”€ Processes (Operations)
â”‚  â””â”€ ontology_class: "Process"
â”‚
â””â”€ Events (Messages, Timetags)
   â””â”€ ontology_class: "Event"
1.2 ì˜¨í†¨ë¡œì§€ í´ë˜ìŠ¤ ë§¤í•‘
python# Palantir Foundry Object Typesì™€ ë§¤í•‘
ONTOLOGY_CLASS_MAPPING = {
    "port":      "Location",    # Port â†’ Location (RDF: hvdc:Port rdfs:subClassOf hvdc:Location)
    "hub":       "Location",    # Hub â†’ Location
    "site":      "Location",    # Site â†’ Location
    "vessel":    "Asset",       # Vessel â†’ Asset
    "person":    "Party",       # Person â†’ Party
    "operation": "Process",     # Operation â†’ Process
    "cargo":     "Asset",       # Cargo â†’ Asset
    "equipment": "Asset",       # Equipment â†’ Asset
    "document":  "Document",    # Document â†’ Document
    "permit":    "Document",    # Permit â†’ Document (ê·œì œ ì„œë¥˜)
    "timetag":   "Event",       # TimeTag â†’ Event
    "message":   "Event",       # Message â†’ Event
}

# RDF/OWL í˜•ì‹ ë³€í™˜ ì˜ˆì‹œ
"""
@prefix hvdc: <http://ontology.hvdc.project/> .
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .

hvdc:Port rdf:type owl:Class ;
    rdfs:subClassOf hvdc:Location ;
    rdfs:label "Port Node"@en ;
    rdfs:comment "Maritime port for cargo operations"@en .

hvdc:ZAYED_PORT rdf:type hvdc:Port ;
    hvdc:customsCode "ADNOC 47150" ;
    hvdc:description "ì¤‘ëŸ‰/ë²Œí¬ í™”ë¬¼ ì²˜ë¦¬í•­"@ko .
"""

ğŸ”— 2. ê´€ê³„ íƒ€ì… (12ì¢… Relationships)
2.1 êµ¬ì¡°ì  ê´€ê³„ (Structural)
python# 1. belongs_to (ì†Œì† ê´€ê³„)
# ìš©ë„: ê³„ì¸µ êµ¬ì¡° ì •ì˜
HVDC_Project â†’ JPT71_System, ABU_System, HVDC_Infrastructure
HVDC_Infrastructure â†’ ZAYED_PORT, KHALIFA_PORT, JEBEL_ALI_PORT, MOSB, MIR, SHU, DAS, AGI

# RDF í‘œí˜„:
# hvdc:JPT71_System hvdc:belongsTo hvdc:HVDC_Project .
2.2 ìš´ì˜ ê´€ê³„ (Operational)
python# 2. feeds_into (ë¬¼ë¥˜ íë¦„)
ZAYED_PORT â†’ MOSB
KHALIFA_PORT â†’ MOSB
JEBEL_ALI_PORT â†’ MOSB

# 3. dispatches (ë°°ì†¡)
MOSB â†’ MIR, SHU, DAS, AGI

# 4. connected_to (ì—°ê²°)
DAS â†” AGI  # ì„¬ ê°„ ì—°ê²°

# 5. hosts (í˜¸ìŠ¤íŒ…)
MOSB â†’ SCT_Logistics_Team

# 6. governed_by (ê±°ë²„ë„ŒìŠ¤)
MOSB â†’ ADNOC_LS
2.3 ì—”í‹°í‹° ê´€ê³„ (Entity)
python# 7. operates (ìš´ì˜)
Person â†’ Vessel

# 8. works_at (ê·¼ë¬´)
Person â†’ Location

# 9. performed (ìˆ˜í–‰)
Person â†’ Operation

# 10. uses (ì‚¬ìš©)
Operation â†’ Vessel

# 11. transported_by (ìš´ì†¡)
Cargo â†’ Vessel

# 12. stored_at (ë³´ê´€)
Cargo â†’ Location
2.4 ì‹œë§¨í‹± ê´€ê³„ (Semantic)
python# 13. same_as (ë™ì¼ ì—”í‹°í‹°)
# ìš©ë„: ì¤‘ë³µ ì œê±°, ì—”í‹°í‹° í•´ì†Œ
vessel:JPT71_A â†same_asâ†’ vessel:JPT71
person:John_Doe â†same_asâ†’ person:J.Doe

# êµ¬í˜„:
def build_identity_graph_hvdc(G):
    """same_as ë§í¬ë¡œ ì¤‘ë³µ ì—”í‹°í‹° í•´ì†Œ"""
    vessels = [n for n, d in G.nodes(data=True) if d.get("type") == "vessel"]
    for i, v1 in enumerate(vessels):
        for v2 in vessels[i + 1:]:
            # ì •ê·œí™” ë¹„êµ
            if normalize_name(v1) == normalize_name(v2):
                G.add_edge(v1, v2, rel="same_as", weight=1.0)
            # ìœ ì‚¬ë„ ë¹„êµ (85% ì´ìƒ)
            elif SequenceMatcher(None, v1, v2).ratio() >= 0.85:
                G.add_edge(v1, v2, rel="same_as", weight=0.9)

âœ… 3. ì˜¨í†¨ë¡œì§€ ê²€ì¦ (Validation)
3.1 ê²€ì¦ ë¡œì§
pythondef validate_hvdc_ontology(G: nx.Graph) -> dict:
    """HVDC v3.0 ì˜¨í†¨ë¡œì§€ ê·œì¹™ ì¤€ìˆ˜ ê²€ì¦"""

    results = {}

    # 1. í•„ìˆ˜ ë…¸ë“œ ì¡´ì¬ í™•ì¸ (8ê°œ HVDC ë…¸ë“œ)
    required_nodes = [
        "ZAYED_PORT", "KHALIFA_PORT", "JEBEL_ALI_PORT",  # 3 Ports
        "MOSB",                                            # 1 Hub
        "MIR", "SHU",                                      # 2 Onshore Sites
        "DAS", "AGI"                                       # 2 Offshore Sites
    ]

    hvdc_nodes_present = [n for n in required_nodes if n in G.nodes]
    results["hvdc_nodes_count"] = len(hvdc_nodes_present)
    results["hvdc_nodes_list"] = hvdc_nodes_present

    # ê²€ì¦ ê·œì¹™: 8/8 ë…¸ë“œ í•„ìˆ˜
    assert len(hvdc_nodes_present) == 8, "Missing HVDC nodes!"

    # 2. MOSB í—ˆë¸Œ ì—°ê²°ì„± ê²€ì¦
    # ê·œì¹™: MOSBëŠ” 3ê°œ Portë¡œë¶€í„° feeds_into ë°›ê³ , 4ê°œ Siteë¡œ dispatches
    mosb_incoming = [u for u, v in G.edges() if v == "MOSB"]
    mosb_outgoing = [v for u, v in G.edges() if u == "MOSB"]

    results["mosb_incoming"] = len(mosb_incoming)
    results["mosb_outgoing"] = len(mosb_outgoing)

    # ê²€ì¦ ê·œì¹™: MOSB incoming >= 3, outgoing >= 4
    assert len(mosb_incoming) >= 3, "MOSB must receive from 3+ ports"
    assert len(mosb_outgoing) >= 4, "MOSB must dispatch to 4+ sites"

    # 3. ê´€ê³„ íƒ€ì… ë‹¤ì–‘ì„± (ëª©í‘œ: 12ì¢…+)
    edge_types = set([d.get("rel") for u, v, d in G.edges(data=True)])
    results["edge_types_count"] = len(edge_types)
    results["edge_types_list"] = sorted(edge_types)

    # ê²€ì¦ ê·œì¹™: >= 12 ì¢…ë¥˜
    assert len(edge_types) >= 12, f"Need 12+ edge types, got {len(edge_types)}"

    # 4. ì˜¨í†¨ë¡œì§€ í´ë˜ìŠ¤ ì»¤ë²„ë¦¬ì§€
    ontology_classes = set([
        d.get("ontology_class")
        for n, d in G.nodes(data=True)
        if "ontology_class" in d
    ])
    results["ontology_classes"] = sorted(ontology_classes)

    # ê²€ì¦ ê·œì¹™: ìµœì†Œ 5ê°œ í´ë˜ìŠ¤ (Location, Asset, Party, Process, Event)
    required_classes = {"Location", "Asset", "Party", "Process", "Event"}
    assert required_classes.issubset(ontology_classes), "Missing core ontology classes"

    # 5. same_as ë§í¬ (ì¤‘ë³µ ì œê±° íš¨ê³¼)
    same_as_edges = [
        (u, v) for u, v, d in G.edges(data=True)
        if d.get("rel") == "same_as"
    ]
    results["same_as_links"] = len(same_as_edges)

    # 6. ë„¤íŠ¸ì›Œí¬ ë°€ë„ (í‰ê·  ì—°ê²°ë„)
    avg_degree = sum(dict(G.degree()).values()) / G.number_of_nodes()
    results["avg_degree"] = avg_degree

    # ê²€ì¦ ê·œì¹™: avg_degree >= 3.2 (ê³ ë°€ë„ ë„¤íŠ¸ì›Œí¬)
    assert avg_degree >= 3.2, f"Network too sparse: avg_degree={avg_degree:.2f}"

    # 7. ì „ì²´ í†µê³„
    results["total_nodes"] = G.number_of_nodes()
    results["total_edges"] = G.number_of_edges()

    return results
3.2 ê²€ì¦ ì¶œë ¥ ì˜ˆì‹œ
yaml[HVDC v3.0 ONTOLOGY VALIDATION]
============================================================
[OK] HVDC Nodes: 8/8
     ['ZAYED_PORT', 'KHALIFA_PORT', 'JEBEL_ALI_PORT',
      'MOSB', 'MIR', 'SHU', 'DAS', 'AGI']

[OK] MOSB Hub: 3 incoming, 6 outgoing
     - Incoming: ZAYED_PORT, KHALIFA_PORT, JEBEL_ALI_PORT
     - Outgoing: MIR, SHU, DAS, AGI, SCT_Logistics_Team, ADNOC_LS

[OK] Edge types: 13 (target: >=12)
     ['belongs_to', 'connected_to', 'dispatches', 'feeds_into',
      'governed_by', 'hosts', 'performed', 'same_as', 'suppliedBy',
      'transported_by', 'uses', 'works_at']

[OK] Ontology classes:
     ['Asset', 'Document', 'Event', 'Location', 'Party',
      'Process', 'Project', 'System']

[OK] Same_as links: 47
     - ì¤‘ë³µ vessel: 18ìŒ
     - ì¤‘ë³µ person: 12ìŒ
     - ì¤‘ë³µ port: 17ìŒ

[OK] Avg degree: 3.85 (target: >=3.2)

[OK] Total: 127 nodes, 245 edges
============================================================

ğŸ’¡ 4. Palantir Foundry ì—°ë™ ì „ëµ
4.1 NetworkX â†’ Palantir Ontology ë³€í™˜
pythondef convert_to_palantir_ontology(G: nx.Graph) -> dict:
    """
    NetworkX ê·¸ë˜í”„ë¥¼ Palantir Foundry Ontology JSONìœ¼ë¡œ ë³€í™˜
    """

    ontology = {
        "objectTypes": [],
        "linkTypes": [],
        "actionTypes": []
    }

    # 1. Object Types ìƒì„±
    node_types = {}
    for node, data in G.nodes(data=True):
        ontology_class = data.get("ontology_class", "Object")

        if ontology_class not in node_types:
            node_types[ontology_class] = {
                "apiName": ontology_class,
                "displayName": ontology_class,
                "pluralDisplayName": f"{ontology_class}s",
                "properties": []
            }

        # ì†ì„± ì¶”ì¶œ
        for key, value in data.items():
            if key not in ["type", "ontology_class", "label", "color", "level"]:
                property_def = {
                    "apiName": key,
                    "dataType": infer_datatype(value)
                }
                if property_def not in node_types[ontology_class]["properties"]:
                    node_types[ontology_class]["properties"].append(property_def)

    ontology["objectTypes"] = list(node_types.values())

    # 2. Link Types ìƒì„±
    link_types = {}
    for u, v, data in G.edges(data=True):
        rel = data.get("rel", "related_to")

        if rel not in link_types:
            source_class = G.nodes[u].get("ontology_class", "Object")
            target_class = G.nodes[v].get("ontology_class", "Object")

            link_types[rel] = {
                "apiName": rel,
                "displayName": rel.replace("_", " ").title(),
                "sourceObjectType": source_class,
                "targetObjectType": target_class,
                "cardinality": "MANY_TO_MANY"
            }

    ontology["linkTypes"] = list(link_types.values())

    return ontology

def infer_datatype(value):
    """ê°’ íƒ€ì… ì¶”ë¡ """
    if isinstance(value, bool):
        return "BOOLEAN"
    elif isinstance(value, int):
        return "INTEGER"
    elif isinstance(value, float):
        return "DOUBLE"
    elif isinstance(value, str):
        return "STRING"
    else:
        return "STRING"
4.2 Palantir Pipeline Builder ì—°ë™
pythondef create_palantir_pipeline(G: nx.Graph):
    """
    Foundry Pipeline Builder ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
    """

    pipeline_code = """
from transforms.api import transform, Input, Output
from pyspark.sql import functions as F

@transform(
    ontology_graph=Input("/path/to/networkx/graph"),
    hvdc_locations=Output("/HVDC/Ontology/Locations"),
    hvdc_assets=Output("/HVDC/Ontology/Assets"),
    hvdc_parties=Output("/HVDC/Ontology/Parties")
)
def build_hvdc_ontology(ontology_graph, hvdc_locations, hvdc_assets, hvdc_parties):
    '''
    NetworkX ê·¸ë˜í”„ë¥¼ Palantir Object Typesë¡œ ë³€í™˜
    '''

    # Load NetworkX graph
    import networkx as nx
    G = nx.read_gpickle(ontology_graph.path)

    # Extract Location nodes
    locations = []
    for node, data in G.nodes(data=True):
        if data.get("ontology_class") == "Location":
            locations.append({
                "location_id": node,
                "name": data.get("label", node),
                "type": data.get("type"),
                "subtype": data.get("subtype"),
                "capacity": data.get("area_sqm") or data.get("laydown_area_sqm"),
                "operator": data.get("operator"),
                "customs_code": data.get("customs_code")
            })

    # Convert to Spark DataFrame
    locations_df = spark.createDataFrame(locations)
    hvdc_locations.write_dataframe(locations_df)

    # Similar for Assets and Parties...
"""

    return pipeline_code

ğŸš€ 5. ê°œì„  ì œì•ˆ (Enhanced Features)
5.1 ì¶”ë¡  ê·œì¹™ ì—”ì§„ ì¶”ê°€
pythondef add_inference_rules(G: nx.Graph) -> nx.Graph:
    """
    ì˜¨í†¨ë¡œì§€ ì¶”ë¡  ê·œì¹™ ì ìš©
    """

    # Rule 1: Transitive Property (ê°„ì ‘ ì—°ê²° ì¶”ë¡ )
    # IF Item :storedAt Location AND Location :belongsTo System
    # THEN Item :indirectlyBelongsTo System
    for item in [n for n, d in G.nodes(data=True) if d.get("type") == "item"]:
        location = get_connected_node(G, item, "storedAt")
        if location:
            system = get_connected_node(G, location, "belongs_to")
            if system:
                G.add_edge(item, system,
                          rel="indirectly_belongs_to",
                          weight=0.5,
                          inferred=True)

    # Rule 2: Cargo Flow Path (ë¬¼ë¥˜ ê²½ë¡œ ì¶”ë¡ )
    # IF Cargo :transported_by Vessel AND Vessel :operates Portâ†’MOSB
    # THEN Cargo :flows_through MOSB
    for cargo in [n for n, d in G.nodes(data=True) if d.get("type") == "cargo"]:
        vessel = get_connected_node(G, cargo, "transported_by")
        if vessel:
            ports = get_connected_nodes(G, vessel, "operates")
            for port in ports:
                if G.has_edge(port, "MOSB"):
                    G.add_edge(cargo, "MOSB",
                              rel="flows_through",
                              weight=0.7,
                              inferred=True)

    # Rule 3: Critical Path Detection (ì¤‘ìš” ê²½ë¡œ ê°ì§€)
    # IF Item :dependsOn Item AND dependency_depth > 3
    # THEN Item.riskLevel = "HIGH"
    for item in [n for n, d in G.nodes(data=True) if d.get("type") == "item"]:
        dependency_chain = get_dependency_chain(G, item)
        if len(dependency_chain) > 3:
            G.nodes[item]["riskLevel"] = "HIGH"
            G.nodes[item]["dependency_depth"] = len(dependency_chain)
            G.nodes[item]["inferred"] = True

    # Rule 4: Co-Location Clustering (ë™ì¼ ìœ„ì¹˜ ê·¸ë£¹)
    # IF Item_A :storedAt Location AND Item_B :storedAt Location
    # THEN Item_A :co_located_with Item_B
    location_groups = defaultdict(list)
    for item in [n for n, d in G.nodes(data=True) if d.get("type") == "item"]:
        location = get_connected_node(G, item, "storedAt")
        if location:
            location_groups[location].append(item)

    for location, items in location_groups.items():
        for i, item_a in enumerate(items):
            for item_b in items[i+1:]:
                G.add_edge(item_a, item_b,
                          rel="co_located_with",
                          weight=0.4,
                          inferred=True,
                          location=location)

    return G
5.2 SPARQL ì¿¼ë¦¬ ì§€ì›
pythondef add_sparql_query_support(G: nx.Graph):
    """
    NetworkX ê·¸ë˜í”„ì— SPARQL ì¿¼ë¦¬ ê¸°ëŠ¥ ì¶”ê°€
    """

    # RDF ë³€í™˜
    import rdflib
    from rdflib import Graph as RDFGraph, Namespace, Literal, URIRef

    rdf_graph = RDFGraph()
    HVDC = Namespace("http://ontology.hvdc.project/")

    # ë…¸ë“œë¥¼ RDF íŠ¸ë¦¬í”Œë¡œ ë³€í™˜
    for node, data in G.nodes(data=True):
        node_uri = HVDC[node.replace(" ", "_")]

        # rdf:type
        ontology_class = data.get("ontology_class", "Object")
        rdf_graph.add((node_uri, RDF.type, HVDC[ontology_class]))

        # ì†ì„±
        for key, value in data.items():
            if key not in ["type", "ontology_class"]:
                pred = HVDC[key]
                if isinstance(value, str):
                    rdf_graph.add((node_uri, pred, Literal(value)))
                elif isinstance(value, (int, float)):
                    rdf_graph.add((node_uri, pred, Literal(value)))

    # ì—£ì§€ë¥¼ RDF íŠ¸ë¦¬í”Œë¡œ ë³€í™˜
    for u, v, data in G.edges(data=True):
        u_uri = HVDC[u.replace(" ", "_")]
        v_uri = HVDC[v.replace(" ", "_")]
        rel = data.get("rel", "related_to")

        rdf_graph.add((u_uri, HVDC[rel], v_uri))

    # SPARQL ì¿¼ë¦¬ ì‹¤í–‰ í•¨ìˆ˜
    def query_sparql(sparql_query: str):
        """SPARQL ì¿¼ë¦¬ ì‹¤í–‰"""
        results = rdf_graph.query(sparql_query)
        return list(results)

    return rdf_graph, query_sparql

# ì‚¬ìš© ì˜ˆì‹œ
rdf_graph, query = add_sparql_query_support(G)

# ì¿¼ë¦¬: MOSBë¡œ dispatchesí•˜ëŠ” ëª¨ë“  ë…¸ë“œ
sparql = """
PREFIX hvdc: <http://ontology.hvdc.project/>

SELECT ?location ?label
WHERE {
    ?mosb rdf:type hvdc:Location .
    ?mosb hvdc:label "MOSB" .
    ?mosb hvdc:dispatches ?location .
    ?location hvdc:label ?label .
}
"""

results = query(sparql)
for row in results:
    print(f"Location: {row.location}, Label: {row.label}")
5.3 ì‹¤ì‹œê°„ ë°ì´í„° ì—…ë°ì´íŠ¸ íŒŒì´í”„ë¼ì¸
pythondef create_streaming_pipeline():
    """
    ì‹¤ì‹œê°„ ì˜¨í†¨ë¡œì§€ ì—…ë°ì´íŠ¸ íŒŒì´í”„ë¼ì¸
    """

    streaming_code = """
from kafka import KafkaConsumer
import json
import networkx as nx

# Kafka ì»¨ìŠˆë¨¸ ì„¤ì •
consumer = KafkaConsumer(
    'hvdc-events',
    bootstrap_servers=['localhost:9092'],
    value_deserializer=lambda m: json.loads(m.decode('utf-8'))
)

# ì˜¨í†¨ë¡œì§€ ê·¸ë˜í”„ ë¡œë“œ
G = nx.read_gpickle("unified_network_data_v12_hvdc.gpickle")

def update_ontology(event):
    '''ì´ë²¤íŠ¸ ê¸°ë°˜ ì˜¨í†¨ë¡œì§€ ì—…ë°ì´íŠ¸'''

    if event['type'] == 'item_moved':
        # ì•„ì´í…œ ìœ„ì¹˜ ë³€ê²½
        item_id = event['item_id']
        new_location = event['new_location']
        old_location = event['old_location']

        # ê¸°ì¡´ ë§í¬ ì œê±°
        if G.has_edge(item_id, old_location):
            G.remove_edge(item_id, old_location)

        # ìƒˆ ë§í¬ ì¶”ê°€
        G.add_edge(item_id, new_location,
                  rel="storedAt",
                  weight=1.0,
                  timestamp=event['timestamp'])

        # ì¶”ë¡  ê·œì¹™ ì¬ì‹¤í–‰
        G = add_inference_rules(G)

        # ê²€ì¦
        validate_hvdc_ontology(G)

        # ì €ì¥
        nx.write_gpickle(G, "unified_network_data_v12_hvdc.gpickle")

    elif event['type'] == 'new_shipment':
        # ìƒˆ ì„ ì  ë„ì°©
        shipment_id = event['shipment_id']
        items = event['items']
        port = event['arrival_port']

        for item in items:
            G.add_node(item['id'],
                      type='item',
                      ontology_class='Asset',
                      **item['properties'])

            G.add_edge(item['id'], shipment_id, rel='belongsTo')
            G.add_edge(item['id'], port, rel='arrivedAt')

# ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬
for message in consumer:
    event = message.value
    update_ontology(event)
    print(f"Updated ontology: {event['type']}")
"""

    return streaming_code

ğŸ“Š 6. ì½”ë“œ í’ˆì§ˆ ë° ê°œì„ ì 
6.1 í˜„ì¬ êµ¬í˜„ì˜ ê°•ì 
yamlâœ… Strengths:
  Architecture:
    - ëª…í™•í•œ 4-Level Hierarchy
    - RDF/OWL ìŠ¤íƒ€ì¼ ì‹œë§¨í‹± ëª¨ë¸
    - ê³„ì¸µì  ë…¸ë“œ êµ¬ì¡° (level 0-3)

  Data Quality:
    - same_as ê´€ê³„ë¡œ ì¤‘ë³µ ì œê±°
    - ì •ê·œí™” í•¨ìˆ˜ (normalize_name)
    - ìœ ì‚¬ë„ ê¸°ë°˜ ìë™ ë§¤ì¹­ (85% threshold)

  Validation:
    - í¬ê´„ì ì¸ ê²€ì¦ ë¡œì§
    - í•„ìˆ˜ ë…¸ë“œ/ê´€ê³„ í™•ì¸
    - ë„¤íŠ¸ì›Œí¬ ë°€ë„ ê²€ì¦

  Extensibility:
    - ì˜¨í†¨ë¡œì§€ í´ë˜ìŠ¤ ë§¤í•‘ í™•ì¥ ê°€ëŠ¥
    - ê´€ê³„ íƒ€ì… ì¶”ê°€ ìš©ì´
    - ëª¨ë“ˆí™”ëœ í•¨ìˆ˜ êµ¬ì¡°
6.2 ê°œì„  í•„ìš” ì˜ì—­
yamlâš ï¸ Areas for Improvement:

1. ì†ì„± ìŠ¤í‚¤ë§ˆ ë¶€ì¡±:
   Problem: ë…¸ë“œ ì†ì„±ì´ ë™ì ìœ¼ë¡œ ì¶”ê°€ë¨ (íƒ€ì… ë¶ˆëª…í™•)
   Solution: Pydantic ëª¨ë¸ë¡œ ì†ì„± ìŠ¤í‚¤ë§ˆ ì •ì˜

   # ê°œì„  ì½”ë“œ
   from pydantic import BaseModel, Field

   class LocationNode(BaseModel):
       id: str
       label: str
       type: Literal["port", "hub", "site"]
       ontology_class: str = "Location"
       capacity: Optional[int] = None
       operator: Optional[str] = None
       customs_code: Optional[str] = None

2. ì¶”ë¡  ê·œì¹™ ë¯¸êµ¬í˜„:
   Problem: same_asë§Œ ìˆê³  ë‹¤ë¥¸ ì¶”ë¡  ì—†ìŒ
   Solution: add_inference_rules() í•¨ìˆ˜ ì¶”ê°€ (ìœ„ 5.1 ì°¸ì¡°)

3. SPARQL ì§€ì› ë¶€ì¬:
   Problem: ë³µì¡í•œ ì¿¼ë¦¬ ë¶ˆê°€ëŠ¥
   Solution: RDFLib í†µí•© (ìœ„ 5.2 ì°¸ì¡°)

4. ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ ë¯¸ì§€ì›:
   Problem: ì •ì  ê·¸ë˜í”„ë§Œ ìƒì„±
   Solution: Kafka/Redis ìŠ¤íŠ¸ë¦¬ë° íŒŒì´í”„ë¼ì¸ (ìœ„ 5.3 ì°¸ì¡°)

5. Palantir ì§ì ‘ ì—°ë™ ë¶€ì¬:
   Problem: JSON ë‚´ë³´ë‚´ê¸°ë§Œ ìˆìŒ
   Solution: Foundry SDK ì‚¬ìš©í•œ ì§ì ‘ ì—…ë¡œë“œ

6. ì„±ëŠ¥ ìµœì í™” í•„ìš”:
   Problem: same_as O(nÂ²) ë¹„êµ
   Solution: Locality-Sensitive Hashing (LSH)

   # ê°œì„  ì½”ë“œ
   from datasketch import MinHash, MinHashLSH

   def fast_deduplication(G):
       lsh = MinHashLSH(threshold=0.85, num_perm=128)

       vessels = [n for n, d in G.nodes(data=True) if d.get("type") == "vessel"]

       for vessel in vessels:
           mh = MinHash(num_perm=128)
           for char in G.nodes[vessel]["label"]:
               mh.update(char.encode('utf8'))
           lsh.insert(vessel, mh)

       # ìœ ì‚¬ ë…¸ë“œ ì°¾ê¸° (O(n) ì‹œê°„)
       for vessel in vessels:
           mh = MinHash(num_perm=128)
           for char in G.nodes[vessel]["label"]:
               mh.update(char.encode('utf8'))

           similar = lsh.query(mh)
           for sim in similar:
               if sim != vessel:
                   G.add_edge(vessel, sim, rel="same_as", weight=0.9)
