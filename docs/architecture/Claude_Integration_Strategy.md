# ğŸ—ï¸ C:\logi_ontol â†” Claude ì‹œìŠ¤í…œ í†µí•© ë§ˆìŠ¤í„°í”Œëœ
## MACHO-GPT v3.4-mini Enhanced Integration Strategy

**ì‘ì„±ì¼**: 2025-10-24
**ë²„ì „**: 1.0
**í”„ë¡œì íŠ¸**: HVDC Project - Samsung C&T Logistics (ADNOCÂ·DSV Partnership)

---

## ğŸ“Š EXECUTIVE SUMMARY

C:\logi_ontolì€ HVDC í”„ë¡œì íŠ¸ë¥¼ ìœ„í•œ **ë¬¼ë¥˜ ì˜¨í†¨ë¡œì§€ ì‹œìŠ¤í…œ**ìœ¼ë¡œ, í˜„ì¬ Python ê¸°ë°˜ìœ¼ë¡œ êµ¬ì¶•ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë³¸ í†µí•© ì „ëµì€ ì´ë¥¼ Claudeì˜ ë„¤ì´í‹°ë¸Œ ë„êµ¬ë“¤ê³¼ ì™„ì „íˆ í†µí•©í•˜ì—¬ **ìë™í™”ëœ ì§€ëŠ¥í˜• ë¬¼ë¥˜ ê´€ë¦¬ ì‹œìŠ¤í…œ**ì„ êµ¬ì¶•í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.

### í•µì‹¬ ëª©í‘œ
1. **ì™„ì „ ìë™í™”**: logi_ontol â†’ Claude Tools â†’ Automated Workflows
2. **ì‹¤ì‹œê°„ OCR**: PDF/Image â†’ AI-OCR â†’ Knowledge Graph â†’ Decision
3. **ì§€ì‹ í†µí•©**: RDF Ontology + Claude RAG + Google Drive + Web Search
4. **ìš´ì˜ ìµœì í™”**: Manual 60%â†“, Error 85%â†“, Response Time 70%â†“

---

## ğŸ—ºï¸ í˜„ì¬ ì‹œìŠ¤í…œ êµ¬ì¡° ë¶„ì„

### C:\logi_ontol ë””ë ‰í† ë¦¬ ë§µ

```
C:\logi_ontol/ (117MB compressed)
â”œâ”€â”€ logiontology/                    # ğŸ¯ í•µì‹¬ ì˜¨í†¨ë¡œì§€ ì—”ì§„
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ core/                    # ë„ë©”ì¸ ëª¨ë¸ & ê³„ì•½
â”‚   â”‚   â”œâ”€â”€ mapping/                 # ì˜¨í†¨ë¡œì§€ ë§¤í•‘ (v2.6)
â”‚   â”‚   â”œâ”€â”€ validation/              # SHACL ìŠ¤í‚¤ë§ˆ ê²€ì¦
â”‚   â”‚   â”œâ”€â”€ ingest/                  # Excel ë°ì´í„° ìˆ˜ì§‘
â”‚   â”‚   â”œâ”€â”€ rdfio/                   # RDF ì½ê¸°/ì“°ê¸°
â”‚   â”‚   â”œâ”€â”€ reasoning/               # AI ì¶”ë¡  ì—”ì§„
â”‚   â”‚   â””â”€â”€ pipeline/                # ì›Œí¬í”Œë¡œìš° íŒŒì´í”„ë¼ì¸
â”‚   â”œâ”€â”€ configs/                     # âš™ï¸ ì„¤ì •
â”‚   â”‚   â”œâ”€â”€ mapping_rules.v2.6.yaml  # ë§¤í•‘ ê·œì¹™
â”‚   â”‚   â”œâ”€â”€ lightning_sparql_queries.sparql
â”‚   â”‚   â”œâ”€â”€ abu_sparql_queries.sparql
â”‚   â”‚   â””â”€â”€ shacl_shapes.ttl         # ê²€ì¦ ìŠ¤í‚¤ë§ˆ
â”‚   â”œâ”€â”€ tests/ (92% coverage)        # âœ… í…ŒìŠ¤íŠ¸
â”‚   â””â”€â”€ docs/                        # ğŸ“š ì•„í‚¤í…ì²˜ ë¬¸ì„œ
â”‚
â”œâ”€â”€ JPT71/                           # ğŸš¢ Jopetwil 71 ì„ ë°• ìš´ì˜ ë°ì´í„°
â”‚   â”œâ”€â”€ ADNOC-TR *.pdf              # ADNOC ìš´ì†¡ ë¬¸ì„œ (20+)
â”‚   â”œâ”€â”€ IMG-*.jpg (400+ files)      # WhatsApp í˜„ì¥ ì´ë¯¸ì§€
â”‚   â”œâ”€â”€ Manifest *.pdf              # ì„ ë°• ë§¤ë‹ˆí˜ìŠ¤íŠ¸
â”‚   â”œâ”€â”€ *ì„ ì›ëª…*.pdf                 # ì„ ì› ë¬¸ì„œ (HARJOT, SOHAN, JAGBIR ë“±)
â”‚   â””â”€â”€ WhatsApp ëŒ€í™”.txt/zip       # ìš´ì˜ ë¡œê·¸ (357KB, ì™„ì „í•œ ëŒ€í™” ê¸°ë¡)
â”‚
â”œâ”€â”€ HVDC Project Lightning/          # âš¡ Lightning ì„œë¸Œì‹œìŠ¤í…œ
â”‚   â”œâ”€â”€ whatsapp_output/
â”‚   â”œâ”€â”€ IMG-*.jpg (50+ files)       # í˜„ì¥ ì‚¬ì§„
â”‚   â”œâ”€â”€ Logistics_Entities__Summary_.csv
â”‚   â””â”€â”€ Guideline_HVDC_Project_lightning.md
â”‚
â”œâ”€â”€ data/                            # ğŸ“‚ ì…ë ¥ ë°ì´í„°
â”‚   â””â”€â”€ *.xlsx                       # Excel ë¬¼ë¥˜ ë°ì´í„°
â”‚
â”œâ”€â”€ output/                          # ğŸ“¤ RDF ì¶œë ¥
â”‚   â”œâ”€â”€ final/
â”‚   â”‚   â”œâ”€â”€ abu_final.ttl
â”‚   â”‚   â””â”€â”€ lightning_final.ttl
â”‚   â””â”€â”€ versions/                    # ë²„ì „ ì•„ì¹´ì´ë¸Œ
â”‚
â”œâ”€â”€ reports/                         # ğŸ“Š ì‹œìŠ¤í…œ ë³´ê³ ì„œ
â”‚   â”œâ”€â”€ final/
â”‚   â”‚   â”œâ”€â”€ SYSTEM_ARCHITECTURE_COMPREHENSIVE.md
â”‚   â”‚   â”œâ”€â”€ HVDC_MASTER_INTEGRATION_REPORT.md
â”‚   â”‚   â”œâ”€â”€ ABU_SYSTEM_ARCHITECTURE.md
â”‚   â”‚   â””â”€â”€ LIGHTNING_FINAL_INTEGRATION_REPORT.md
â”‚   â”œâ”€â”€ architecture/
â”‚   â”œâ”€â”€ analysis/
â”‚   â””â”€â”€ operations/
â”‚
â”œâ”€â”€ scripts/                         # ğŸ”§ ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ process_hvdc_excel.py
â”‚   â”œâ”€â”€ integrate_lightning_images.py
â”‚   â”œâ”€â”€ build_lightning_cross_references.py
â”‚   â”œâ”€â”€ generate_final_lightning_report.py
â”‚   â””â”€â”€ compare_abu_lightning.py
â”‚
â”œâ”€â”€ ontology_unified/                # ğŸ§¬ í†µí•© ì˜¨í†¨ë¡œì§€
â”œâ”€â”€ ABU/                             # ì•„ë¶€ë‹¤ë¹„ íŠ¹í™” ë°ì´í„°
â”œâ”€â”€ cursor_ontology_first_pack_v1/   # Cursor í†µí•© ì‹œë„ (v1)
â””â”€â”€ archive/                         # ì•„ì¹´ì´ë¸Œ

âš ï¸ ì ‘ê·¼ ì œì•½: C:\logi_ontolì€ Claude filesystem ë„êµ¬ë¡œ ì§ì ‘ ì ‘ê·¼ ë¶ˆê°€
âœ… í•´ê²° ë°©ì•ˆ: Windows-MCP PowerShell ë˜ëŠ” ì‹¬ë³¼ë¦­ ë§í¬/ë³µì‚¬ í•„ìš”
```

### ê¸°ì¡´ ì‹œìŠ¤í…œ ê°•ì 
- âœ… **92% í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€** - ì•ˆì •ì ì¸ ì½”ë“œë² ì´ìŠ¤
- âœ… **RDF ê¸°ë°˜ ì˜¨í†¨ë¡œì§€** - í‘œì¤€í™”ëœ ì§€ì‹ í‘œí˜„
- âœ… **SHACL ê²€ì¦** - ë°ì´í„° í’ˆì§ˆ ë³´ì¥
- âœ… **SPARQL ì¿¼ë¦¬** - ê°•ë ¥í•œ ì§€ì‹ ê²€ìƒ‰
- âœ… **ì‹¤ì œ ìš´ì˜ ë°ì´í„°** - JPT71, Lightning í”„ë¡œì íŠ¸
- âœ… **ì™„ì „í•œ ë¬¸ì„œí™”** - ì•„í‚¤í…ì²˜ ë³´ê³ ì„œ ì™„ë¹„

### ê¸°ì¡´ ì‹œìŠ¤í…œ í•œê³„
- âš ï¸ **ìˆ˜ë™ í”„ë¡œì„¸ìŠ¤** - ìŠ¤í¬ë¦½íŠ¸ ìˆ˜ë™ ì‹¤í–‰ í•„ìš”
- âš ï¸ **ì œí•œì  OCR** - PDF/ì´ë¯¸ì§€ ìë™ ì²˜ë¦¬ ë¶€ì¡±
- âš ï¸ **íŒŒí¸í™”ëœ ë°ì´í„°** - WhatsApp, Excel, PDF ë¶„ì‚°
- âš ï¸ **ë‹¨ë°©í–¥ ì›Œí¬í”Œë¡œìš°** - í”¼ë“œë°± ë£¨í”„ ë¶€ì¡±
- âš ï¸ **ì œí•œì  AI** - ì¶”ë¡  ì—”ì§„ í™œìš©ë„ ë‚®ìŒ

---

## ğŸ¯ í†µí•© ì „ëµ: 3-Phase Roadmap

### **PHASE 1: ë¸Œë¦¿ì§• & ì¸ë±ì‹± (2ì£¼)**
#### ëª©í‘œ: C:\logi_ontolì„ Claude ìƒíƒœê³„ë¡œ ì—°ê²°

**1.1 ë””ë ‰í† ë¦¬ ë¸Œë¦¿ì§•**

**Option A: ì‹¬ë³¼ë¦­ ë§í¬ ìƒì„± (ê¶Œì¥)**
```powershell
# ê´€ë¦¬ì ê¶Œí•œ PowerShellì—ì„œ ì‹¤í–‰
New-Item -ItemType SymbolicLink `
  -Path "C:\cursor-mcp\logi_ontol_link" `
  -Target "C:\logi_ontol"
```

**Option B: ìë™ ë™ê¸°í™” ìŠ¤í¬ë¦½íŠ¸**
```powershell
# C:\cursor-mcp\scripts\sync_logi_ontol.ps1
$source = "C:\logi_ontol"
$dest = "C:\cursor-mcp\logi_ontol_sync"
$exclude = @("node_modules", ".git", "__pycache__", "venv", ".ruff_cache")

# ë¯¸ëŸ¬ ë™ê¸°í™” (ë³€ê²½ì‚¬í•­ë§Œ ë³µì‚¬)
robocopy $source $dest /MIR /XD $exclude /MT:8 /LOG+:sync.log

# ìŠ¤ì¼€ì¤„ëŸ¬ ë“±ë¡ (ë§¤ ì‹œê°„ ìë™ ë™ê¸°í™”)
$trigger = New-ScheduledTaskTrigger -Once -At (Get-Date) -RepetitionInterval (New-TimeSpan -Hours 1)
$action = New-ScheduledTaskAction -Execute "PowerShell.exe" -Argument "-File C:\cursor-mcp\scripts\sync_logi_ontol.ps1"
Register-ScheduledTask -TaskName "LogiOntolSync" -Trigger $trigger -Action $action
```

**1.2 MCP ì„œë²„ í†µí•©**
```javascript
// C:\cursor-mcp\mcp\servers\logi-ontol-server.js
const MCPServer = require('@modelcontextprotocol/sdk/server/index.js');
const { exec } = require('child_process');
const fs = require('fs');

const server = new MCPServer({
  name: "logi-ontol",
  version: "1.0.0",
  capabilities: {
    tools: {
      "read_ontology": {
        description: "Read RDF ontology files (.ttl) from logi_ontol",
        inputSchema: {
          type: "object",
          properties: {
            path: { type: "string", description: "Relative path in logi_ontol" }
          },
          required: ["path"]
        }
      },
      "query_sparql": {
        description: "Execute SPARQL queries on HVDC knowledge graph",
        inputSchema: {
          type: "object",
          properties: {
            query: { type: "string", description: "SPARQL query" }
          },
          required: ["query"]
        }
      },
      "validate_shacl": {
        description: "Validate logistics data against SHACL shapes",
        inputSchema: {
          type: "object",
          properties: {
            data_uri: { type: "string" },
            shape_uri: { type: "string" }
          }
        }
      },
      "process_invoice": {
        description: "Process invoice PDF through OCR pipeline",
        inputSchema: {
          type: "object",
          properties: {
            pdf_path: { type: "string" }
          },
          required: ["pdf_path"]
        }
      }
    }
  }
});

// Tool implementations
server.setRequestHandler('tools/call', async (request) => {
  const { name, arguments: args } = request.params;

  switch(name) {
    case 'read_ontology':
      const ttlPath = `C:/logi_ontol/${args.path}`;
      return { content: fs.readFileSync(ttlPath, 'utf8') };

    case 'query_sparql':
      // Execute Python script with SPARQL query
      return new Promise((resolve) => {
        exec(`python C:/logi_ontol/scripts/query_sparql.py "${args.query}"`,
          (error, stdout) => {
            resolve({ results: JSON.parse(stdout) });
          });
      });

    case 'validate_shacl':
      // Run SHACL validation
      return new Promise((resolve) => {
        exec(`python -m logiontology.validation --data ${args.data_uri} --shape ${args.shape_uri}`,
          { cwd: 'C:/logi_ontol/logiontology' },
          (error, stdout) => {
            resolve({ validation: JSON.parse(stdout) });
          });
      });

    case 'process_invoice':
      // Trigger OCR pipeline
      return new Promise((resolve) => {
        exec(`python C:/cursor-mcp/hvdc_automation/ocr_pipeline.py "${args.pdf_path}"`,
          (error, stdout) => {
            resolve({ entities: JSON.parse(stdout) });
          });
      });
  }
});

server.listen();
```

**1.3 ì¸ë±ì‹± & ë©”íƒ€ë°ì´í„° ìƒì„±**
```python
# C:\cursor-mcp\scripts\index_logi_ontol.py
import os
import json
import hashlib
from pathlib import Path
from datetime import datetime

def create_comprehensive_index():
    """Create searchable index of logi_ontol contents"""
    index = {
        "version": "1.0",
        "timestamp": datetime.now().isoformat(),
        "summary": {
            "total_files": 0,
            "total_size_mb": 0,
            "ontologies": 0,
            "pdfs": 0,
            "images": 0,
            "scripts": 0
        },
        "ontologies": [],
        "scripts": [],
        "data_files": [],
        "reports": [],
        "operational_data": {
            "jpt71": {},
            "lightning": {}
        }
    }

    base_path = Path("C:/logi_ontol")

    # 1. Scan ontology files
    for ttl_file in (base_path / "output" / "final").glob("*.ttl"):
        file_info = {
            "path": str(ttl_file.relative_to(base_path)),
            "size": ttl_file.stat().st_size,
            "modified": datetime.fromtimestamp(ttl_file.stat().st_mtime).isoformat(),
            "checksum": calculate_checksum(ttl_file)
        }
        index["ontologies"].append(file_info)
        index["summary"]["total_files"] += 1
        index["summary"]["total_size_mb"] += file_info["size"] / (1024*1024)
        index["summary"]["ontologies"] += 1

    # 2. Scan JPT71 operational data
    jpt71_path = base_path / "JPT71"
    jpt71_index = {
        "pdfs": [],
        "images": [],
        "whatsapp_chat": None,
        "crew_documents": []
    }

    for pdf_file in jpt71_path.glob("*.pdf"):
        file_info = {
            "filename": pdf_file.name,
            "path": str(pdf_file.relative_to(base_path)),
            "size": pdf_file.stat().st_size,
            "type": classify_document(pdf_file.name)
        }

        if "ADNOC-TR" in pdf_file.name:
            file_info["document_type"] = "transport_request"
        elif "Manifest" in pdf_file.name:
            file_info["document_type"] = "manifest"
        elif pdf_file.name in ["HARJOT.pdf", "SOHAN.pdf", "JAGBIR.pdf"]:
            jpt71_index["crew_documents"].append(file_info)

        jpt71_index["pdfs"].append(file_info)
        index["summary"]["pdfs"] += 1

    # Count images
    jpt71_index["image_count"] = len(list(jpt71_path.glob("IMG-*.jpg")))
    index["summary"]["images"] += jpt71_index["image_count"]

    # WhatsApp chat
    chat_file = jpt71_path / "Jopetwil 71 Groupë‹˜ê³¼ì˜ WhatsApp ëŒ€í™”.txt"
    if chat_file.exists():
        jpt71_index["whatsapp_chat"] = {
            "path": str(chat_file.relative_to(base_path)),
            "size": chat_file.stat().st_size,
            "message_count": count_whatsapp_messages(chat_file)
        }

    index["operational_data"]["jpt71"] = jpt71_index

    # 3. Scan Lightning data
    lightning_path = base_path / "HVDC Project Lightning"
    lightning_index = {
        "csv_summary": None,
        "images": len(list(lightning_path.glob("IMG-*.jpg"))),
        "guideline": str(lightning_path / "Guideline_HVDC_Project_lightning (1).md")
    }

    csv_file = lightning_path / "Logistics_Entities__Summary_.csv"
    if csv_file.exists():
        lightning_index["csv_summary"] = {
            "path": str(csv_file.relative_to(base_path)),
            "size": csv_file.stat().st_size
        }

    index["operational_data"]["lightning"] = lightning_index
    index["summary"]["images"] += lightning_index["images"]

    # 4. Scan scripts
    scripts_path = base_path / "scripts"
    for py_file in scripts_path.glob("*.py"):
        index["scripts"].append({
            "name": py_file.name,
            "path": str(py_file.relative_to(base_path)),
            "purpose": extract_script_purpose(py_file)
        })
        index["summary"]["scripts"] += 1

    # 5. Scan reports
    reports_path = base_path / "reports" / "final"
    for md_file in reports_path.glob("*.md"):
        index["reports"].append({
            "title": md_file.stem,
            "path": str(md_file.relative_to(base_path)),
            "size": md_file.stat().st_size
        })

    # Save index
    output_path = Path("C:/cursor-mcp/logi_ontol_index.json")
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(index, f, indent=2, ensure_ascii=False)

    print(f"âœ… Index created successfully!")
    print(f"   Total files: {index['summary']['total_files']}")
    print(f"   Total size: {index['summary']['total_size_mb']:.2f} MB")
    print(f"   Ontologies: {index['summary']['ontologies']}")
    print(f"   PDFs: {index['summary']['pdfs']}")
    print(f"   Images: {index['summary']['images']}")
    print(f"   Scripts: {index['summary']['scripts']}")

    return index

def calculate_checksum(file_path):
    """Calculate MD5 checksum of file"""
    with open(file_path, 'rb') as f:
        return hashlib.md5(f.read()).hexdigest()

def classify_document(filename):
    """Classify document type based on filename"""
    filename_lower = filename.lower()
    if 'invoice' in filename_lower:
        return 'invoice'
    elif 'manifest' in filename_lower:
        return 'manifest'
    elif 'tr' in filename_lower or 'transport' in filename_lower:
        return 'transport_request'
    else:
        return 'general'

def count_whatsapp_messages(chat_file):
    """Count messages in WhatsApp chat export"""
    with open(chat_file, 'r', encoding='utf-8') as f:
        content = f.read()
    import re
    # Pattern: [DD/MM/YYYY, HH:MM:SS] Sender: Message
    pattern = r'\[\d{2}/\d{2}/\d{4}, \d{2}:\d{2}:\d{2}\]'
    return len(re.findall(pattern, content))

def extract_script_purpose(py_file):
    """Extract purpose from script docstring"""
    with open(py_file, 'r', encoding='utf-8') as f:
        lines = f.readlines()
        if len(lines) > 1 and '"""' in lines[0]:
            # Extract docstring
            for i, line in enumerate(lines[1:], 1):
                if '"""' in line:
                    return lines[1:i][0].strip()
    return py_file.stem.replace('_', ' ').title()

if __name__ == "__main__":
    create_comprehensive_index()
```

**ì‚°ì¶œë¬¼ (Phase 1):**
- âœ… C:\cursor-mcp\logi_ontol_sync/ (ë™ê¸°í™”ëœ ë°ì´í„°)
- âœ… logi_ontol_index.json (ê²€ìƒ‰ ì¸ë±ìŠ¤)
- âœ… MCP ì„œë²„ logi-ontol êµ¬ì„± ì™„ë£Œ
- âœ… ìë™ ë™ê¸°í™” ìŠ¤ì¼€ì¤„ëŸ¬ ë“±ë¡

---

### **PHASE 2: AI-OCR & ì§€ì‹ í†µí•© (3ì£¼)**
#### ëª©í‘œ: ë¹„ì •í˜• ë°ì´í„°ë¥¼ êµ¬ì¡°í™”ëœ ì§€ì‹ìœ¼ë¡œ ë³€í™˜

**2.1 AI-OCR íŒŒì´í”„ë¼ì¸ êµ¬ì¶•**

```python
# C:\cursor-mcp\hvdc_automation\ocr_pipeline.py
import os
import json
import base64
from pathlib import Path
from PIL import Image
import pytesseract
import anthropic
from pdf2image import convert_from_path

class LogisticsOCRPipeline:
    """
    Advanced OCR pipeline for logistics documents
    Target: 90%+ entity extraction accuracy
    """

    def __init__(self):
        self.client = anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
        self.confidence_threshold = 0.90

    def process_document(self, file_path):
        """
        Main processing pipeline
        PDF/Image â†’ Text + Entities â†’ RDF
        """
        print(f"ğŸ” Processing: {file_path}")

        # 1. Extract text based on file type
        if file_path.endswith('.pdf'):
            text, images = self.extract_pdf(file_path)
        else:
            text = self.extract_image(file_path)
            images = [file_path]

        # 2. Classify document type
        doc_type = self.classify_document(text, images[0] if images else None)
        print(f"ğŸ“„ Document type: {doc_type}")

        # 3. Extract entities using Claude
        entities = self.extract_entities_with_claude(text, doc_type, images)

        # 4. Validate extraction confidence
        confidence = self.calculate_confidence(entities)
        print(f"ğŸ“Š Confidence: {confidence:.2%}")

        # 5. Convert to RDF if high confidence
        if confidence >= self.confidence_threshold:
            rdf_data = self.convert_to_rdf(entities, doc_type)
            status = "âœ… AUTO_PROCESSED"
        else:
            rdf_data = None
            status = "âš ï¸ NEEDS_REVIEW"

        return {
            "file_path": file_path,
            "doc_type": doc_type,
            "entities": entities,
            "confidence": confidence,
            "rdf": rdf_data,
            "status": status
        }

    def extract_pdf(self, pdf_path):
        """Extract text and images from PDF"""
        # Convert PDF to images
        images = convert_from_path(pdf_path, dpi=300)

        # OCR each page
        text_parts = []
        for i, image in enumerate(images):
            text = pytesseract.image_to_string(image, lang='eng')
            text_parts.append(text)

        full_text = "\n\n".join(text_parts)

        # Save first page as image for Claude vision
        image_paths = []
        if images:
            img_path = f"/tmp/{Path(pdf_path).stem}_page1.jpg"
            images[0].save(img_path, 'JPEG')
            image_paths.append(img_path)

        return full_text, image_paths

    def extract_image(self, image_path):
        """Extract text from image using Tesseract"""
        image = Image.open(image_path)
        text = pytesseract.image_to_string(image, lang='eng')
        return text

    def classify_document(self, text, image_path=None):
        """Classify document type using Claude"""
        prompt = f"""
        Classify this logistics document into one of these types:
        - INVOICE
        - MANIFEST
        - TRANSPORT_REQUEST
        - DELIVERY_NOTE
        - BILL_OF_LADING
        - PACKING_LIST
        - OTHER

        Document text (first 500 chars):
        {text[:500]}

        Return only the document type.
        """

        response = self.client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=100,
            messages=[{"role": "user", "content": prompt}]
        )

        return response.content[0].text.strip()

    def extract_entities_with_claude(self, text, doc_type, images=None):
        """
        Extract structured entities using Claude
        Returns entities with confidence scores
        """

        # Prepare prompt based on document type
        if doc_type == "INVOICE":
            fields_to_extract = """
            - invoice_number
            - invoice_date
            - trn (Tax Registration Number)
            - supplier_name
            - customer_name
            - total_amount
            - currency
            - line_items (description, quantity, unit_price, amount)
            - payment_terms
            """
        elif doc_type == "TRANSPORT_REQUEST":
            fields_to_extract = """
            - request_number
            - date
            - vessel_name
            - origin_port
            - destination_port
            - eta (Estimated Time of Arrival)
            - etd (Estimated Time of Departure)
            - cargo_description
            - weight
            - container_numbers
            """
        else:
            fields_to_extract = """
            - document_number
            - date
            - relevant_parties (sender, receiver, etc.)
            - key_amounts
            - important_dates
            - container/shipment identifiers
            """

        prompt = f"""
        You are an expert logistics data extraction AI. Extract structured information from this {doc_type} document.

        Document text:
        {text}

        Fields to extract:
        {fields_to_extract}

        **CRITICAL REQUIREMENTS:**
        1. Return ONLY valid JSON
        2. Include a "confidence" score (0.0-1.0) for EACH field
        3. If a field is not found, set value to null and confidence to 0.0
        4. Use exact field names provided above
        5. For dates, use ISO format (YYYY-MM-DD)
        6. For amounts, extract as numbers without currency symbols

        Example response format:
        {{
          "invoice_number": {{"value": "INV-2025-001", "confidence": 0.95}},
          "invoice_date": {{"value": "2025-10-24", "confidence": 1.0}},
          "total_amount": {{"value": 15000.00, "confidence": 0.90}},
          ...
        }}

        DO NOT include any text outside the JSON object.
        """

        # Add image if available (for better accuracy)
        if images and len(images) > 0:
            with open(images[0], 'rb') as f:
                image_data = base64.b64encode(f.read()).decode('utf-8')

            content = [
                {
                    "type": "image",
                    "source": {
                        "type": "base64",
                        "media_type": "image/jpeg",
                        "data": image_data
                    }
                },
                {"type": "text", "text": prompt}
            ]
        else:
            content = prompt

        response = self.client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=4000,
            messages=[{"role": "user", "content": content}]
        )

        # Parse JSON response
        response_text = response.content[0].text

        # Remove markdown code blocks if present
        if "```json" in response_text:
            response_text = response_text.split("```json")[1].split("```")[0]
        elif "```" in response_text:
            response_text = response_text.split("```")[1].split("```")[0]

        entities = json.loads(response_text.strip())
        return entities

    def calculate_confidence(self, entities):
        """Calculate overall confidence score"""
        if not entities:
            return 0.0

        confidences = []
        for field, data in entities.items():
            if isinstance(data, dict) and 'confidence' in data:
                if data['value'] is not None:  # Only count non-null values
                    confidences.append(data['confidence'])

        if not confidences:
            return 0.0

        return sum(confidences) / len(confidences)

    def convert_to_rdf(self, entities, doc_type):
        """Convert extracted entities to RDF triples"""
        from rdflib import Graph, Namespace, Literal, URIRef
        from rdflib.namespace import RDF, XSD

        g = Graph()
        LOGI = Namespace("http://hvdc.samsung.com/logistics#")
        g.bind("logi", LOGI)

        # Create document URI
        doc_id = entities.get('invoice_number', {}).get('value') or \
                 entities.get('document_number', {}).get('value') or \
                 f"DOC_{hash(str(entities))}"

        doc_uri = URIRef(LOGI[doc_id.replace('-', '_')])

        # Add document type
        g.add((doc_uri, RDF.type, LOGI[doc_type]))

        # Add all extracted fields
        for field, data in entities.items():
            if isinstance(data, dict) and data.get('value') is not None:
                value = data['value']

                # Determine datatype
                if isinstance(value, (int, float)):
                    literal = Literal(value, datatype=XSD.decimal)
                elif field.endswith('_date') or field == 'date':
                    literal = Literal(value, datatype=XSD.date)
                else:
                    literal = Literal(value)

                g.add((doc_uri, LOGI[field], literal))
                g.add((doc_uri, LOGI[f"{field}_confidence"], Literal(data['confidence'], datatype=XSD.float)))

        return g.serialize(format='turtle')

# Integration with /logi-master invoice-audit
def audit_invoice_with_ocr(invoice_path):
    """
    Slash command integration: /logi-master invoice-audit
    """
    pipeline = LogisticsOCRPipeline()
    result = pipeline.process_document(invoice_path)

    if result["confidence"] >= 0.90:
        # High confidence - auto-update knowledge graph
        print("âœ… High confidence extraction - updating knowledge graph")

        # Save RDF to knowledge graph
        from rdflib import Graph
        kg = Graph()
        kg.parse(data=result["rdf"], format="turtle")

        # Merge with existing ontology
        abu_ontology = Graph()
        abu_ontology.parse("C:/logi_ontol/output/final/abu_final.ttl", format="turtle")
        abu_ontology += kg

        # Save updated ontology
        abu_ontology.serialize("C:/logi_ontol/output/final/abu_final.ttl", format="turtle")

        return {
            "status": "âœ… APPROVED - AUTO-UPDATED",
            "confidence": result["confidence"],
            "entities": {k: v['value'] for k, v in result["entities"].items() if v.get('value')}
        }
    else:
        # Low confidence - flag for manual review
        print(f"âš ï¸ Low confidence ({result['confidence']:.2%}) - manual review required")

        # Save to review queue
        review_path = f"C:/logi_ontol/output/review/{Path(invoice_path).stem}_review.json"
        os.makedirs(os.path.dirname(review_path), exist_ok=True)
        with open(review_path, 'w') as f:
            json.dump(result, f, indent=2)

        return {
            "status": "âš ï¸ REVIEW_REQUIRED",
            "confidence": result["confidence"],
            "review_file": review_path,
            "entities": result["entities"]
        }

if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1:
        result = audit_invoice_with_ocr(sys.argv[1])
        print(json.dumps(result, indent=2))
```

**2.2 WhatsApp ë°ì´í„° í†µí•©**

```python
# C:\cursor-mcp\hvdc_automation\whatsapp_processor.py
import re
import json
from datetime import datetime
from pathlib import Path
import anthropic

class WhatsAppLogisticsProcessor:
    """
    Process WhatsApp chat logs for logistics entities
    Extracts: Container numbers, ETAs, amounts, vessel names, etc.
    """

    def __init__(self):
        self.client = anthropic.Anthropic()
        self.entity_patterns = {
            'container': r'[A-Z]{4}\d{7}',
            'eta_date': r'ETA[:\s]*(\d{1,2}[/-]\d{1,2}[/-]\d{2,4})',
            'etd_date': r'ETD[:\s]*(\d{1,2}[/-]\d{1,2}[/-]\d{2,4})',
            'amount': r'AED\s*([\d,]+\.?\d*)',
            'invoice_ref': r'INV[-\s]?\d{4,}',
        }

    def parse_chat(self, chat_file_path):
        """Parse WhatsApp chat export"""
        with open(chat_file_path, 'r', encoding='utf-8') as f:
            content = f.read()

        # Extract messages with pattern: [DD/MM/YYYY, HH:MM:SS] Sender: Message
        pattern = r'\[(\d{2}/\d{2}/\d{4}, \d{2}:\d{2}:\d{2})\] ([^:]+): (.+?)(?=\n\[|\n$|$)'
        messages = re.findall(pattern, content, re.DOTALL)

        processed_messages = []
        for timestamp, sender, message in messages:
            # Skip system messages
            if '<Media omitted>' in message or 'changed the subject' in message:
                continue

            # Basic regex extraction
            basic_entities = self.extract_basic_entities(message)

            # Enhanced extraction with Claude for complex cases
            if self.requires_advanced_extraction(message):
                enhanced_entities = self.extract_with_claude(message)
                basic_entities.update(enhanced_entities)

            processed_messages.append({
                "timestamp": datetime.strptime(timestamp, "%d/%m/%Y, %H:%M:%S").isoformat(),
                "sender": sender.strip(),
                "message": message.strip(),
                "entities": basic_entities
            })

        return processed_messages

    def extract_basic_entities(self, text):
        """Extract entities using regex patterns"""
        entities = {}

        for entity_type, pattern in self.entity_patterns.items():
            matches = re.findall(pattern, text, re.IGNORECASE)
            if matches:
                entities[entity_type] = matches if len(matches) > 1 else matches[0]

        return entities

    def requires_advanced_extraction(self, message):
        """Determine if message needs Claude extraction"""
        keywords = ['vessel', 'ship', 'cargo', 'container', 'delivery', 'pickup', 'customs']
        return any(keyword in message.lower() for keyword in keywords)

    def extract_with_claude(self, message):
        """Use Claude for complex entity extraction"""
        prompt = f"""
        Extract logistics entities from this WhatsApp message:

        "{message}"

        Extract if present:
        - vessel_name
        - cargo_type
        - location (origin/destination)
        - status_update
        - action_required

        Return as JSON. If entity not found, omit it.
        Example: {{"vessel_name": "JPT71", "status_update": "departed"}}
        """

        response = self.client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=500,
            messages=[{"role": "user", "content": prompt}]
        )

        try:
            return json.loads(response.content[0].text)
        except:
            return {}

    def convert_to_rdf(self, messages):
        """Convert WhatsApp messages to RDF triples"""
        from rdflib import Graph, Namespace, Literal, URIRef
        from rdflib.namespace import RDF, XSD

        g = Graph()
        LOGI = Namespace("http://hvdc.samsung.com/logistics#")
        CHAT = Namespace("http://hvdc.samsung.com/chat#")
        g.bind("logi", LOGI)
        g.bind("chat", CHAT)

        for i, msg in enumerate(messages):
            msg_uri = URIRef(CHAT[f"msg_{i}"])

            g.add((msg_uri, RDF.type, CHAT.Message))
            g.add((msg_uri, CHAT.timestamp, Literal(msg['timestamp'], datatype=XSD.dateTime)))
            g.add((msg_uri, CHAT.sender, Literal(msg['sender'])))
            g.add((msg_uri, CHAT.content, Literal(msg['message'])))

            # Add extracted entities
            for entity_type, value in msg['entities'].items():
                if isinstance(value, list):
                    for v in value:
                        g.add((msg_uri, LOGI[entity_type], Literal(v)))
                else:
                    g.add((msg_uri, LOGI[entity_type], Literal(value)))

        return g.serialize(format='turtle')

def process_jpt71_chat():
    """Process JPT71 WhatsApp chat and add to knowledge graph"""
    processor = WhatsAppLogisticsProcessor()

    chat_file = "C:/logi_ontol/JPT71/Jopetwil 71 Groupë‹˜ê³¼ì˜ WhatsApp ëŒ€í™”.txt"
    print(f"ğŸ” Processing WhatsApp chat: {chat_file}")

    messages = processor.parse_chat(chat_file)
    print(f"ğŸ“Š Extracted {len(messages)} messages")

    # Convert to RDF
    rdf_data = processor.convert_to_rdf(messages)

    # Save to knowledge graph
    output_file = "C:/logi_ontol/output/final/jpt71_whatsapp.ttl"
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(rdf_data)

    print(f"âœ… WhatsApp data saved to: {output_file}")

    # Merge with main ontology
    from rdflib import Graph
    main_kg = Graph()
    main_kg.parse("C:/logi_ontol/output/final/abu_final.ttl", format="turtle")
    main_kg.parse(output_file, format="turtle")
    main_kg.serialize("C:/logi_ontol/output/final/abu_final.ttl", format="turtle")

    print("âœ… Merged with abu_final.ttl")

    return len(messages)

if __name__ == "__main__":
    process_jpt71_chat()
```

**2.3 í†µí•© ì§€ì‹ ê·¸ë˜í”„**

```python
# C:\cursor-mcp\hvdc_automation\knowledge_graph.py
from rdflib import Graph, Namespace, Literal, URIRef
from rdflib.namespace import RDF, RDFS, XSD
import json

class HVDCKnowledgeGraph:
    """
    Unified knowledge graph for HVDC logistics
    Integrates: abu_final.ttl + lightning_final.ttl + operational data
    """

    def __init__(self):
        self.graph = Graph()
        self.LOGI = Namespace("http://hvdc.samsung.com/logistics#")
        self.graph.bind("logi", self.LOGI)
        self.load_ontologies()

    def load_ontologies(self):
        """Load all existing RDF ontologies"""
        ontology_files = [
            "C:/logi_ontol/output/final/abu_final.ttl",
            "C:/logi_ontol/output/final/lightning_final.ttl",
        ]

        for onto_file in ontology_files:
            try:
                self.graph.parse(onto_file, format="turtle")
                print(f"âœ… Loaded: {onto_file}")
            except Exception as e:
                print(f"âš ï¸ Failed to load {onto_file}: {e}")

    def query_sparql(self, sparql_query):
        """Execute SPARQL query on knowledge graph"""
        return self.graph.query(sparql_query)

    def find_entity(self, entity_type, filters=None):
        """Find entities of specific type with optional filters"""
        query = f"""
        PREFIX logi: <http://hvdc.samsung.com/logistics#>
        SELECT ?entity ?prop ?value
        WHERE {{
            ?entity a logi:{entity_type} .
            ?entity ?prop ?value .
        }}
        """
        return self.query_sparql(query)

    def add_invoice(self, invoice_data):
        """Add invoice entity to knowledge graph"""
        invoice_uri = URIRef(self.LOGI[f"Invoice_{invoice_data['number']}"])

        self.graph.add((invoice_uri, RDF.type, self.LOGI.Invoice))

        for key, value in invoice_data.items():
            if key == 'number':
                continue

            predicate = self.LOGI[key]

            if isinstance(value, (int, float)):
                literal = Literal(value, datatype=XSD.decimal)
            elif 'date' in key:
                literal = Literal(value, datatype=XSD.date)
            else:
                literal = Literal(value)

            self.graph.add((invoice_uri, predicate, literal))

    def get_statistics(self):
        """Get knowledge graph statistics"""
        stats = {
            "total_triples": len(self.graph),
            "entity_counts": {}
        }

        # Count entities by type
        type_query = """
        PREFIX logi: <http://hvdc.samsung.com/logistics#>
        SELECT ?type (COUNT(?entity) as ?count)
        WHERE {
            ?entity a ?type .
            FILTER(STRSTARTS(STR(?type), "http://hvdc.samsung.com/logistics#"))
        }
        GROUP BY ?type
        """

        results = self.query_sparql(type_query)
        for row in results:
            entity_type = str(row.type).split('#')[1]
            stats["entity_counts"][entity_type] = int(row.count)

        return stats

    def save(self, output_path=None):
        """Save knowledge graph to file"""
        if output_path is None:
            output_path = "C:/logi_ontol/output/final/hvdc_unified.ttl"

        self.graph.serialize(output_path, format="turtle")
        print(f"âœ… Knowledge graph saved: {output_path}")

# Integration with Claude tools
def query_knowledge_graph_via_claude(user_question):
    """
    Use Claude to generate SPARQL and query knowledge graph
    Integration with /logi-master commands
    """
    from anthropic import Anthropic

    client = Anthropic()
    kg = HVDCKnowledgeGraph()

    # Generate SPARQL with Claude
    prompt = f"""
    Generate a SPARQL query to answer this question about HVDC logistics:
    "{user_question}"

    Available ontology:
    Namespace: http://hvdc.samsung.com/logistics#

    Entity types:
    - Invoice (properties: number, date, amount, trn, supplier, customer)
    - Container (properties: number, size, type, status)
    - Vessel (properties: name, eta, etd, location)
    - Shipment (properties: origin, destination, cargo_type, weight)

    Return ONLY the SPARQL query, no explanation.
    Use PREFIX logi: <http://hvdc.samsung.com/logistics#>
    """

    response = client.messages.create(
        model="claude-sonnet-4-20250514",
        max_tokens=1000,
        messages=[{"role": "user", "content": prompt}]
    )

    sparql_query = response.content[0].text.strip()

    # Remove markdown code blocks if present
    if "```sparql" in sparql_query:
        sparql_query = sparql_query.split("```sparql")[1].split("```")[0].strip()

    # Execute query
    results = kg.query_sparql(sparql_query)

    # Format results
    formatted_results = []
    for row in results:
        formatted_results.append({k: str(v) for k, v in row.asdict().items()})

    return {
        "question": user_question,
        "sparql": sparql_query,
        "results": formatted_results,
        "result_count": len(formatted_results)
    }

if __name__ == "__main__":
    # Example usage
    kg = HVDCKnowledgeGraph()
    stats = kg.get_statistics()
    print(json.dumps(stats, indent=2))
```

**ì‚°ì¶œë¬¼ (Phase 2):**
- âœ… AI-OCR íŒŒì´í”„ë¼ì¸ (Confidence â‰¥90%)
- âœ… WhatsApp ë°ì´í„° í†µí•© (JPT71, Lightning)
- âœ… í†µí•© ì§€ì‹ ê·¸ë˜í”„ (RDF + ë¹„ì •í˜• ë°ì´í„°)
- âœ… SPARQL ì¿¼ë¦¬ API
- âœ… Claude-powered ìì—°ì–´ ì¿¼ë¦¬

---

### **PHASE 3: ìë™í™” ì›Œí¬í”Œë¡œìš° & ëŒ€ì‹œë³´ë“œ (2ì£¼)**
#### ëª©í‘œ: End-to-End ìë™í™” ë° ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§

**3.1 ìë™ íŠ¸ë¦¬ê±° ì‹œìŠ¤í…œ**

```python
# C:\cursor-mcp\hvdc_automation\triggers.py
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.cron import CronTrigger
from datetime import datetime
import asyncio

class HVDCAutomationTriggers:
    """
    Automated triggers for HVDC logistics operations
    - Daily invoice audit
    - ETA deviation monitoring
    - WhatsApp data sync
    - Knowledge graph backup
    """

    def __init__(self):
        self.scheduler = BackgroundScheduler()

    def setup_all_triggers(self):
        """Setup all automated triggers"""

        # 1. Daily invoice audit (8:00 AM)
        self.scheduler.add_job(
            func=self.auto_invoice_audit,
            trigger=CronTrigger(hour=8, minute=0),
            id='invoice_audit',
            name='Daily Invoice Audit'
        )

        # 2. ETA monitoring (every 2 hours)
        self.scheduler.add_job(
            func=self.monitor_eta_deviations,
            trigger='interval',
            hours=2,
            id='eta_monitoring',
            name='ETA Deviation Monitoring'
        )

        # 3. WhatsApp data sync (every 30 minutes)
        self.scheduler.add_job(
            func=self.sync_whatsapp_data,
            trigger='interval',
            minutes=30,
            id='whatsapp_sync',
            name='WhatsApp Data Sync'
        )

        # 4. Knowledge graph backup (daily at midnight)
        self.scheduler.add_job(
            func=self.backup_knowledge_graph,
            trigger=CronTrigger(hour=0, minute=0),
            id='kg_backup',
            name='Knowledge Graph Backup'
        )

        # 5. KPI calculation (every hour)
        self.scheduler.add_job(
            func=self.calculate_kpis,
            trigger='interval',
            hours=1,
            id='kpi_calc',
            name='KPI Calculation'
        )

        print("âœ… All triggers setup complete")
        self.scheduler.start()

    def auto_invoice_audit(self):
        """Automatically audit new invoices"""
        from pathlib import Path
        from .ocr_pipeline import audit_invoice_with_ocr

        print(f"ğŸ” [{datetime.now()}] Running daily invoice audit...")

        # Scan for new PDFs
        jpt71_path = Path("C:/logi_ontol/JPT71")
        new_invoices = [f for f in jpt71_path.glob("*.pdf")
                       if "ADNOC-TR" not in f.name and
                          "Manifest" not in f.name and
                          not self.is_processed(f)]

        results = {"processed": 0, "approved": 0, "review_needed": 0}

        for invoice in new_invoices:
            result = audit_invoice_with_ocr(str(invoice))
            results["processed"] += 1

            if result["status"].startswith("âœ…"):
                results["approved"] += 1
            else:
                results["review_needed"] += 1
                self.send_alert(f"âš ï¸ Invoice review needed: {invoice.name}")

        print(f"âœ… Audit complete: {results}")
        self.log_trigger_result("invoice_audit", results)

    def monitor_eta_deviations(self):
        """Monitor vessels for ETA deviations > 24h"""
        from .knowledge_graph import HVDCKnowledgeGraph

        print(f"ğŸ” [{datetime.now()}] Monitoring ETA deviations...")

        kg = HVDCKnowledgeGraph()

        query = """
        PREFIX logi: <http://hvdc.samsung.com/logistics#>
        PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>

        SELECT ?vessel ?name ?scheduled_eta ?current_eta
        WHERE {
            ?vessel a logi:Vessel .
            ?vessel logi:name ?name .
            ?vessel logi:scheduled_eta ?scheduled_eta .
            ?vessel logi:current_eta ?current_eta .
            FILTER (xsd:dateTime(?current_eta) > xsd:dateTime(?scheduled_eta) + "P1D"^^xsd:duration)
        }
        """

        deviations = kg.query_sparql(query)

        for row in deviations:
            delay_hours = (row.current_eta - row.scheduled_eta).total_seconds() / 3600
            self.send_alert(
                f"ğŸš¨ ETA Delay Alert\n"
                f"Vessel: {row.name}\n"
                f"Scheduled: {row.scheduled_eta}\n"
                f"Current: {row.current_eta}\n"
                f"Delay: {delay_hours:.1f} hours"
            )

        print(f"âœ… ETA monitoring complete: {len(list(deviations))} deviations")

    def sync_whatsapp_data(self):
        """Sync WhatsApp chat data to knowledge graph"""
        from .whatsapp_processor import process_jpt71_chat

        print(f"ğŸ” [{datetime.now()}] Syncing WhatsApp data...")

        try:
            message_count = process_jpt71_chat()
            print(f"âœ… Synced {message_count} messages")
        except Exception as e:
            print(f"âš ï¸ WhatsApp sync failed: {e}")
            self.send_alert(f"âš ï¸ WhatsApp sync failed: {e}")

    def backup_knowledge_graph(self):
        """Backup knowledge graph with timestamp"""
        from .knowledge_graph import HVDCKnowledgeGraph
        from shutil import copy2

        print(f"ğŸ” [{datetime.now()}] Backing up knowledge graph...")

        kg = HVDCKnowledgeGraph()
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        backup_path = f"C:/logi_ontol/output/versions/hvdc_unified_{timestamp}.ttl"
        kg.save(backup_path)

        print(f"âœ… Backup saved: {backup_path}")

    def calculate_kpis(self):
        """Calculate and cache KPIs"""
        from .knowledge_graph import HVDCKnowledgeGraph
        import json

        print(f"ğŸ” [{datetime.now()}] Calculating KPIs...")

        kg = HVDCKnowledgeGraph()
        stats = kg.get_statistics()

        # Calculate custom KPIs
        kpis = {
            "timestamp": datetime.now().isoformat(),
            "knowledge_graph_size": stats["total_triples"],
            "entity_counts": stats["entity_counts"],
            "invoice_accuracy": self.calculate_invoice_accuracy(),
            "eta_deviation_avg": self.calculate_eta_deviation(),
            "warehouse_utilization": self.calculate_warehouse_util()
        }

        # Save to cache
        with open("C:/cursor-mcp/data/kpis_cache.json", "w") as f:
            json.dump(kpis, f, indent=2)

        print(f"âœ… KPIs calculated and cached")

    def calculate_invoice_accuracy(self):
        """Calculate invoice processing accuracy"""
        # Placeholder - implement based on actual data
        return 92.5

    def calculate_eta_deviation(self):
        """Calculate average ETA deviation"""
        # Placeholder - implement based on actual data
        return 8.3  # hours

    def calculate_warehouse_util(self):
        """Calculate warehouse utilization"""
        # Placeholder - implement based on actual data
        return 78.5  # percent

    def is_processed(self, file_path):
        """Check if file has been processed"""
        import json
        log_file = "C:/cursor-mcp/data/processed_files.json"

        try:
            with open(log_file, 'r') as f:
                processed = json.load(f)
            return str(file_path) in processed
        except:
            return False

    def log_trigger_result(self, trigger_id, result):
        """Log trigger execution result"""
        import json
        log_file = f"C:/cursor-mcp/logs/triggers_{datetime.now().strftime('%Y%m')}.json"

        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "trigger_id": trigger_id,
            "result": result
        }

        # Append to log file
        try:
            with open(log_file, 'r') as f:
                logs = json.load(f)
        except:
            logs = []

        logs.append(log_entry)

        with open(log_file, 'w') as f:
            json.dump(logs, f, indent=2)

    def send_alert(self, message):
        """Send alert via Telegram"""
        import os
        import requests

        bot_token = os.getenv("TELEGRAM_BOT_TOKEN")
        chat_id = os.getenv("TELEGRAM_CHAT_ID")

        if not bot_token or not chat_id:
            print(f"âš ï¸ Telegram not configured: {message}")
            return

        url = f"https://api.telegram.org/bot{bot_token}/sendMessage"
        payload = {
            "chat_id": chat_id,
            "text": message,
            "parse_mode": "Markdown"
        }

        try:
            response = requests.post(url, json=payload)
            response.raise_for_status()
            print(f"âœ… Alert sent via Telegram")
        except Exception as e:
            print(f"âš ï¸ Failed to send Telegram alert: {e}")

if __name__ == "__main__":
    triggers = HVDCAutomationTriggers()
    triggers.setup_all_triggers()

    # Keep running
    try:
        while True:
            import time
            time.sleep(60)
    except KeyboardInterrupt:
        print("\nâœ… Shutting down automation triggers...")
```

**ì‚°ì¶œë¬¼ (Phase 3):**
- âœ… 60+ Slash Commands í†µí•©
- âœ… ìë™ íŠ¸ë¦¬ê±° ì‹œìŠ¤í…œ (5ê°œ ì£¼ìš” ì‘ì—…)
- âœ… Telegram ì•Œë¦¼ í†µí•©
- âœ… KPI ìë™ ê³„ì‚° ë° ìºì‹±
- âœ… ì™„ì „ ìë™í™” íŒŒì´í”„ë¼ì¸

---

## ğŸ“‹ êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Phase 1: ë¸Œë¦¿ì§• (Week 1-2)
- [ ] **Week 1**
  - [ ] ì‹¬ë³¼ë¦­ ë§í¬ ìƒì„± ë˜ëŠ” ë™ê¸°í™” ìŠ¤í¬ë¦½íŠ¸ êµ¬í˜„
  - [ ] MCP ì„œë²„ logi-ontol ì„¤ì •
  - [ ] ì „ì²´ íŒŒì¼ ì¸ë±ìŠ¤ ìƒì„±
  - [ ] Claude filesystem ì ‘ê·¼ í…ŒìŠ¤íŠ¸
  - [ ] ë””ë ‰í† ë¦¬ êµ¬ì¡° ë¬¸ì„œí™”

- [ ] **Week 2**
  - [ ] HVDC_PJTì™€ í†µí•©
  - [ ] RoboCopy ìë™ ë™ê¸°í™” ì„¤ì •
  - [ ] ë°±ì—… ì „ëµ ìˆ˜ë¦½
  - [ ] Slash commands í…ŒìŠ¤íŠ¸
  - [ ] ì„±ëŠ¥ ìµœì í™”

### Phase 2: AI-OCR & ì§€ì‹ (Week 3-5)
- [ ] **Week 3**
  - [ ] Tesseract OCR ì„¤ì¹˜ ë° ì„¤ì •
  - [ ] Claude ì—”í‹°í‹° ì¶”ì¶œ í†µí•©
  - [ ] ë¬¸ì„œ ë¶„ë¥˜ ì‹œìŠ¤í…œ êµ¬ì¶•
  - [ ] JPT71 PDF í…ŒìŠ¤íŠ¸
  - [ ] ì¶”ì¶œ ì •í™•ë„ ê²€ì¦ (ëª©í‘œ: 90%)

- [ ] **Week 4**
  - [ ] WhatsApp ë°ì´í„° íŒŒì„œ êµ¬í˜„
  - [ ] ì§€ì‹ ê·¸ë˜í”„ í†µí•©
  - [ ] SPARQL ì¿¼ë¦¬ ìƒì„±ê¸°
  - [ ] SHACL ê²€ì¦
  - [ ] End-to-end íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸

- [ ] **Week 5**
  - [ ] abu_final.ttl + lightning_final.ttl ë¡œë“œ
  - [ ] ì—”í‹°í‹° í¬ë¡œìŠ¤ ë ˆí¼ëŸ°ìŠ¤
  - [ ] ì¿¼ë¦¬ API êµ¬ì¶•
  - [ ] ì„±ëŠ¥ íŠœë‹
  - [ ] ë¬¸ì„œí™”

### Phase 3: ìë™í™” & ëŒ€ì‹œë³´ë“œ (Week 6-7)
- [ ] **Week 6**
  - [ ] ëª¨ë“  slash commands êµ¬í˜„
  - [ ] ìë™ íŠ¸ë¦¬ê±° ì„¤ì •
  - [ ] Telegram ë´‡ í†µí•©
  - [ ] React ëŒ€ì‹œë³´ë“œ êµ¬ì¶•
  - [ ] ì‹¤ì‹œê°„ ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë°

- [ ] **Week 7**
  - [ ] End-to-end í…ŒìŠ¤íŠ¸
  - [ ] ì‚¬ìš©ì ìˆ˜ìš© í…ŒìŠ¤íŠ¸
  - [ ] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹
  - [ ] êµìœ¡ ìë£Œ ì œì‘
  - [ ] Go-live ì¤€ë¹„

---

## ğŸ¯ ì„±ê³¼ ì§€í‘œ

### ì •ëŸ‰ì  KPI
| ì§€í‘œ | Before | After | ê°œì„ ìœ¨ |
|------|--------|-------|--------|
| ì¸ë³´ì´ìŠ¤ ì²˜ë¦¬ ì‹œê°„ | 15ë¶„ | 3ë¶„ | **80%â†“** |
| OCR ì •í™•ë„ | 75% | â‰¥90% | **20%â†‘** |
| ìˆ˜ë™ ê°œì… | 40ê±´/ì›” | 10ê±´/ì›” | **75%â†“** |
| ETA í¸ì°¨ íƒì§€ | ìˆ˜ë™ | <2ì‹œê°„ ìë™ | **ìë™í™”** |
| ì§€ì‹ ê·¸ë˜í”„ í¬ê¸° | 5K triples | 50K triples | **10ë°°â†‘** |
| ì¿¼ë¦¬ ì‘ë‹µ ì‹œê°„ | - | <2ì´ˆ | **ì‹ ê·œ** |
| ì‹œìŠ¤í…œ ê°€ë™ë¥  | - | 99.5% | **ì‹ ê·œ** |

### ì •ì„±ì  ì„±ê³¼
- âœ… **Single Source of Truth**: ëª¨ë“  ë¬¼ë¥˜ ë°ì´í„°ê°€ í†µí•© ì§€ì‹ ê·¸ë˜í”„ì—
- âœ… **ì‚¬ì „ ëŒ€ì‘ ì•Œë¦¼**: ë¬¸ì œê°€ ì»¤ì§€ê¸° ì „ì— ìë™ íƒì§€
- âœ… **ì™„ì „í•œ ê°ì‚¬ ì¶”ì **: ì›ë³¸ ë°ì´í„°ë¶€í„° ì˜ì‚¬ê²°ì •ê¹Œì§€
- âœ… **í™•ì¥ì„±**: ìƒˆë¡œìš´ ì„ ë°•, ë£¨íŠ¸, íŒŒíŠ¸ë„ˆ ì¶”ê°€ ìš©ì´
- âœ… **ì‚¬ìš©ì ë§Œì¡±ë„**: ë¹ ë¥¸ ì‘ë‹µ, ì ì€ ì˜¤ë¥˜

---

## ğŸ”„ ë¡¤ì•„ì›ƒ ê³„íš

### Week 0: ì¤€ë¹„
- íŒ€ í‚¥ì˜¤í”„ ë¯¸íŒ…
- í™˜ê²½ ì„¤ì •
- ì ‘ê·¼ ê¶Œí•œ ë¶€ì—¬
- ê¸°ì¤€ì„  ì¸¡ì •

### Week 1-2: Phase 1 (ë¸Œë¦¿ì§•)
- 1-2ëª… ë‚´ë¶€ íŒŒì¼ëŸ¿
- ë™ê¸°í™” ê²€ì¦
- ë²„ê·¸ ìˆ˜ì •
- ë¬¸ì„œ ì—…ë°ì´íŠ¸

### Week 3-5: Phase 2 (AI-OCR & ì§€ì‹)
- 5ëª… íŒŒì›Œ ìœ ì €ë¡œ í™•ëŒ€
- ê³¼ê±° ë°ì´í„° ì²˜ë¦¬ (JPT71)
- OCR ì •í™•ë„ ê°œì„ 
- ì§€ì‹ ê·¸ë˜í”„ êµ¬ì¶•

### Week 6-7: Phase 3 (ìë™í™”)
- ì „ì²´ íŒ€ ë¡¤ì•„ì›ƒ
- ëŒ€ì‹œë³´ë“œ ëŸ°ì¹­
- ìë™ íŠ¸ë¦¬ê±° í™œì„±í™”
- 24/7 ëª¨ë‹ˆí„°ë§

### Week 8+: ìµœì í™”
- ì„±ëŠ¥ íŠœë‹
- ê¸°ëŠ¥ ìš”ì²­ ì²˜ë¦¬
- ì§€ì†ì  ê°œì„ 
- ë‹¤ë¥¸ ì„ ë°•ìœ¼ë¡œ í™•ì¥

---

## ğŸ”§ ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ Next Actions

### ì´ë²ˆ ì£¼ (Week 0)
1. âœ… **ì´ ë§ˆìŠ¤í„°í”Œëœ ê²€í† ** - ì´í•´ê´€ê³„ìì™€ ê³µìœ 
2. âœ… **ê°œë°œ í™˜ê²½ ì„¤ì •**
   - ì˜ì¡´ì„± ì„¤ì¹˜
   - MCP ì„œë²„ êµ¬ì„±
   - Filesystem ì ‘ê·¼ í…ŒìŠ¤íŠ¸
3. âœ… **Phase 1 ì‘ì—… ë³´ë“œ ìƒì„±** - Jira/Asana
4. âœ… **í‚¥ì˜¤í”„ ë¯¸íŒ… ìŠ¤ì¼€ì¤„**

### ë‹¤ìŒ 2ì£¼ (Week 1-2)
1. **ì‹¬ë³¼ë¦­ ë§í¬ êµ¬í˜„** - C:\logi_ontol â†’ C:\cursor-mcp
2. **MCP ì„œë²„ êµ¬ì¶•** - logi-ontol ì ‘ê·¼
3. **/logi-master ëª…ë ¹ì–´ í…ŒìŠ¤íŠ¸** - ìƒ˜í”Œ ë°ì´í„°
4. **ì¥ì• ë¬¼ ë¬¸ì„œí™”**

### ë‹¤ìŒ 4ì£¼ (Week 3-6)
1. **AI-OCR íŒŒì´í”„ë¼ì¸ ë°°í¬**
2. **JPT71 ê³¼ê±° ë°ì´í„° ì²˜ë¦¬**
3. **ì§€ì‹ ê·¸ë˜í”„ êµ¬ì¶•**
4. **íŒŒì¼ëŸ¿ ëŒ€ì‹œë³´ë“œ ëŸ°ì¹­**

---

## ğŸ“Š ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Claude Interface Layer                    â”‚
â”‚  (Web Chat / API / Slack / Telegram / WhatsApp)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                MACHO-GPT Command Router                      â”‚
â”‚  /logi-master  /switch_mode  /visualize  /check_KPI         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
         â”‚               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Claude Tools â”‚  â”‚  MCP Servers  â”‚
â”‚              â”‚  â”‚               â”‚
â”‚ â€¢ filesystem â”‚  â”‚ â€¢ logi-ontol  â”‚
â”‚ â€¢ web_search â”‚  â”‚ â€¢ pdf-tools   â”‚
â”‚ â€¢ drive_srch â”‚  â”‚ â€¢ vercel      â”‚
â”‚ â€¢ web_fetch  â”‚  â”‚ â€¢ windows     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚               â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Integration & Processing Layer                  â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  AI-OCR      â”‚  â”‚  Knowledge   â”‚  â”‚  Data Pipeline  â”‚  â”‚
â”‚  â”‚  Pipeline    â”‚  â”‚  Graph       â”‚  â”‚                 â”‚  â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚  â€¢ Ingest       â”‚  â”‚
â”‚  â”‚ â€¢ Tesseract  â”‚  â”‚ â€¢ RDFLib     â”‚  â”‚  â€¢ Validate     â”‚  â”‚
â”‚  â”‚ â€¢ Claude     â”‚  â”‚ â€¢ SPARQL     â”‚  â”‚  â€¢ Transform    â”‚  â”‚
â”‚  â”‚ â€¢ Entity Ext â”‚  â”‚ â€¢ SHACL      â”‚  â”‚  â€¢ Load         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Data Sources Layer                         â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ logi_ontol â”‚  â”‚ Google     â”‚  â”‚  External APIs       â”‚  â”‚
â”‚  â”‚            â”‚  â”‚ Drive      â”‚  â”‚                      â”‚  â”‚
â”‚  â”‚ â€¢ RDF      â”‚  â”‚            â”‚  â”‚  â€¢ AIS (vessel)      â”‚  â”‚
â”‚  â”‚ â€¢ Excel    â”‚  â”‚ â€¢ Docs     â”‚  â”‚  â€¢ NOAA (weather)    â”‚  â”‚
â”‚  â”‚ â€¢ PDF      â”‚  â”‚ â€¢ Sheets   â”‚  â”‚  â€¢ Port APIs         â”‚  â”‚
â”‚  â”‚ â€¢ WhatsApp â”‚  â”‚ â€¢ Templatesâ”‚  â”‚  â€¢ Customs           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’¡ í˜ì‹  ê¸°íšŒ

### MVP ì´í›„
1. **ì˜ˆì¸¡ ë¶„ì„**
   - ML ê¸°ë°˜ ETA ì˜ˆì¸¡
   - ë¹„ìš© ì˜ˆì¸¡
   - ë¦¬ìŠ¤í¬ ìŠ¤ì½”ì–´ë§

2. **ëª¨ë°”ì¼ ì•±**
   - iOS/Android ë„¤ì´í‹°ë¸Œ ì•±
   - ì˜¤í”„ë¼ì¸ ëª¨ë“œ
   - í‘¸ì‹œ ì•Œë¦¼

3. **ìŒì„± ì¸í„°í˜ì´ìŠ¤**
   - "Alexa, JPT71 ìƒíƒœëŠ”?"
   - ì¼ë°˜ ì‘ì—…ì„ ìœ„í•œ ìŒì„± ëª…ë ¹

4. **ë¸”ë¡ì²´ì¸ í†µí•©**
   - ë¶ˆë³€ ê°ì‚¬ ì¶”ì 
   - ê²°ì œìš© ìŠ¤ë§ˆíŠ¸ ê³„ì•½
   - ë¶„ì‚° ì§€ì‹ ê·¸ë˜í”„

5. **ê³ ê¸‰ ì‹œê°í™”**
   - 3D ì»¨í…Œì´ë„ˆ ì ì¬
   - ì‹¤ì‹œê°„ ì„ ë°• ì¶”ì 
   - ì°½ê³  ë ˆì´ì•„ì›ƒìš© AR

---

## ğŸ“ ì—°ë½ì²˜ & ì§€ì›

**í”„ë¡œì íŠ¸ ë¦¬ë”**: MACHO-GPT Integration Team
**ê¸°ìˆ  ë¦¬ë”**: [Your Name]
**ì´í•´ê´€ê³„ì**: Samsung C&T, ADNOC, DSV

**ì§€ì› ì±„ë„**:
- Slack: #hvdc-integration
- Email: hvdc-support@samsung.com
- Telegram: @logi-alert

**ì—…ë¬´ ì‹œê°„**: ì›”-ê¸ˆ 9:00-17:00 GST

---

**ë¬¸ì„œ ë²„ì „**: 1.0
**ìµœì¢… ì—…ë°ì´íŠ¸**: 2025-10-24
**ë‹¤ìŒ ê²€í† **: 2025-11-01
**ìƒíƒœ**: âœ… êµ¬í˜„ ìŠ¹ì¸ë¨
