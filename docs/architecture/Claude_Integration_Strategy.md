# üèóÔ∏è C:\logi_ontol ‚Üî Claude ÏãúÏä§ÌÖú ÌÜµÌï© ÎßàÏä§ÌÑ∞ÌîåÎûú
## MACHO-GPT v3.4-mini Enhanced Integration Strategy

**ÏûëÏÑ±Ïùº**: 2025-10-24
**Î≤ÑÏ†Ñ**: 1.0
**ÌîÑÎ°úÏ†ùÌä∏**: HVDC Project - Samsung C&T Logistics (ADNOC¬∑DSV Partnership)

---

## üìä EXECUTIVE SUMMARY

C:\logi_ontolÏùÄ HVDC ÌîÑÎ°úÏ†ùÌä∏Î•º ÏúÑÌïú **Î¨ºÎ•ò Ïò®ÌÜ®Î°úÏßÄ ÏãúÏä§ÌÖú**ÏúºÎ°ú, ÌòÑÏû¨ Python Í∏∞Î∞òÏúºÎ°ú Íµ¨Ï∂ïÎêòÏñ¥ ÏûàÏäµÎãàÎã§. Î≥∏ ÌÜµÌï© Ï†ÑÎûµÏùÄ Ïù¥Î•º ClaudeÏùò ÎÑ§Ïù¥Ìã∞Î∏å ÎèÑÍµ¨Îì§Í≥º ÏôÑÏ†ÑÌûà ÌÜµÌï©ÌïòÏó¨ **ÏûêÎèôÌôîÎêú ÏßÄÎä•Ìòï Î¨ºÎ•ò Í¥ÄÎ¶¨ ÏãúÏä§ÌÖú**ÏùÑ Íµ¨Ï∂ïÌïòÎäî Í≤ÉÏùÑ Î™©ÌëúÎ°ú Ìï©ÎãàÎã§.

### ÌïµÏã¨ Î™©Ìëú
1. **ÏôÑÏ†Ñ ÏûêÎèôÌôî**: logi_ontol ‚Üí Claude Tools ‚Üí Automated Workflows
2. **Ïã§ÏãúÍ∞Ñ OCR**: PDF/Image ‚Üí AI-OCR ‚Üí Knowledge Graph ‚Üí Decision
3. **ÏßÄÏãù ÌÜµÌï©**: RDF Ontology + Claude RAG + Google Drive + Web Search
4. **Ïö¥ÏòÅ ÏµúÏ†ÅÌôî**: Manual 60%‚Üì, Error 85%‚Üì, Response Time 70%‚Üì

---

## üó∫Ô∏è ÌòÑÏû¨ ÏãúÏä§ÌÖú Íµ¨Ï°∞ Î∂ÑÏÑù

### C:\logi_ontol ÎîîÎ†âÌÜ†Î¶¨ Îßµ

```
C:\logi_ontol/ (117MB compressed)
‚îú‚îÄ‚îÄ logiontology/                    # üéØ ÌïµÏã¨ Ïò®ÌÜ®Î°úÏßÄ ÏóîÏßÑ
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ core/                    # ÎèÑÎ©îÏù∏ Î™®Îç∏ & Í≥ÑÏïΩ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mapping/                 # Ïò®ÌÜ®Î°úÏßÄ Îß§Ìïë (v2.6)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ validation/              # SHACL Ïä§ÌÇ§Îßà Í≤ÄÏ¶ù
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ingest/                  # Excel Îç∞Ïù¥ÌÑ∞ ÏàòÏßë
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rdfio/                   # RDF ÏùΩÍ∏∞/Ïì∞Í∏∞
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ reasoning/               # AI Ï∂îÎ°† ÏóîÏßÑ
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ pipeline/                # ÏõåÌÅ¨ÌîåÎ°úÏö∞ ÌååÏù¥ÌîÑÎùºÏù∏
‚îÇ   ‚îú‚îÄ‚îÄ configs/                     # ‚öôÔ∏è ÏÑ§Ï†ï
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mapping_rules.v2.6.yaml  # Îß§Ìïë Í∑úÏπô
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ lightning_sparql_queries.sparql
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ abu_sparql_queries.sparql
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ shacl_shapes.ttl         # Í≤ÄÏ¶ù Ïä§ÌÇ§Îßà
‚îÇ   ‚îú‚îÄ‚îÄ tests/ (92% coverage)        # ‚úÖ ÌÖåÏä§Ìä∏
‚îÇ   ‚îî‚îÄ‚îÄ docs/                        # üìö ÏïÑÌÇ§ÌÖçÏ≤ò Î¨∏ÏÑú
‚îÇ
‚îú‚îÄ‚îÄ JPT71/                           # üö¢ Jopetwil 71 ÏÑ†Î∞ï Ïö¥ÏòÅ Îç∞Ïù¥ÌÑ∞
‚îÇ   ‚îú‚îÄ‚îÄ ADNOC-TR *.pdf              # ADNOC Ïö¥ÏÜ° Î¨∏ÏÑú (20+)
‚îÇ   ‚îú‚îÄ‚îÄ IMG-*.jpg (400+ files)      # WhatsApp ÌòÑÏû• Ïù¥ÎØ∏ÏßÄ
‚îÇ   ‚îú‚îÄ‚îÄ Manifest *.pdf              # ÏÑ†Î∞ï Îß§ÎãàÌéòÏä§Ìä∏
‚îÇ   ‚îú‚îÄ‚îÄ *ÏÑ†ÏõêÎ™Ö*.pdf                 # ÏÑ†Ïõê Î¨∏ÏÑú (HARJOT, SOHAN, JAGBIR Îì±)
‚îÇ   ‚îî‚îÄ‚îÄ WhatsApp ÎåÄÌôî.txt/zip       # Ïö¥ÏòÅ Î°úÍ∑∏ (357KB, ÏôÑÏ†ÑÌïú ÎåÄÌôî Í∏∞Î°ù)
‚îÇ
‚îú‚îÄ‚îÄ HVDC Project Lightning/          # ‚ö° Lightning ÏÑúÎ∏åÏãúÏä§ÌÖú
‚îÇ   ‚îú‚îÄ‚îÄ whatsapp_output/
‚îÇ   ‚îú‚îÄ‚îÄ IMG-*.jpg (50+ files)       # ÌòÑÏû• ÏÇ¨ÏßÑ
‚îÇ   ‚îú‚îÄ‚îÄ Logistics_Entities__Summary_.csv
‚îÇ   ‚îî‚îÄ‚îÄ Guideline_HVDC_Project_lightning.md
‚îÇ
‚îú‚îÄ‚îÄ data/                            # üìÇ ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞
‚îÇ   ‚îî‚îÄ‚îÄ *.xlsx                       # Excel Î¨ºÎ•ò Îç∞Ïù¥ÌÑ∞
‚îÇ
‚îú‚îÄ‚îÄ output/                          # üì§ RDF Ï∂úÎ†•
‚îÇ   ‚îú‚îÄ‚îÄ final/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ abu_final.ttl
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ lightning_final.ttl
‚îÇ   ‚îî‚îÄ‚îÄ versions/                    # Î≤ÑÏ†Ñ ÏïÑÏπ¥Ïù¥Î∏å
‚îÇ
‚îú‚îÄ‚îÄ reports/                         # üìä ÏãúÏä§ÌÖú Î≥¥Í≥†ÏÑú
‚îÇ   ‚îú‚îÄ‚îÄ final/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ SYSTEM_ARCHITECTURE_COMPREHENSIVE.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ HVDC_MASTER_INTEGRATION_REPORT.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ABU_SYSTEM_ARCHITECTURE.md
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ LIGHTNING_FINAL_INTEGRATION_REPORT.md
‚îÇ   ‚îú‚îÄ‚îÄ architecture/
‚îÇ   ‚îú‚îÄ‚îÄ analysis/
‚îÇ   ‚îî‚îÄ‚îÄ operations/
‚îÇ
‚îú‚îÄ‚îÄ scripts/                         # üîß Ï≤òÎ¶¨ Ïä§ÌÅ¨Î¶ΩÌä∏
‚îÇ   ‚îú‚îÄ‚îÄ process_hvdc_excel.py
‚îÇ   ‚îú‚îÄ‚îÄ integrate_lightning_images.py
‚îÇ   ‚îú‚îÄ‚îÄ build_lightning_cross_references.py
‚îÇ   ‚îú‚îÄ‚îÄ generate_final_lightning_report.py
‚îÇ   ‚îî‚îÄ‚îÄ compare_abu_lightning.py
‚îÇ
‚îú‚îÄ‚îÄ ontology_unified/                # üß¨ ÌÜµÌï© Ïò®ÌÜ®Î°úÏßÄ
‚îú‚îÄ‚îÄ ABU/                             # ÏïÑÎ∂ÄÎã§ÎπÑ ÌäπÌôî Îç∞Ïù¥ÌÑ∞
‚îú‚îÄ‚îÄ cursor_ontology_first_pack_v1/   # Cursor ÌÜµÌï© ÏãúÎèÑ (v1)
‚îî‚îÄ‚îÄ archive/                         # ÏïÑÏπ¥Ïù¥Î∏å

‚ö†Ô∏è Ï†ëÍ∑º Ï†úÏïΩ: C:\logi_ontolÏùÄ Claude filesystem ÎèÑÍµ¨Î°ú ÏßÅÏ†ë Ï†ëÍ∑º Î∂àÍ∞Ä
‚úÖ Ìï¥Í≤∞ Î∞©Ïïà: Windows-MCP PowerShell ÎòêÎäî Ïã¨Î≥ºÎ¶≠ ÎßÅÌÅ¨/Î≥µÏÇ¨ ÌïÑÏöî
```

### Í∏∞Ï°¥ ÏãúÏä§ÌÖú Í∞ïÏ†ê
- ‚úÖ **92% ÌÖåÏä§Ìä∏ Ïª§Î≤ÑÎ¶¨ÏßÄ** - ÏïàÏ†ïÏ†ÅÏù∏ ÏΩîÎìúÎ≤†Ïù¥Ïä§
- ‚úÖ **RDF Í∏∞Î∞ò Ïò®ÌÜ®Î°úÏßÄ** - ÌëúÏ§ÄÌôîÎêú ÏßÄÏãù ÌëúÌòÑ
- ‚úÖ **SHACL Í≤ÄÏ¶ù** - Îç∞Ïù¥ÌÑ∞ ÌíàÏßà Î≥¥Ïû•
- ‚úÖ **SPARQL ÏøºÎ¶¨** - Í∞ïÎ†•Ìïú ÏßÄÏãù Í≤ÄÏÉâ
- ‚úÖ **Ïã§Ï†ú Ïö¥ÏòÅ Îç∞Ïù¥ÌÑ∞** - JPT71, Lightning ÌîÑÎ°úÏ†ùÌä∏
- ‚úÖ **ÏôÑÏ†ÑÌïú Î¨∏ÏÑúÌôî** - ÏïÑÌÇ§ÌÖçÏ≤ò Î≥¥Í≥†ÏÑú ÏôÑÎπÑ

### Í∏∞Ï°¥ ÏãúÏä§ÌÖú ÌïúÍ≥Ñ
- ‚ö†Ô∏è **ÏàòÎèô ÌîÑÎ°úÏÑ∏Ïä§** - Ïä§ÌÅ¨Î¶ΩÌä∏ ÏàòÎèô Ïã§Ìñâ ÌïÑÏöî
- ‚ö†Ô∏è **Ï†úÌïúÏ†Å OCR** - PDF/Ïù¥ÎØ∏ÏßÄ ÏûêÎèô Ï≤òÎ¶¨ Î∂ÄÏ°±
- ‚ö†Ô∏è **ÌååÌé∏ÌôîÎêú Îç∞Ïù¥ÌÑ∞** - WhatsApp, Excel, PDF Î∂ÑÏÇ∞
- ‚ö†Ô∏è **Îã®Î∞©Ìñ• ÏõåÌÅ¨ÌîåÎ°úÏö∞** - ÌîºÎìúÎ∞± Î£®ÌîÑ Î∂ÄÏ°±
- ‚ö†Ô∏è **Ï†úÌïúÏ†Å AI** - Ï∂îÎ°† ÏóîÏßÑ ÌôúÏö©ÎèÑ ÎÇÆÏùå

---

## üéØ ÌÜµÌï© Ï†ÑÎûµ: 3-Phase Roadmap

### **PHASE 1: Î∏åÎ¶øÏßï & Ïù∏Îç±Ïã± (2Ï£º)**
#### Î™©Ìëú: C:\logi_ontolÏùÑ Claude ÏÉùÌÉúÍ≥ÑÎ°ú Ïó∞Í≤∞

**1.1 ÎîîÎ†âÌÜ†Î¶¨ Î∏åÎ¶øÏßï**

**Option A: Ïã¨Î≥ºÎ¶≠ ÎßÅÌÅ¨ ÏÉùÏÑ± (Í∂åÏû•)**
```powershell
# Í¥ÄÎ¶¨Ïûê Í∂åÌïú PowerShellÏóêÏÑú Ïã§Ìñâ
New-Item -ItemType SymbolicLink `
  -Path "C:\cursor-mcp\logi_ontol_link" `
  -Target "C:\logi_ontol"
```

**Option B: ÏûêÎèô ÎèôÍ∏∞Ìôî Ïä§ÌÅ¨Î¶ΩÌä∏**
```powershell
# C:\cursor-mcp\scripts\sync_logi_ontol.ps1
$source = "C:\logi_ontol"
$dest = "C:\cursor-mcp\logi_ontol_sync"
$exclude = @("node_modules", ".git", "__pycache__", "venv", ".ruff_cache")

# ÎØ∏Îü¨ ÎèôÍ∏∞Ìôî (Î≥ÄÍ≤ΩÏÇ¨Ìï≠Îßå Î≥µÏÇ¨)
robocopy $source $dest /MIR /XD $exclude /MT:8 /LOG+:sync.log

# Ïä§ÏºÄÏ§ÑÎü¨ Îì±Î°ù (Îß§ ÏãúÍ∞Ñ ÏûêÎèô ÎèôÍ∏∞Ìôî)
$trigger = New-ScheduledTaskTrigger -Once -At (Get-Date) -RepetitionInterval (New-TimeSpan -Hours 1)
$action = New-ScheduledTaskAction -Execute "PowerShell.exe" -Argument "-File C:\cursor-mcp\scripts\sync_logi_ontol.ps1"
Register-ScheduledTask -TaskName "LogiOntolSync" -Trigger $trigger -Action $action
```

**1.2 MCP ÏÑúÎ≤Ñ ÌÜµÌï©**
```javascript
// C:\cursor-mcp\mcp\servers\logi-ontol-server.js
const MCPServer = require('@modelcontextprotocol/sdk/server/index.js');
const { exec } = require('child_process');
const fs = require('fs');

const server = new MCPServer({
  name: "logi-ontol",
  version: "1.0.0",
  capabilities: {
    tools: {
      "read_ontology": {
        description: "Read RDF ontology files (.ttl) from logi_ontol",
        inputSchema: {
          type: "object",
          properties: {
            path: { type: "string", description: "Relative path in logi_ontol" }
          },
          required: ["path"]
        }
      },
      "query_sparql": {
        description: "Execute SPARQL queries on HVDC knowledge graph",
        inputSchema: {
          type: "object",
          properties: {
            query: { type: "string", description: "SPARQL query" }
          },
          required: ["query"]
        }
      },
      "validate_shacl": {
        description: "Validate logistics data against SHACL shapes",
        inputSchema: {
          type: "object",
          properties: {
            data_uri: { type: "string" },
            shape_uri: { type: "string" }
          }
        }
      },
      "process_invoice": {
        description: "Process invoice PDF through OCR pipeline",
        inputSchema: {
          type: "object",
          properties: {
            pdf_path: { type: "string" }
          },
          required: ["pdf_path"]
        }
      }
    }
  }
});

// Tool implementations
server.setRequestHandler('tools/call', async (request) => {
  const { name, arguments: args } = request.params;

  switch(name) {
    case 'read_ontology':
      const ttlPath = `C:/logi_ontol/${args.path}`;
      return { content: fs.readFileSync(ttlPath, 'utf8') };

    case 'query_sparql':
      // Execute Python script with SPARQL query
      return new Promise((resolve) => {
        exec(`python C:/logi_ontol/scripts/query_sparql.py "${args.query}"`,
          (error, stdout) => {
            resolve({ results: JSON.parse(stdout) });
          });
      });

    case 'validate_shacl':
      // Run SHACL validation
      return new Promise((resolve) => {
        exec(`python -m logiontology.validation --data ${args.data_uri} --shape ${args.shape_uri}`,
          { cwd: 'C:/logi_ontol/logiontology' },
          (error, stdout) => {
            resolve({ validation: JSON.parse(stdout) });
          });
      });

    case 'process_invoice':
      // Trigger OCR pipeline
      return new Promise((resolve) => {
        exec(`python C:/cursor-mcp/hvdc_automation/ocr_pipeline.py "${args.pdf_path}"`,
          (error, stdout) => {
            resolve({ entities: JSON.parse(stdout) });
          });
      });
  }
});

server.listen();
```

**1.3 Ïù∏Îç±Ïã± & Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ ÏÉùÏÑ±**
```python
# C:\cursor-mcp\scripts\index_logi_ontol.py
import os
import json
import hashlib
from pathlib import Path
from datetime import datetime

def create_comprehensive_index():
    """Create searchable index of logi_ontol contents"""
    index = {
        "version": "1.0",
        "timestamp": datetime.now().isoformat(),
        "summary": {
            "total_files": 0,
            "total_size_mb": 0,
            "ontologies": 0,
            "pdfs": 0,
            "images": 0,
            "scripts": 0
        },
        "ontologies": [],
        "scripts": [],
        "data_files": [],
        "reports": [],
        "operational_data": {
            "jpt71": {},
            "lightning": {}
        }
    }

    base_path = Path("C:/logi_ontol")

    # 1. Scan ontology files
    for ttl_file in (base_path / "output" / "final").glob("*.ttl"):
        file_info = {
            "path": str(ttl_file.relative_to(base_path)),
            "size": ttl_file.stat().st_size,
            "modified": datetime.fromtimestamp(ttl_file.stat().st_mtime).isoformat(),
            "checksum": calculate_checksum(ttl_file)
        }
        index["ontologies"].append(file_info)
        index["summary"]["total_files"] += 1
        index["summary"]["total_size_mb"] += file_info["size"] / (1024*1024)
        index["summary"]["ontologies"] += 1

    # 2. Scan JPT71 operational data
    jpt71_path = base_path / "JPT71"
    jpt71_index = {
        "pdfs": [],
        "images": [],
        "whatsapp_chat": None,
        "crew_documents": []
    }

    for pdf_file in jpt71_path.glob("*.pdf"):
        file_info = {
            "filename": pdf_file.name,
            "path": str(pdf_file.relative_to(base_path)),
            "size": pdf_file.stat().st_size,
            "type": classify_document(pdf_file.name)
        }

        if "ADNOC-TR" in pdf_file.name:
            file_info["document_type"] = "transport_request"
        elif "Manifest" in pdf_file.name:
            file_info["document_type"] = "manifest"
        elif pdf_file.name in ["HARJOT.pdf", "SOHAN.pdf", "JAGBIR.pdf"]:
            jpt71_index["crew_documents"].append(file_info)

        jpt71_index["pdfs"].append(file_info)
        index["summary"]["pdfs"] += 1

    # Count images
    jpt71_index["image_count"] = len(list(jpt71_path.glob("IMG-*.jpg")))
    index["summary"]["images"] += jpt71_index["image_count"]

    # WhatsApp chat
    chat_file = jpt71_path / "Jopetwil 71 GroupÎãòÍ≥ºÏùò WhatsApp ÎåÄÌôî.txt"
    if chat_file.exists():
        jpt71_index["whatsapp_chat"] = {
            "path": str(chat_file.relative_to(base_path)),
            "size": chat_file.stat().st_size,
            "message_count": count_whatsapp_messages(chat_file)
        }

    index["operational_data"]["jpt71"] = jpt71_index

    # 3. Scan Lightning data
    lightning_path = base_path / "HVDC Project Lightning"
    lightning_index = {
        "csv_summary": None,
        "images": len(list(lightning_path.glob("IMG-*.jpg"))),
        "guideline": str(lightning_path / "Guideline_HVDC_Project_lightning (1).md")
    }

    csv_file = lightning_path / "Logistics_Entities__Summary_.csv"
    if csv_file.exists():
        lightning_index["csv_summary"] = {
            "path": str(csv_file.relative_to(base_path)),
            "size": csv_file.stat().st_size
        }

    index["operational_data"]["lightning"] = lightning_index
    index["summary"]["images"] += lightning_index["images"]

    # 4. Scan scripts
    scripts_path = base_path / "scripts"
    for py_file in scripts_path.glob("*.py"):
        index["scripts"].append({
            "name": py_file.name,
            "path": str(py_file.relative_to(base_path)),
            "purpose": extract_script_purpose(py_file)
        })
        index["summary"]["scripts"] += 1

    # 5. Scan reports
    reports_path = base_path / "reports" / "final"
    for md_file in reports_path.glob("*.md"):
        index["reports"].append({
            "title": md_file.stem,
            "path": str(md_file.relative_to(base_path)),
            "size": md_file.stat().st_size
        })

    # Save index
    output_path = Path("C:/cursor-mcp/logi_ontol_index.json")
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(index, f, indent=2, ensure_ascii=False)

    print(f"‚úÖ Index created successfully!")
    print(f"   Total files: {index['summary']['total_files']}")
    print(f"   Total size: {index['summary']['total_size_mb']:.2f} MB")
    print(f"   Ontologies: {index['summary']['ontologies']}")
    print(f"   PDFs: {index['summary']['pdfs']}")
    print(f"   Images: {index['summary']['images']}")
    print(f"   Scripts: {index['summary']['scripts']}")

    return index

def calculate_checksum(file_path):
    """Calculate MD5 checksum of file"""
    with open(file_path, 'rb') as f:
        return hashlib.md5(f.read()).hexdigest()

def classify_document(filename):
    """Classify document type based on filename"""
    filename_lower = filename.lower()
    if 'invoice' in filename_lower:
        return 'invoice'
    elif 'manifest' in filename_lower:
        return 'manifest'
    elif 'tr' in filename_lower or 'transport' in filename_lower:
        return 'transport_request'
    else:
        return 'general'

def count_whatsapp_messages(chat_file):
    """Count messages in WhatsApp chat export"""
    with open(chat_file, 'r', encoding='utf-8') as f:
        content = f.read()
    import re
    # Pattern: [DD/MM/YYYY, HH:MM:SS] Sender: Message
    pattern = r'\[\d{2}/\d{2}/\d{4}, \d{2}:\d{2}:\d{2}\]'
    return len(re.findall(pattern, content))

def extract_script_purpose(py_file):
    """Extract purpose from script docstring"""
    with open(py_file, 'r', encoding='utf-8') as f:
        lines = f.readlines()
        if len(lines) > 1 and '"""' in lines[0]:
            # Extract docstring
            for i, line in enumerate(lines[1:], 1):
                if '"""' in line:
                    return lines[1:i][0].strip()
    return py_file.stem.replace('_', ' ').title()

if __name__ == "__main__":
    create_comprehensive_index()
```

**ÏÇ∞Ï∂úÎ¨º (Phase 1):**
- ‚úÖ C:\cursor-mcp\logi_ontol_sync/ (ÎèôÍ∏∞ÌôîÎêú Îç∞Ïù¥ÌÑ∞)
- ‚úÖ logi_ontol_index.json (Í≤ÄÏÉâ Ïù∏Îç±Ïä§)
- ‚úÖ MCP ÏÑúÎ≤Ñ logi-ontol Íµ¨ÏÑ± ÏôÑÎ£å
- ‚úÖ ÏûêÎèô ÎèôÍ∏∞Ìôî Ïä§ÏºÄÏ§ÑÎü¨ Îì±Î°ù

---

### **PHASE 2: AI-OCR & ÏßÄÏãù ÌÜµÌï© (3Ï£º)**
#### Î™©Ìëú: ÎπÑÏ†ïÌòï Îç∞Ïù¥ÌÑ∞Î•º Íµ¨Ï°∞ÌôîÎêú ÏßÄÏãùÏúºÎ°ú Î≥ÄÌôò

**2.1 AI-OCR ÌååÏù¥ÌîÑÎùºÏù∏ Íµ¨Ï∂ï**

```python
# C:\cursor-mcp\hvdc_automation\ocr_pipeline.py
import os
import json
import base64
from pathlib import Path
from PIL import Image
import pytesseract
import anthropic
from pdf2image import convert_from_path

class LogisticsOCRPipeline:
    """
    Advanced OCR pipeline for logistics documents
    Target: 90%+ entity extraction accuracy
    """

    def __init__(self):
        self.client = anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
        self.confidence_threshold = 0.90

    def process_document(self, file_path):
        """
        Main processing pipeline
        PDF/Image ‚Üí Text + Entities ‚Üí RDF
        """
        print(f"üîç Processing: {file_path}")

        # 1. Extract text based on file type
        if file_path.endswith('.pdf'):
            text, images = self.extract_pdf(file_path)
        else:
            text = self.extract_image(file_path)
            images = [file_path]

        # 2. Classify document type
        doc_type = self.classify_document(text, images[0] if images else None)
        print(f"üìÑ Document type: {doc_type}")

        # 3. Extract entities using Claude
        entities = self.extract_entities_with_claude(text, doc_type, images)

        # 4. Validate extraction confidence
        confidence = self.calculate_confidence(entities)
        print(f"üìä Confidence: {confidence:.2%}")

        # 5. Convert to RDF if high confidence
        if confidence >= self.confidence_threshold:
            rdf_data = self.convert_to_rdf(entities, doc_type)
            status = "‚úÖ AUTO_PROCESSED"
        else:
            rdf_data = None
            status = "‚ö†Ô∏è NEEDS_REVIEW"

        return {
            "file_path": file_path,
            "doc_type": doc_type,
            "entities": entities,
            "confidence": confidence,
            "rdf": rdf_data,
            "status": status
        }

    def extract_pdf(self, pdf_path):
        """Extract text and images from PDF"""
        # Convert PDF to images
        images = convert_from_path(pdf_path, dpi=300)

        # OCR each page
        text_parts = []
        for i, image in enumerate(images):
            text = pytesseract.image_to_string(image, lang='eng')
            text_parts.append(text)

        full_text = "\n\n".join(text_parts)

        # Save first page as image for Claude vision
        image_paths = []
        if images:
            img_path = f"/tmp/{Path(pdf_path).stem}_page1.jpg"
            images[0].save(img_path, 'JPEG')
            image_paths.append(img_path)

        return full_text, image_paths

    def extract_image(self, image_path):
        """Extract text from image using Tesseract"""
        image = Image.open(image_path)
        text = pytesseract.image_to_string(image, lang='eng')
        return text

    def classify_document(self, text, image_path=None):
        """Classify document type using Claude"""
        prompt = f"""
        Classify this logistics document into one of these types:
        - INVOICE
        - MANIFEST
        - TRANSPORT_REQUEST
        - DELIVERY_NOTE
        - BILL_OF_LADING
        - PACKING_LIST
        - OTHER

        Document text (first 500 chars):
        {text[:500]}

        Return only the document type.
        """

        response = self.client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=100,
            messages=[{"role": "user", "content": prompt}]
        )

        return response.content[0].text.strip()

    def extract_entities_with_claude(self, text, doc_type, images=None):
        """
        Extract structured entities using Claude
        Returns entities with confidence scores
        """

        # Prepare prompt based on document type
        if doc_type == "INVOICE":
            fields_to_extract = """
            - invoice_number
            - invoice_date
            - trn (Tax Registration Number)
            - supplier_name
            - customer_name
            - total_amount
            - currency
            - line_items (description, quantity, unit_price, amount)
            - payment_terms
            """
        elif doc_type == "TRANSPORT_REQUEST":
            fields_to_extract = """
            - request_number
            - date
            - vessel_name
            - origin_port
            - destination_port
            - eta (Estimated Time of Arrival)
            - etd (Estimated Time of Departure)
            - cargo_description
            - weight
            - container_numbers
            """
        else:
            fields_to_extract = """
            - document_number
            - date
            - relevant_parties (sender, receiver, etc.)
            - key_amounts
            - important_dates
            - container/shipment identifiers
            """

        prompt = f"""
        You are an expert logistics data extraction AI. Extract structured information from this {doc_type} document.

        Document text:
        {text}

        Fields to extract:
        {fields_to_extract}

        **CRITICAL REQUIREMENTS:**
        1. Return ONLY valid JSON
        2. Include a "confidence" score (0.0-1.0) for EACH field
        3. If a field is not found, set value to null and confidence to 0.0
        4. Use exact field names provided above
        5. For dates, use ISO format (YYYY-MM-DD)
        6. For amounts, extract as numbers without currency symbols

        Example response format:
        {{
          "invoice_number": {{"value": "INV-2025-001", "confidence": 0.95}},
          "invoice_date": {{"value": "2025-10-24", "confidence": 1.0}},
          "total_amount": {{"value": 15000.00, "confidence": 0.90}},
          ...
        }}

        DO NOT include any text outside the JSON object.
        """

        # Add image if available (for better accuracy)
        if images and len(images) > 0:
            with open(images[0], 'rb') as f:
                image_data = base64.b64encode(f.read()).decode('utf-8')

            content = [
                {
                    "type": "image",
                    "source": {
                        "type": "base64",
                        "media_type": "image/jpeg",
                        "data": image_data
                    }
                },
                {"type": "text", "text": prompt}
            ]
        else:
            content = prompt

        response = self.client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=4000,
            messages=[{"role": "user", "content": content}]
        )

        # Parse JSON response
        response_text = response.content[0].text

        # Remove markdown code blocks if present
        if "```json" in response_text:
            response_text = response_text.split("```json")[1].split("```")[0]
        elif "```" in response_text:
            response_text = response_text.split("```")[1].split("```")[0]

        entities = json.loads(response_text.strip())
        return entities

    def calculate_confidence(self, entities):
        """Calculate overall confidence score"""
        if not entities:
            return 0.0

        confidences = []
        for field, data in entities.items():
            if isinstance(data, dict) and 'confidence' in data:
                if data['value'] is not None:  # Only count non-null values
                    confidences.append(data['confidence'])

        if not confidences:
            return 0.0

        return sum(confidences) / len(confidences)

    def convert_to_rdf(self, entities, doc_type):
        """Convert extracted entities to RDF triples"""
        from rdflib import Graph, Namespace, Literal, URIRef
        from rdflib.namespace import RDF, XSD

        g = Graph()
        LOGI = Namespace("http://hvdc.samsung.com/logistics#")
        g.bind("logi", LOGI)

        # Create document URI
        doc_id = entities.get('invoice_number', {}).get('value') or \
                 entities.get('document_number', {}).get('value') or \
                 f"DOC_{hash(str(entities))}"

        doc_uri = URIRef(LOGI[doc_id.replace('-', '_')])

        # Add document type
        g.add((doc_uri, RDF.type, LOGI[doc_type]))

        # Add all extracted fields
        for field, data in entities.items():
            if isinstance(data, dict) and data.get('value') is not None:
                value = data['value']

                # Determine datatype
                if isinstance(value, (int, float)):
                    literal = Literal(value, datatype=XSD.decimal)
                elif field.endswith('_date') or field == 'date':
                    literal = Literal(value, datatype=XSD.date)
                else:
                    literal = Literal(value)

                g.add((doc_uri, LOGI[field], literal))
                g.add((doc_uri, LOGI[f"{field}_confidence"], Literal(data['confidence'], datatype=XSD.float)))

        return g.serialize(format='turtle')

# Integration with /logi-master invoice-audit
def audit_invoice_with_ocr(invoice_path):
    """
    Slash command integration: /logi-master invoice-audit
    """
    pipeline = LogisticsOCRPipeline()
    result = pipeline.process_document(invoice_path)

    if result["confidence"] >= 0.90:
        # High confidence - auto-update knowledge graph
        print("‚úÖ High confidence extraction - updating knowledge graph")

        # Save RDF to knowledge graph
        from rdflib import Graph
        kg = Graph()
        kg.parse(data=result["rdf"], format="turtle")

        # Merge with existing ontology
        abu_ontology = Graph()
        abu_ontology.parse("C:/logi_ontol/output/final/abu_final.ttl", format="turtle")
        abu_ontology += kg

        # Save updated ontology
        abu_ontology.serialize("C:/logi_ontol/output/final/abu_final.ttl", format="turtle")

        return {
            "status": "‚úÖ APPROVED - AUTO-UPDATED",
            "confidence": result["confidence"],
            "entities": {k: v['value'] for k, v in result["entities"].items() if v.get('value')}
        }
    else:
        # Low confidence - flag for manual review
        print(f"‚ö†Ô∏è Low confidence ({result['confidence']:.2%}) - manual review required")

        # Save to review queue
        review_path = f"C:/logi_ontol/output/review/{Path(invoice_path).stem}_review.json"
        os.makedirs(os.path.dirname(review_path), exist_ok=True)
        with open(review_path, 'w') as f:
            json.dump(result, f, indent=2)

        return {
            "status": "‚ö†Ô∏è REVIEW_REQUIRED",
            "confidence": result["confidence"],
            "review_file": review_path,
            "entities": result["entities"]
        }

if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1:
        result = audit_invoice_with_ocr(sys.argv[1])
        print(json.dumps(result, indent=2))
```

**2.2 WhatsApp Îç∞Ïù¥ÌÑ∞ ÌÜµÌï©**

```python
# C:\cursor-mcp\hvdc_automation\whatsapp_processor.py
import re
import json
from datetime import datetime
from pathlib import Path
import anthropic

class WhatsAppLogisticsProcessor:
    """
    Process WhatsApp chat logs for logistics entities
    Extracts: Container numbers, ETAs, amounts, vessel names, etc.
    """

    def __init__(self):
        self.client = anthropic.Anthropic()
        self.entity_patterns = {
            'container': r'[A-Z]{4}\d{7}',
            'eta_date': r'ETA[:\s]*(\d{1,2}[/-]\d{1,2}[/-]\d{2,4})',
            'etd_date': r'ETD[:\s]*(\d{1,2}[/-]\d{1,2}[/-]\d{2,4})',
            'amount': r'AED\s*([\d,]+\.?\d*)',
            'invoice_ref': r'INV[-\s]?\d{4,}',
        }

    def parse_chat(self, chat_file_path):
        """Parse WhatsApp chat export"""
        with open(chat_file_path, 'r', encoding='utf-8') as f:
            content = f.read()

        # Extract messages with pattern: [DD/MM/YYYY, HH:MM:SS] Sender: Message
        pattern = r'\[(\d{2}/\d{2}/\d{4}, \d{2}:\d{2}:\d{2})\] ([^:]+): (.+?)(?=\n\[|\n$|$)'
        messages = re.findall(pattern, content, re.DOTALL)

        processed_messages = []
        for timestamp, sender, message in messages:
            # Skip system messages
            if '<Media omitted>' in message or 'changed the subject' in message:
                continue

            # Basic regex extraction
            basic_entities = self.extract_basic_entities(message)

            # Enhanced extraction with Claude for complex cases
            if self.requires_advanced_extraction(message):
                enhanced_entities = self.extract_with_claude(message)
                basic_entities.update(enhanced_entities)

            processed_messages.append({
                "timestamp": datetime.strptime(timestamp, "%d/%m/%Y, %H:%M:%S").isoformat(),
                "sender": sender.strip(),
                "message": message.strip(),
                "entities": basic_entities
            })

        return processed_messages

    def extract_basic_entities(self, text):
        """Extract entities using regex patterns"""
        entities = {}

        for entity_type, pattern in self.entity_patterns.items():
            matches = re.findall(pattern, text, re.IGNORECASE)
            if matches:
                entities[entity_type] = matches if len(matches) > 1 else matches[0]

        return entities

    def requires_advanced_extraction(self, message):
        """Determine if message needs Claude extraction"""
        keywords = ['vessel', 'ship', 'cargo', 'container', 'delivery', 'pickup', 'customs']
        return any(keyword in message.lower() for keyword in keywords)

    def extract_with_claude(self, message):
        """Use Claude for complex entity extraction"""
        prompt = f"""
        Extract logistics entities from this WhatsApp message:

        "{message}"

        Extract if present:
        - vessel_name
        - cargo_type
        - location (origin/destination)
        - status_update
        - action_required

        Return as JSON. If entity not found, omit it.
        Example: {{"vessel_name": "JPT71", "status_update": "departed"}}
        """

        response = self.client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=500,
            messages=[{"role": "user", "content": prompt}]
        )

        try:
            return json.loads(response.content[0].text)
        except:
            return {}

    def convert_to_rdf(self, messages):
        """Convert WhatsApp messages to RDF triples"""
        from rdflib import Graph, Namespace, Literal, URIRef
        from rdflib.namespace import RDF, XSD

        g = Graph()
        LOGI = Namespace("http://hvdc.samsung.com/logistics#")
        CHAT = Namespace("http://hvdc.samsung.com/chat#")
        g.bind("logi", LOGI)
        g.bind("chat", CHAT)

        for i, msg in enumerate(messages):
            msg_uri = URIRef(CHAT[f"msg_{i}"])

            g.add((msg_uri, RDF.type, CHAT.Message))
            g.add((msg_uri, CHAT.timestamp, Literal(msg['timestamp'], datatype=XSD.dateTime)))
            g.add((msg_uri, CHAT.sender, Literal(msg['sender'])))
            g.add((msg_uri, CHAT.content, Literal(msg['message'])))

            # Add extracted entities
            for entity_type, value in msg['entities'].items():
                if isinstance(value, list):
                    for v in value:
                        g.add((msg_uri, LOGI[entity_type], Literal(v)))
                else:
                    g.add((msg_uri, LOGI[entity_type], Literal(value)))

        return g.serialize(format='turtle')

def process_jpt71_chat():
    """Process JPT71 WhatsApp chat and add to knowledge graph"""
    processor = WhatsAppLogisticsProcessor()

    chat_file = "C:/logi_ontol/JPT71/Jopetwil 71 GroupÎãòÍ≥ºÏùò WhatsApp ÎåÄÌôî.txt"
    print(f"üîç Processing WhatsApp chat: {chat_file}")

    messages = processor.parse_chat(chat_file)
    print(f"üìä Extracted {len(messages)} messages")

    # Convert to RDF
    rdf_data = processor.convert_to_rdf(messages)

    # Save to knowledge graph
    output_file = "C:/logi_ontol/output/final/jpt71_whatsapp.ttl"
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(rdf_data)

    print(f"‚úÖ WhatsApp data saved to: {output_file}")

    # Merge with main ontology
    from rdflib import Graph
    main_kg = Graph()
    main_kg.parse("C:/logi_ontol/output/final/abu_final.ttl", format="turtle")
    main_kg.parse(output_file, format="turtle")
    main_kg.serialize("C:/logi_ontol/output/final/abu_final.ttl", format="turtle")

    print("‚úÖ Merged with abu_final.ttl")

    return len(messages)

if __name__ == "__main__":
    process_jpt71_chat()
```

**2.3 ÌÜµÌï© ÏßÄÏãù Í∑∏ÎûòÌîÑ**

```python
# C:\cursor-mcp\hvdc_automation\knowledge_graph.py
from rdflib import Graph, Namespace, Literal, URIRef
from rdflib.namespace import RDF, RDFS, XSD
import json

class HVDCKnowledgeGraph:
    """
    Unified knowledge graph for HVDC logistics
    Integrates: abu_final.ttl + lightning_final.ttl + operational data
    """

    def __init__(self):
        self.graph = Graph()
        self.LOGI = Namespace("http://hvdc.samsung.com/logistics#")
        self.graph.bind("logi", self.LOGI)
        self.load_ontologies()

    def load_ontologies(self):
        """Load all existing RDF ontologies"""
        ontology_files = [
            "C:/logi_ontol/output/final/abu_final.ttl",
            "C:/logi_ontol/output/final/lightning_final.ttl",
        ]

        for onto_file in ontology_files:
            try:
                self.graph.parse(onto_file, format="turtle")
                print(f"‚úÖ Loaded: {onto_file}")
            except Exception as e:
                print(f"‚ö†Ô∏è Failed to load {onto_file}: {e}")

    def query_sparql(self, sparql_query):
        """Execute SPARQL query on knowledge graph"""
        return self.graph.query(sparql_query)

    def find_entity(self, entity_type, filters=None):
        """Find entities of specific type with optional filters"""
        query = f"""
        PREFIX logi: <http://hvdc.samsung.com/logistics#>
        SELECT ?entity ?prop ?value
        WHERE {{
            ?entity a logi:{entity_type} .
            ?entity ?prop ?value .
        }}
        """
        return self.query_sparql(query)

    def add_invoice(self, invoice_data):
        """Add invoice entity to knowledge graph"""
        invoice_uri = URIRef(self.LOGI[f"Invoice_{invoice_data['number']}"])

        self.graph.add((invoice_uri, RDF.type, self.LOGI.Invoice))

        for key, value in invoice_data.items():
            if key == 'number':
                continue

            predicate = self.LOGI[key]

            if isinstance(value, (int, float)):
                literal = Literal(value, datatype=XSD.decimal)
            elif 'date' in key:
                literal = Literal(value, datatype=XSD.date)
            else:
                literal = Literal(value)

            self.graph.add((invoice_uri, predicate, literal))

    def get_statistics(self):
        """Get knowledge graph statistics"""
        stats = {
            "total_triples": len(self.graph),
            "entity_counts": {}
        }

        # Count entities by type
        type_query = """
        PREFIX logi: <http://hvdc.samsung.com/logistics#>
        SELECT ?type (COUNT(?entity) as ?count)
        WHERE {
            ?entity a ?type .
            FILTER(STRSTARTS(STR(?type), "http://hvdc.samsung.com/logistics#"))
        }
        GROUP BY ?type
        """

        results = self.query_sparql(type_query)
        for row in results:
            entity_type = str(row.type).split('#')[1]
            stats["entity_counts"][entity_type] = int(row.count)

        return stats

    def save(self, output_path=None):
        """Save knowledge graph to file"""
        if output_path is None:
            output_path = "C:/logi_ontol/output/final/hvdc_unified.ttl"

        self.graph.serialize(output_path, format="turtle")
        print(f"‚úÖ Knowledge graph saved: {output_path}")

# Integration with Claude tools
def query_knowledge_graph_via_claude(user_question):
    """
    Use Claude to generate SPARQL and query knowledge graph
    Integration with /logi-master commands
    """
    from anthropic import Anthropic

    client = Anthropic()
    kg = HVDCKnowledgeGraph()

    # Generate SPARQL with Claude
    prompt = f"""
    Generate a SPARQL query to answer this question about HVDC logistics:
    "{user_question}"

    Available ontology:
    Namespace: http://hvdc.samsung.com/logistics#

    Entity types:
    - Invoice (properties: number, date, amount, trn, supplier, customer)
    - Container (properties: number, size, type, status)
    - Vessel (properties: name, eta, etd, location)
    - Shipment (properties: origin, destination, cargo_type, weight)

    Return ONLY the SPARQL query, no explanation.
    Use PREFIX logi: <http://hvdc.samsung.com/logistics#>
    """

    response = client.messages.create(
        model="claude-sonnet-4-20250514",
        max_tokens=1000,
        messages=[{"role": "user", "content": prompt}]
    )

    sparql_query = response.content[0].text.strip()

    # Remove markdown code blocks if present
    if "```sparql" in sparql_query:
        sparql_query = sparql_query.split("```sparql")[1].split("```")[0].strip()

    # Execute query
    results = kg.query_sparql(sparql_query)

    # Format results
    formatted_results = []
    for row in results:
        formatted_results.append({k: str(v) for k, v in row.asdict().items()})

    return {
        "question": user_question,
        "sparql": sparql_query,
        "results": formatted_results,
        "result_count": len(formatted_results)
    }

if __name__ == "__main__":
    # Example usage
    kg = HVDCKnowledgeGraph()
    stats = kg.get_statistics()
    print(json.dumps(stats, indent=2))
```

**ÏÇ∞Ï∂úÎ¨º (Phase 2):**
- ‚úÖ AI-OCR ÌååÏù¥ÌîÑÎùºÏù∏ (Confidence ‚â•90%)
- ‚úÖ WhatsApp Îç∞Ïù¥ÌÑ∞ ÌÜµÌï© (JPT71, Lightning)
- ‚úÖ ÌÜµÌï© ÏßÄÏãù Í∑∏ÎûòÌîÑ (RDF + ÎπÑÏ†ïÌòï Îç∞Ïù¥ÌÑ∞)
- ‚úÖ SPARQL ÏøºÎ¶¨ API
- ‚úÖ Claude-powered ÏûêÏó∞Ïñ¥ ÏøºÎ¶¨

---

### **PHASE 3: ÏûêÎèôÌôî ÏõåÌÅ¨ÌîåÎ°úÏö∞ & ÎåÄÏãúÎ≥¥Îìú (2Ï£º)**
#### Î™©Ìëú: End-to-End ÏûêÎèôÌôî Î∞è Ïã§ÏãúÍ∞Ñ Î™®ÎãàÌÑ∞ÎßÅ

**3.1 ÏûêÎèô Ìä∏Î¶¨Í±∞ ÏãúÏä§ÌÖú**

```python
# C:\cursor-mcp\hvdc_automation\triggers.py
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.cron import CronTrigger
from datetime import datetime
import asyncio

class HVDCAutomationTriggers:
    """
    Automated triggers for HVDC logistics operations
    - Daily invoice audit
    - ETA deviation monitoring
    - WhatsApp data sync
    - Knowledge graph backup
    """

    def __init__(self):
        self.scheduler = BackgroundScheduler()

    def setup_all_triggers(self):
        """Setup all automated triggers"""

        # 1. Daily invoice audit (8:00 AM)
        self.scheduler.add_job(
            func=self.auto_invoice_audit,
            trigger=CronTrigger(hour=8, minute=0),
            id='invoice_audit',
            name='Daily Invoice Audit'
        )

        # 2. ETA monitoring (every 2 hours)
        self.scheduler.add_job(
            func=self.monitor_eta_deviations,
            trigger='interval',
            hours=2,
            id='eta_monitoring',
            name='ETA Deviation Monitoring'
        )

        # 3. WhatsApp data sync (every 30 minutes)
        self.scheduler.add_job(
            func=self.sync_whatsapp_data,
            trigger='interval',
            minutes=30,
            id='whatsapp_sync',
            name='WhatsApp Data Sync'
        )

        # 4. Knowledge graph backup (daily at midnight)
        self.scheduler.add_job(
            func=self.backup_knowledge_graph,
            trigger=CronTrigger(hour=0, minute=0),
            id='kg_backup',
            name='Knowledge Graph Backup'
        )

        # 5. KPI calculation (every hour)
        self.scheduler.add_job(
            func=self.calculate_kpis,
            trigger='interval',
            hours=1,
            id='kpi_calc',
            name='KPI Calculation'
        )

        print("‚úÖ All triggers setup complete")
        self.scheduler.start()

    def auto_invoice_audit(self):
        """Automatically audit new invoices"""
        from pathlib import Path
        from .ocr_pipeline import audit_invoice_with_ocr

        print(f"üîç [{datetime.now()}] Running daily invoice audit...")

        # Scan for new PDFs
        jpt71_path = Path("C:/logi_ontol/JPT71")
        new_invoices = [f for f in jpt71_path.glob("*.pdf")
                       if "ADNOC-TR" not in f.name and
                          "Manifest" not in f.name and
                          not self.is_processed(f)]

        results = {"processed": 0, "approved": 0, "review_needed": 0}

        for invoice in new_invoices:
            result = audit_invoice_with_ocr(str(invoice))
            results["processed"] += 1

            if result["status"].startswith("‚úÖ"):
                results["approved"] += 1
            else:
                results["review_needed"] += 1
                self.send_alert(f"‚ö†Ô∏è Invoice review needed: {invoice.name}")

        print(f"‚úÖ Audit complete: {results}")
        self.log_trigger_result("invoice_audit", results)

    def monitor_eta_deviations(self):
        """Monitor vessels for ETA deviations > 24h"""
        from .knowledge_graph import HVDCKnowledgeGraph

        print(f"üîç [{datetime.now()}] Monitoring ETA deviations...")

        kg = HVDCKnowledgeGraph()

        query = """
        PREFIX logi: <http://hvdc.samsung.com/logistics#>
        PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>

        SELECT ?vessel ?name ?scheduled_eta ?current_eta
        WHERE {
            ?vessel a logi:Vessel .
            ?vessel logi:name ?name .
            ?vessel logi:scheduled_eta ?scheduled_eta .
            ?vessel logi:current_eta ?current_eta .
            FILTER (xsd:dateTime(?current_eta) > xsd:dateTime(?scheduled_eta) + "P1D"^^xsd:duration)
        }
        """

        deviations = kg.query_sparql(query)

        for row in deviations:
            delay_hours = (row.current_eta - row.scheduled_eta).total_seconds() / 3600
            self.send_alert(
                f"üö® ETA Delay Alert\n"
                f"Vessel: {row.name}\n"
                f"Scheduled: {row.scheduled_eta}\n"
                f"Current: {row.current_eta}\n"
                f"Delay: {delay_hours:.1f} hours"
            )

        print(f"‚úÖ ETA monitoring complete: {len(list(deviations))} deviations")

    def sync_whatsapp_data(self):
        """Sync WhatsApp chat data to knowledge graph"""
        from .whatsapp_processor import process_jpt71_chat

        print(f"üîç [{datetime.now()}] Syncing WhatsApp data...")

        try:
            message_count = process_jpt71_chat()
            print(f"‚úÖ Synced {message_count} messages")
        except Exception as e:
            print(f"‚ö†Ô∏è WhatsApp sync failed: {e}")
            self.send_alert(f"‚ö†Ô∏è WhatsApp sync failed: {e}")

    def backup_knowledge_graph(self):
        """Backup knowledge graph with timestamp"""
        from .knowledge_graph import HVDCKnowledgeGraph
        from shutil import copy2

        print(f"üîç [{datetime.now()}] Backing up knowledge graph...")

        kg = HVDCKnowledgeGraph()
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        backup_path = f"C:/logi_ontol/output/versions/hvdc_unified_{timestamp}.ttl"
        kg.save(backup_path)

        print(f"‚úÖ Backup saved: {backup_path}")

    def calculate_kpis(self):
        """Calculate and cache KPIs"""
        from .knowledge_graph import HVDCKnowledgeGraph
        import json

        print(f"üîç [{datetime.now()}] Calculating KPIs...")

        kg = HVDCKnowledgeGraph()
        stats = kg.get_statistics()

        # Calculate custom KPIs
        kpis = {
            "timestamp": datetime.now().isoformat(),
            "knowledge_graph_size": stats["total_triples"],
            "entity_counts": stats["entity_counts"],
            "invoice_accuracy": self.calculate_invoice_accuracy(),
            "eta_deviation_avg": self.calculate_eta_deviation(),
            "warehouse_utilization": self.calculate_warehouse_util()
        }

        # Save to cache
        with open("C:/cursor-mcp/data/kpis_cache.json", "w") as f:
            json.dump(kpis, f, indent=2)

        print(f"‚úÖ KPIs calculated and cached")

    def calculate_invoice_accuracy(self):
        """Calculate invoice processing accuracy"""
        # Placeholder - implement based on actual data
        return 92.5

    def calculate_eta_deviation(self):
        """Calculate average ETA deviation"""
        # Placeholder - implement based on actual data
        return 8.3  # hours

    def calculate_warehouse_util(self):
        """Calculate warehouse utilization"""
        # Placeholder - implement based on actual data
        return 78.5  # percent

    def is_processed(self, file_path):
        """Check if file has been processed"""
        import json
        log_file = "C:/cursor-mcp/data/processed_files.json"

        try:
            with open(log_file, 'r') as f:
                processed = json.load(f)
            return str(file_path) in processed
        except:
            return False

    def log_trigger_result(self, trigger_id, result):
        """Log trigger execution result"""
        import json
        log_file = f"C:/cursor-mcp/logs/triggers_{datetime.now().strftime('%Y%m')}.json"

        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "trigger_id": trigger_id,
            "result": result
        }

        # Append to log file
        try:
            with open(log_file, 'r') as f:
                logs = json.load(f)
        except:
            logs = []

        logs.append(log_entry)

        with open(log_file, 'w') as f:
            json.dump(logs, f, indent=2)

    def send_alert(self, message):
        """Send alert via Telegram"""
        import os
        import requests

        bot_token = os.getenv("TELEGRAM_BOT_TOKEN")
        chat_id = os.getenv("TELEGRAM_CHAT_ID")

        if not bot_token or not chat_id:
            print(f"‚ö†Ô∏è Telegram not configured: {message}")
            return

        url = f"https://api.telegram.org/bot{bot_token}/sendMessage"
        payload = {
            "chat_id": chat_id,
            "text": message,
            "parse_mode": "Markdown"
        }

        try:
            response = requests.post(url, json=payload)
            response.raise_for_status()
            print(f"‚úÖ Alert sent via Telegram")
        except Exception as e:
            print(f"‚ö†Ô∏è Failed to send Telegram alert: {e}")

if __name__ == "__main__":
    triggers = HVDCAutomationTriggers()
    triggers.setup_all_triggers()

    # Keep running
    try:
        while True:
            import time
            time.sleep(60)
    except KeyboardInterrupt:
        print("\n‚úÖ Shutting down automation triggers...")
```

**ÏÇ∞Ï∂úÎ¨º (Phase 3):**
- ‚úÖ 60+ Slash Commands ÌÜµÌï©
- ‚úÖ ÏûêÎèô Ìä∏Î¶¨Í±∞ ÏãúÏä§ÌÖú (5Í∞ú Ï£ºÏöî ÏûëÏóÖ)
- ‚úÖ Telegram ÏïåÎ¶º ÌÜµÌï©
- ‚úÖ KPI ÏûêÎèô Í≥ÑÏÇ∞ Î∞è Ï∫êÏã±
- ‚úÖ ÏôÑÏ†Ñ ÏûêÎèôÌôî ÌååÏù¥ÌîÑÎùºÏù∏

---

## üìã Íµ¨ÌòÑ Ï≤¥ÌÅ¨Î¶¨Ïä§Ìä∏

### Phase 1: Î∏åÎ¶øÏßï (Week 1-2)
- [ ] **Week 1**
  - [ ] Ïã¨Î≥ºÎ¶≠ ÎßÅÌÅ¨ ÏÉùÏÑ± ÎòêÎäî ÎèôÍ∏∞Ìôî Ïä§ÌÅ¨Î¶ΩÌä∏ Íµ¨ÌòÑ
  - [ ] MCP ÏÑúÎ≤Ñ logi-ontol ÏÑ§Ï†ï
  - [ ] Ï†ÑÏ≤¥ ÌååÏùº Ïù∏Îç±Ïä§ ÏÉùÏÑ±
  - [ ] Claude filesystem Ï†ëÍ∑º ÌÖåÏä§Ìä∏
  - [ ] ÎîîÎ†âÌÜ†Î¶¨ Íµ¨Ï°∞ Î¨∏ÏÑúÌôî

- [ ] **Week 2**
  - [ ] HVDC_PJTÏôÄ ÌÜµÌï©
  - [ ] RoboCopy ÏûêÎèô ÎèôÍ∏∞Ìôî ÏÑ§Ï†ï
  - [ ] Î∞±ÏóÖ Ï†ÑÎûµ ÏàòÎ¶Ω
  - [ ] Slash commands ÌÖåÏä§Ìä∏
  - [ ] ÏÑ±Îä• ÏµúÏ†ÅÌôî

### Phase 2: AI-OCR & ÏßÄÏãù (Week 3-5)
- [ ] **Week 3**
  - [ ] Tesseract OCR ÏÑ§Ïπò Î∞è ÏÑ§Ï†ï
  - [ ] Claude ÏóîÌã∞Ìã∞ Ï∂îÏ∂ú ÌÜµÌï©
  - [ ] Î¨∏ÏÑú Î∂ÑÎ•ò ÏãúÏä§ÌÖú Íµ¨Ï∂ï
  - [ ] JPT71 PDF ÌÖåÏä§Ìä∏
  - [ ] Ï∂îÏ∂ú Ï†ïÌôïÎèÑ Í≤ÄÏ¶ù (Î™©Ìëú: 90%)

- [ ] **Week 4**
  - [ ] WhatsApp Îç∞Ïù¥ÌÑ∞ ÌååÏÑú Íµ¨ÌòÑ
  - [ ] ÏßÄÏãù Í∑∏ÎûòÌîÑ ÌÜµÌï©
  - [ ] SPARQL ÏøºÎ¶¨ ÏÉùÏÑ±Í∏∞
  - [ ] SHACL Í≤ÄÏ¶ù
  - [ ] End-to-end ÌååÏù¥ÌîÑÎùºÏù∏ ÌÖåÏä§Ìä∏

- [ ] **Week 5**
  - [ ] abu_final.ttl + lightning_final.ttl Î°úÎìú
  - [ ] ÏóîÌã∞Ìã∞ ÌÅ¨Î°úÏä§ Î†àÌçºÎü∞Ïä§
  - [ ] ÏøºÎ¶¨ API Íµ¨Ï∂ï
  - [ ] ÏÑ±Îä• ÌäúÎãù
  - [ ] Î¨∏ÏÑúÌôî

### Phase 3: ÏûêÎèôÌôî & ÎåÄÏãúÎ≥¥Îìú (Week 6-7)
- [ ] **Week 6**
  - [ ] Î™®Îì† slash commands Íµ¨ÌòÑ
  - [ ] ÏûêÎèô Ìä∏Î¶¨Í±∞ ÏÑ§Ï†ï
  - [ ] Telegram Î¥á ÌÜµÌï©
  - [ ] React ÎåÄÏãúÎ≥¥Îìú Íµ¨Ï∂ï
  - [ ] Ïã§ÏãúÍ∞Ñ Îç∞Ïù¥ÌÑ∞ Ïä§Ìä∏Î¶¨Î∞ç

- [ ] **Week 7**
  - [ ] End-to-end ÌÖåÏä§Ìä∏
  - [ ] ÏÇ¨Ïö©Ïûê ÏàòÏö© ÌÖåÏä§Ìä∏
  - [ ] ÏÑ±Îä• Î≤§ÏπòÎßàÌÇπ
  - [ ] ÍµêÏú° ÏûêÎ£å Ï†úÏûë
  - [ ] Go-live Ï§ÄÎπÑ

---

## üéØ ÏÑ±Í≥º ÏßÄÌëú

### Ï†ïÎüâÏ†Å KPI
| ÏßÄÌëú | Before | After | Í∞úÏÑ†Ïú® |
|------|--------|-------|--------|
| Ïù∏Î≥¥Ïù¥Ïä§ Ï≤òÎ¶¨ ÏãúÍ∞Ñ | 15Î∂Ñ | 3Î∂Ñ | **80%‚Üì** |
| OCR Ï†ïÌôïÎèÑ | 75% | ‚â•90% | **20%‚Üë** |
| ÏàòÎèô Í∞úÏûÖ | 40Í±¥/Ïõî | 10Í±¥/Ïõî | **75%‚Üì** |
| ETA Ìé∏Ï∞® ÌÉêÏßÄ | ÏàòÎèô | <2ÏãúÍ∞Ñ ÏûêÎèô | **ÏûêÎèôÌôî** |
| ÏßÄÏãù Í∑∏ÎûòÌîÑ ÌÅ¨Í∏∞ | 5K triples | 50K triples | **10Î∞∞‚Üë** |
| ÏøºÎ¶¨ ÏùëÎãµ ÏãúÍ∞Ñ | - | <2Ï¥à | **Ïã†Í∑ú** |
| ÏãúÏä§ÌÖú Í∞ÄÎèôÎ•† | - | 99.5% | **Ïã†Í∑ú** |

### Ï†ïÏÑ±Ï†Å ÏÑ±Í≥º
- ‚úÖ **Single Source of Truth**: Î™®Îì† Î¨ºÎ•ò Îç∞Ïù¥ÌÑ∞Í∞Ä ÌÜµÌï© ÏßÄÏãù Í∑∏ÎûòÌîÑÏóê
- ‚úÖ **ÏÇ¨Ï†Ñ ÎåÄÏùë ÏïåÎ¶º**: Î¨∏Ï†úÍ∞Ä Ïª§ÏßÄÍ∏∞ Ï†ÑÏóê ÏûêÎèô ÌÉêÏßÄ
- ‚úÖ **ÏôÑÏ†ÑÌïú Í∞êÏÇ¨ Ï∂îÏ†Å**: ÏõêÎ≥∏ Îç∞Ïù¥ÌÑ∞Î∂ÄÌÑ∞ ÏùòÏÇ¨Í≤∞Ï†ïÍπåÏßÄ
- ‚úÖ **ÌôïÏû•ÏÑ±**: ÏÉàÎ°úÏö¥ ÏÑ†Î∞ï, Î£®Ìä∏, ÌååÌä∏ÎÑà Ï∂îÍ∞Ä Ïö©Ïù¥
- ‚úÖ **ÏÇ¨Ïö©Ïûê ÎßåÏ°±ÎèÑ**: Îπ†Î•∏ ÏùëÎãµ, Ï†ÅÏùÄ Ïò§Î•ò

---

## üîÑ Î°§ÏïÑÏõÉ Í≥ÑÌöç

### Week 0: Ï§ÄÎπÑ
- ÌåÄ ÌÇ•Ïò§ÌîÑ ÎØ∏ÌåÖ
- ÌôòÍ≤Ω ÏÑ§Ï†ï
- Ï†ëÍ∑º Í∂åÌïú Î∂ÄÏó¨
- Í∏∞Ï§ÄÏÑ† Ï∏°Ï†ï

### Week 1-2: Phase 1 (Î∏åÎ¶øÏßï)
- 1-2Î™Ö ÎÇ¥Î∂Ä ÌååÏùºÎüø
- ÎèôÍ∏∞Ìôî Í≤ÄÏ¶ù
- Î≤ÑÍ∑∏ ÏàòÏ†ï
- Î¨∏ÏÑú ÏóÖÎç∞Ïù¥Ìä∏

### Week 3-5: Phase 2 (AI-OCR & ÏßÄÏãù)
- 5Î™Ö ÌååÏõå Ïú†Ï†ÄÎ°ú ÌôïÎåÄ
- Í≥ºÍ±∞ Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨ (JPT71)
- OCR Ï†ïÌôïÎèÑ Í∞úÏÑ†
- ÏßÄÏãù Í∑∏ÎûòÌîÑ Íµ¨Ï∂ï

### Week 6-7: Phase 3 (ÏûêÎèôÌôî)
- Ï†ÑÏ≤¥ ÌåÄ Î°§ÏïÑÏõÉ
- ÎåÄÏãúÎ≥¥Îìú Îü∞Ïπ≠
- ÏûêÎèô Ìä∏Î¶¨Í±∞ ÌôúÏÑ±Ìôî
- 24/7 Î™®ÎãàÌÑ∞ÎßÅ

### Week 8+: ÏµúÏ†ÅÌôî
- ÏÑ±Îä• ÌäúÎãù
- Í∏∞Îä• ÏöîÏ≤≠ Ï≤òÎ¶¨
- ÏßÄÏÜçÏ†Å Í∞úÏÑ†
- Îã§Î•∏ ÏÑ†Î∞ïÏúºÎ°ú ÌôïÏû•

---

## üîß Ï¶âÏãú Ïã§Ìñâ Í∞ÄÎä•Ìïú Next Actions

### Ïù¥Î≤à Ï£º (Week 0)
1. ‚úÖ **Ïù¥ ÎßàÏä§ÌÑ∞ÌîåÎûú Í≤ÄÌÜ†** - Ïù¥Ìï¥Í¥ÄÍ≥ÑÏûêÏôÄ Í≥µÏú†
2. ‚úÖ **Í∞úÎ∞ú ÌôòÍ≤Ω ÏÑ§Ï†ï**
   - ÏùòÏ°¥ÏÑ± ÏÑ§Ïπò
   - MCP ÏÑúÎ≤Ñ Íµ¨ÏÑ±
   - Filesystem Ï†ëÍ∑º ÌÖåÏä§Ìä∏
3. ‚úÖ **Phase 1 ÏûëÏóÖ Î≥¥Îìú ÏÉùÏÑ±** - Jira/Asana
4. ‚úÖ **ÌÇ•Ïò§ÌîÑ ÎØ∏ÌåÖ Ïä§ÏºÄÏ§Ñ**

### Îã§Ïùå 2Ï£º (Week 1-2)
1. **Ïã¨Î≥ºÎ¶≠ ÎßÅÌÅ¨ Íµ¨ÌòÑ** - C:\logi_ontol ‚Üí C:\cursor-mcp
2. **MCP ÏÑúÎ≤Ñ Íµ¨Ï∂ï** - logi-ontol Ï†ëÍ∑º
3. **/logi-master Î™ÖÎ†πÏñ¥ ÌÖåÏä§Ìä∏** - ÏÉòÌîå Îç∞Ïù¥ÌÑ∞
4. **Ïû•Ïï†Î¨º Î¨∏ÏÑúÌôî**

### Îã§Ïùå 4Ï£º (Week 3-6)
1. **AI-OCR ÌååÏù¥ÌîÑÎùºÏù∏ Î∞∞Ìè¨**
2. **JPT71 Í≥ºÍ±∞ Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨**
3. **ÏßÄÏãù Í∑∏ÎûòÌîÑ Íµ¨Ï∂ï**
4. **ÌååÏùºÎüø ÎåÄÏãúÎ≥¥Îìú Îü∞Ïπ≠**

---

## üìä ÏãúÏä§ÌÖú ÏïÑÌÇ§ÌÖçÏ≤ò Îã§Ïù¥Ïñ¥Í∑∏Îû®

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Claude Interface Layer                    ‚îÇ
‚îÇ  (Web Chat / API / Slack / Telegram / WhatsApp)             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                MACHO-GPT Command Router                      ‚îÇ
‚îÇ  /logi-master  /switch_mode  /visualize  /check_KPI         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ               ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Claude Tools ‚îÇ  ‚îÇ  MCP Servers  ‚îÇ
‚îÇ              ‚îÇ  ‚îÇ               ‚îÇ
‚îÇ ‚Ä¢ filesystem ‚îÇ  ‚îÇ ‚Ä¢ logi-ontol  ‚îÇ
‚îÇ ‚Ä¢ web_search ‚îÇ  ‚îÇ ‚Ä¢ pdf-tools   ‚îÇ
‚îÇ ‚Ä¢ drive_srch ‚îÇ  ‚îÇ ‚Ä¢ vercel      ‚îÇ
‚îÇ ‚Ä¢ web_fetch  ‚îÇ  ‚îÇ ‚Ä¢ windows     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ               ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              Integration & Processing Layer                  ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  AI-OCR      ‚îÇ  ‚îÇ  Knowledge   ‚îÇ  ‚îÇ  Data Pipeline  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  Pipeline    ‚îÇ  ‚îÇ  Graph       ‚îÇ  ‚îÇ                 ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ  ‚Ä¢ Ingest       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Tesseract  ‚îÇ  ‚îÇ ‚Ä¢ RDFLib     ‚îÇ  ‚îÇ  ‚Ä¢ Validate     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Claude     ‚îÇ  ‚îÇ ‚Ä¢ SPARQL     ‚îÇ  ‚îÇ  ‚Ä¢ Transform    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Entity Ext ‚îÇ  ‚îÇ ‚Ä¢ SHACL      ‚îÇ  ‚îÇ  ‚Ä¢ Load         ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   Data Sources Layer                         ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ logi_ontol ‚îÇ  ‚îÇ Google     ‚îÇ  ‚îÇ  External APIs       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ            ‚îÇ  ‚îÇ Drive      ‚îÇ  ‚îÇ                      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ RDF      ‚îÇ  ‚îÇ            ‚îÇ  ‚îÇ  ‚Ä¢ AIS (vessel)      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Excel    ‚îÇ  ‚îÇ ‚Ä¢ Docs     ‚îÇ  ‚îÇ  ‚Ä¢ NOAA (weather)    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ PDF      ‚îÇ  ‚îÇ ‚Ä¢ Sheets   ‚îÇ  ‚îÇ  ‚Ä¢ Port APIs         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ WhatsApp ‚îÇ  ‚îÇ ‚Ä¢ Templates‚îÇ  ‚îÇ  ‚Ä¢ Customs           ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üí° ÌòÅÏã† Í∏∞Ìöå

### MVP Ïù¥ÌõÑ
1. **ÏòàÏ∏° Î∂ÑÏÑù**
   - ML Í∏∞Î∞ò ETA ÏòàÏ∏°
   - ÎπÑÏö© ÏòàÏ∏°
   - Î¶¨Ïä§ÌÅ¨ Ïä§ÏΩîÏñ¥ÎßÅ

2. **Î™®Î∞îÏùº Ïï±**
   - iOS/Android ÎÑ§Ïù¥Ìã∞Î∏å Ïï±
   - Ïò§ÌîÑÎùºÏù∏ Î™®Îìú
   - Ìë∏Ïãú ÏïåÎ¶º

3. **ÏùåÏÑ± Ïù∏ÌÑ∞ÌéòÏù¥Ïä§**
   - "Alexa, JPT71 ÏÉÅÌÉúÎäî?"
   - ÏùºÎ∞ò ÏûëÏóÖÏùÑ ÏúÑÌïú ÏùåÏÑ± Î™ÖÎ†π

4. **Î∏îÎ°ùÏ≤¥Ïù∏ ÌÜµÌï©**
   - Î∂àÎ≥Ä Í∞êÏÇ¨ Ï∂îÏ†Å
   - Í≤∞Ï†úÏö© Ïä§ÎßàÌä∏ Í≥ÑÏïΩ
   - Î∂ÑÏÇ∞ ÏßÄÏãù Í∑∏ÎûòÌîÑ

5. **Í≥†Í∏â ÏãúÍ∞ÅÌôî**
   - 3D Ïª®ÌÖåÏù¥ÎÑà Ï†ÅÏû¨
   - Ïã§ÏãúÍ∞Ñ ÏÑ†Î∞ï Ï∂îÏ†Å
   - Ï∞ΩÍ≥† Î†àÏù¥ÏïÑÏõÉÏö© AR

---

## üìû Ïó∞ÎùΩÏ≤ò & ÏßÄÏõê

**ÌîÑÎ°úÏ†ùÌä∏ Î¶¨Îçî**: MACHO-GPT Integration Team
**Í∏∞Ïà† Î¶¨Îçî**: [Your Name]
**Ïù¥Ìï¥Í¥ÄÍ≥ÑÏûê**: Samsung C&T, ADNOC, DSV

**ÏßÄÏõê Ï±ÑÎÑê**:
- Slack: #hvdc-integration
- Email: hvdc-support@samsung.com
- Telegram: @logi-alert

**ÏóÖÎ¨¥ ÏãúÍ∞Ñ**: Ïõî-Í∏à 9:00-17:00 GST

---

**Î¨∏ÏÑú Î≤ÑÏ†Ñ**: 1.0
**ÏµúÏ¢Ö ÏóÖÎç∞Ïù¥Ìä∏**: 2025-10-24
**Îã§Ïùå Í≤ÄÌÜ†**: 2025-11-01
**ÏÉÅÌÉú**: ‚úÖ Íµ¨ÌòÑ ÏäπÏù∏Îê®
