#!/usr/bin/env python3
"""
HVDC Excel to RDF Converter
Excel íŒŒì¼ì„ RDF/TTL í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""

from __future__ import annotations
import pandas as pd
import numpy as np
from rdflib import Graph, Namespace, URIRef, Literal, RDF, RDFS, OWL, XSD
from rdflib.plugins.sparql import prepareQuery
import json
import os
from pathlib import Path
from datetime import datetime, timedelta
from typing import Mapping
import warnings
warnings.filterwarnings('ignore')

from .normalize import normalize_columns

# ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì •ì˜
EX = Namespace("http://samsung.com/project-logistics#")
ns = {
    "ex": "http://samsung.com/project-logistics#",
    "rdf": "http://www.w3.org/1999/02/22-rdf-syntax-ns#",
    "rdfs": "http://www.w3.org/2000/01/rdf-schema#",
    "owl": "http://www.w3.org/2002/07/owl#",
    "xsd": "http://www.w3.org/2001/XMLSchema#"
}

# ë§¤í•‘ ê·œì¹™ ì •ì˜
FIELD_MAPPINGS = {
    "Case No.": "hasCase",
    "Date": "hasDate",
    "Location": "hasLocation",
    "Qty": "hasQuantity",
    "Amount": "hasAmount",
    "Handling Fee": "hasHandlingFee",
    "HVDC CODE": "hasHvdcCode",
    "HVDC CODE 2": "hasHvdcCode2",
    "HVDC CODE 3": "hasHvdcCode3",
    "HVDC CODE 4": "hasHvdcCode4",
    "Operation Month": "hasOperationMonth",
    "ETA": "hasETA",
    "SQM": "hasSQM",
    "Handling In freight ton": "hasHandlingIn",
    "Handling out Freight Ton": "hasHandlingOut",
    "CBM": "hasCBM",
    "Pkg": "hasPackage",
    "G.W(KG)": "hasGrossWeight",
    "N.W(kgs)": "hasNetWeight",
    "L(CM)": "hasLength",
    "W(CM)": "hasWidth",
    "H(CM)": "hasHeight"
}

def load_excel(path: str, sheet: str | int | None = 0, rename_map: Mapping[str, str] | None = None) -> pd.DataFrame:
    """Load Excel file and normalize columns"""
    df = pd.read_excel(path, sheet_name=sheet)
    return normalize_columns(df, rename_map=rename_map)

def convert_excel_to_rdf(excel_path: str, output_path: str = None) -> str:
    """
    Excel íŒŒì¼ì„ RDFë¡œ ë³€í™˜
    
    Args:
        excel_path: Excel íŒŒì¼ ê²½ë¡œ
        output_path: ì¶œë ¥ RDF íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: ìë™ ìƒì„±)
        
    Returns:
        str: ìƒì„±ëœ RDF íŒŒì¼ ê²½ë¡œ
    """
    print(f"ğŸ“Š Excel íŒŒì¼ ë¡œë“œ ì¤‘: {excel_path}")
    
    # Excel íŒŒì¼ ë¡œë“œ
    try:
        df = pd.read_excel(excel_path)
        print(f"âœ… ë¡œë“œ ì™„ë£Œ: {len(df)}í–‰, {len(df.columns)}ì—´")
    except Exception as e:
        print(f"âŒ Excel ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None
    
    # ì¶œë ¥ ê²½ë¡œ ì„¤ì •
    if output_path is None:
        excel_name = Path(excel_path).stem
        output_path = f"rdf_output/{excel_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.ttl"
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir = Path(output_path).parent
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # RDF ê·¸ë˜í”„ ìƒì„±
    g = Graph()
    
    # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë°”ì¸ë”©
    for prefix, uri in ns.items():
        g.bind(prefix, Namespace(uri))
    
    # ê° í–‰ì„ RDF íŠ¸ë¦¬í”Œë¡œ ë³€í™˜
    for idx, row in df.iterrows():
        # TransportEvent URI ìƒì„±
        event_uri = EX[f"TransportEvent_{idx+1:05d}"]
        g.add((event_uri, RDF.type, EX.TransportEvent))
        
        # ê° ì»¬ëŸ¼ì„ RDF í”„ë¡œí¼í‹°ë¡œ ë³€í™˜
        for col, val in row.items():
            if pd.isna(val) or col not in FIELD_MAPPINGS:
                continue
                
            prop = EX[FIELD_MAPPINGS[col]]
            
            # ë°ì´í„° íƒ€ì…ì— ë”°ë¥¸ Literal ìƒì„±
            if isinstance(val, (int, float)):
                if col in ["Qty", "Pkg"]:
                    lit = Literal(int(val), datatype=XSD.integer)
                else:
                    lit = Literal(float(val), datatype=XSD.decimal)
            elif isinstance(val, str):
                # ë‚ ì§œ ë¬¸ìì—´ì¸ì§€ í™•ì¸
                try:
                    date_val = pd.to_datetime(val)
                    lit = Literal(date_val.date(), datatype=XSD.date)
                except Exception:
                    lit = Literal(str(val))
            else:
                lit = Literal(str(val))
                
            g.add((event_uri, prop, lit))
    
    # RDF íŒŒì¼ ì €ì¥
    g.serialize(destination=output_path, format="turtle")
    print(f"âœ… RDF ë³€í™˜ ì™„ë£Œ: {output_path}")
    
    return output_path

def batch_convert_excel_to_rdf(input_dir: str, output_dir: str = "rdf_output") -> list[str]:
    """
    ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  Excel íŒŒì¼ì„ RDFë¡œ ë³€í™˜
    
    Args:
        input_dir: Excel íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        
    Returns:
        list[str]: ìƒì„±ëœ RDF íŒŒì¼ ê²½ë¡œ ëª©ë¡
    """
    input_path = Path(input_dir)
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    excel_files = list(input_path.glob("*.xlsx")) + list(input_path.glob("*.xls"))
    converted_files = []
    
    print(f"ğŸ“ Excel íŒŒì¼ {len(excel_files)}ê°œ ë°œê²¬")
    
    for excel_file in excel_files:
        try:
            output_file = output_path / f"{excel_file.stem}.ttl"
            result = convert_excel_to_rdf(str(excel_file), str(output_file))
            if result:
                converted_files.append(result)
        except Exception as e:
            print(f"âŒ {excel_file.name} ë³€í™˜ ì‹¤íŒ¨: {e}")
    
    print(f"âœ… ë°°ì¹˜ ë³€í™˜ ì™„ë£Œ: {len(converted_files)}ê°œ íŒŒì¼")
    return converted_files

def validate_excel_data(df: pd.DataFrame) -> dict:
    """
    Excel ë°ì´í„° ê²€ì¦
    
    Args:
        df: ê²€ì¦í•  DataFrame
        
    Returns:
        dict: ê²€ì¦ ê²°ê³¼
    """
    validation_result = {
        "total_rows": len(df),
        "total_columns": len(df.columns),
        "missing_values": df.isnull().sum().to_dict(),
        "duplicate_rows": df.duplicated().sum(),
        "data_types": df.dtypes.to_dict(),
        "required_columns": [],
        "missing_required": []
    }
    
    # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
    required_cols = ["Case No.", "Date", "Location", "Qty"]
    validation_result["required_columns"] = required_cols
    
    for col in required_cols:
        if col not in df.columns:
            validation_result["missing_required"].append(col)
    
    return validation_result

def generate_excel_summary(df: pd.DataFrame) -> dict:
    """
    Excel ë°ì´í„° ìš”ì•½ ìƒì„±
    
    Args:
        df: ìš”ì•½í•  DataFrame
        
    Returns:
        dict: ìš”ì•½ ì •ë³´
    """
    summary = {
        "file_info": {
            "total_rows": len(df),
            "total_columns": len(df.columns),
            "memory_usage": df.memory_usage(deep=True).sum(),
        },
        "data_quality": {
            "missing_values": df.isnull().sum().sum(),
            "duplicate_rows": df.duplicated().sum(),
            "unique_values": df.nunique().to_dict(),
        },
        "numeric_summary": {},
        "categorical_summary": {}
    }
    
    # ìˆ«ìí˜• ì»¬ëŸ¼ ìš”ì•½
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    if len(numeric_cols) > 0:
        summary["numeric_summary"] = df[numeric_cols].describe().to_dict()
    
    # ë²”ì£¼í˜• ì»¬ëŸ¼ ìš”ì•½
    categorical_cols = df.select_dtypes(include=['object']).columns
    for col in categorical_cols:
        summary["categorical_summary"][col] = {
            "unique_count": df[col].nunique(),
            "most_common": df[col].mode().iloc[0] if len(df[col].mode()) > 0 else None,
            "most_common_count": df[col].value_counts().iloc[0] if len(df[col].value_counts()) > 0 else 0
        }
    
    return summary
