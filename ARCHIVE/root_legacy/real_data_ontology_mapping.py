#!/usr/bin/env python3
"""
ì‹¤ì œ HVDC ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ ì˜¨í†¨ë¡œì§€ ë§¤í•‘
data í´ë”ì˜ Excel íŒŒì¼ë“¤ì„ RDFë¡œ ë³€í™˜
"""

import pandas as pd
import json
from datetime import datetime
from pathlib import Path
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def load_real_data():
    """ì‹¤ì œ ë°ì´í„° íŒŒì¼ ë¡œë“œ"""
    print("ğŸ“Š ì‹¤ì œ ë°ì´í„° íŒŒì¼ ë¡œë“œ ì¤‘...")
    
    data_files = {
        'HITACHI': 'data/HVDC WAREHOUSE_HITACHI(HE).xlsx',
        'SIMENSE': 'data/HVDC WAREHOUSE_SIMENSE(SIM).xlsx',
        'INVOICE': 'data/HVDC WAREHOUSE_INVOICE.xlsx'
    }
    
    loaded_data = {}
    
    for name, file_path in data_files.items():
        try:
            print(f"ğŸ“‹ {name} íŒŒì¼ ë¡œë“œ ì¤‘: {file_path}")
            df = pd.read_excel(file_path)
            loaded_data[name] = df
            print(f"âœ… {name}: {len(df)}í–‰ ë¡œë“œ ì™„ë£Œ")
            print(f"   ì»¬ëŸ¼: {list(df.columns)[:5]}...")  # ì²˜ìŒ 5ê°œ ì»¬ëŸ¼ë§Œ í‘œì‹œ
        except Exception as e:
            print(f"âŒ {name} íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            continue
    
    return loaded_data

def analyze_data_structure(loaded_data):
    """ë°ì´í„° êµ¬ì¡° ë¶„ì„"""
    print("\nğŸ” ë°ì´í„° êµ¬ì¡° ë¶„ì„")
    print("=" * 50)
    
    for name, df in loaded_data.items():
        print(f"\nğŸ“‹ {name} ë°ì´í„° ë¶„ì„:")
        print(f"   í–‰ ìˆ˜: {len(df)}")
        print(f"   ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}")
        print(f"   ì»¬ëŸ¼ ëª©ë¡:")
        for i, col in enumerate(df.columns):
            print(f"     {i+1:2d}. {col}")
        
        # ìƒ˜í”Œ ë°ì´í„° í‘œì‹œ
        print(f"   ìƒ˜í”Œ ë°ì´í„° (ì²˜ìŒ 3í–‰):")
        print(df.head(3).to_string(max_cols=5))
        print("-" * 30)

def load_mapping_rules():
    """ë§¤í•‘ ê·œì¹™ ë¡œë“œ"""
    try:
        with open('mapping_rules_v2.6.json', 'r', encoding='utf-8') as f:
            rules = json.load(f)
        print("âœ… ë§¤í•‘ ê·œì¹™ ë¡œë“œ ì„±ê³µ")
        return rules
    except FileNotFoundError:
        print("âŒ mapping_rules_v2.6.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None

def map_columns_to_ontology(df, rules, data_source):
    """ì»¬ëŸ¼ì„ ì˜¨í†¨ë¡œì§€ë¡œ ë§¤í•‘"""
    if not rules:
        return df
    
    field_map = rules.get('field_map', {})
    
    # ì»¬ëŸ¼ëª… ë§¤í•‘
    mapped_columns = {}
    for col in df.columns:
        # ì •í™•í•œ ë§¤ì¹­ ì‹œë„
        if col in field_map:
            mapped_columns[col] = field_map[col]
        # ë¶€ë¶„ ë§¤ì¹­ ì‹œë„ (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)
        else:
            for excel_col, rdf_prop in field_map.items():
                if col.lower().replace(' ', '').replace('_', '') == excel_col.lower().replace(' ', '').replace('_', ''):
                    mapped_columns[col] = rdf_prop
                    break
    
    print(f"ğŸ“‹ {data_source} ì»¬ëŸ¼ ë§¤í•‘ ê²°ê³¼:")
    for original, mapped in mapped_columns.items():
        print(f"   {original} â†’ {mapped}")
    
    return mapped_columns

def convert_to_rdf(loaded_data, rules):
    """ì‹¤ì œ ë°ì´í„°ë¥¼ RDFë¡œ ë³€í™˜"""
    print("\nğŸ”— ì‹¤ì œ ë°ì´í„°ë¥¼ RDFë¡œ ë³€í™˜ ì¤‘...")
    
    if not rules:
        print("âŒ ë§¤í•‘ ê·œì¹™ì´ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    namespace = rules.get('namespace', 'http://samsung.com/project-logistics#')
    
    # TTL í—¤ë”
    ttl_content = f"""# HVDC Real Data Ontology RDF
# Generated: {datetime.now().isoformat()}
# Source: HITACHI, SIMENSE, INVOICE Excel files

@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .
@prefix owl: <http://www.w3.org/2002/07/owl#> .
@prefix ex: <{namespace}> .

# Ontology Declaration
ex: a owl:Ontology ;
    rdfs:label "HVDC Real Data Warehouse Ontology" ;
    rdfs:comment "Real warehouse data from HITACHI, SIMENSE, INVOICE" ;
    owl:versionInfo "2.6" .

"""
    
    event_counter = 1
    
    # ê° ë°ì´í„°ì…‹ ì²˜ë¦¬
    for data_source, df in loaded_data.items():
        print(f"ğŸ”„ {data_source} ë°ì´í„° ì²˜ë¦¬ ì¤‘... ({len(df)}í–‰)")
        
        # ì»¬ëŸ¼ ë§¤í•‘
        column_mapping = map_columns_to_ontology(df, rules, data_source)
        
        # ìƒìœ„ 100í–‰ë§Œ ì²˜ë¦¬ (í…ŒìŠ¤íŠ¸ìš©)
        sample_df = df.head(100)
        
        for idx, row in sample_df.iterrows():
            event_id = f"TransportEvent_{event_counter:05d}"
            ttl_content += f"ex:{event_id} a ex:TransportEvent ;\n"
            ttl_content += f"    ex:hasDataSource \"{data_source}\" ;\n"
            
            # ë§¤í•‘ëœ ì»¬ëŸ¼ë“¤ ì²˜ë¦¬
            for original_col, rdf_prop in column_mapping.items():
                if original_col in row and pd.notna(row[original_col]):
                    value = row[original_col]
                    
                    # ë°ì´í„° íƒ€ì…ì— ë”°ë¥¸ ì²˜ë¦¬
                    if rdf_prop in ['hasQuantity', 'hasPackageCount']:
                        try:
                            int_value = int(float(str(value)))
                            ttl_content += f'    ex:{rdf_prop} "{int_value}"^^xsd:integer ;\n'
                        except:
                            continue
                    elif rdf_prop in ['hasAmount', 'hasTotalAmount', 'hasHandlingFee', 'hasCBM', 'hasWeight']:
                        try:
                            float_value = float(str(value))
                            ttl_content += f'    ex:{rdf_prop} "{float_value}"^^xsd:decimal ;\n'
                        except:
                            continue
                    elif rdf_prop in ['hasDate', 'hasStartDate', 'hasFinishDate', 'hasOperationMonth']:
                        try:
                            if isinstance(value, pd.Timestamp):
                                date_str = value.strftime('%Y-%m-%d')
                            else:
                                date_str = str(value)
                            ttl_content += f'    ex:{rdf_prop} "{date_str}"^^xsd:date ;\n'
                        except:
                            continue
                    else:
                        # ë¬¸ìì—´ ì²˜ë¦¬
                        clean_value = str(value).replace('"', '\\"').replace('\n', ' ').strip()
                        if clean_value:
                            ttl_content += f'    ex:{rdf_prop} "{clean_value}" ;\n'
            
            ttl_content = ttl_content.rstrip(';\n') + ' .\n\n'
            event_counter += 1
    
    # ì°½ê³  ì¸ìŠ¤í„´ìŠ¤ ì¶”ê°€
    warehouse_classification = rules.get('warehouse_classification', {})
    for storage_type, warehouses in warehouse_classification.items():
        for warehouse in warehouses:
            warehouse_id = warehouse.replace(' ', '_').replace('(', '').replace(')', '')
            
            if storage_type == 'Indoor':
                class_type = 'IndoorWarehouse'
            elif storage_type == 'Outdoor':
                class_type = 'OutdoorWarehouse'
            elif storage_type == 'Site':
                class_type = 'Site'
            elif storage_type == 'dangerous_cargo':
                class_type = 'DangerousCargoWarehouse'
            else:
                class_type = 'Warehouse'
            
            ttl_content += f"""ex:{warehouse_id} a ex:{class_type} ;
    rdfs:label "{warehouse}" ;
    ex:hasStorageType "{storage_type}" .

"""
    
    return ttl_content

def generate_real_data_sparql(rules):
    """ì‹¤ì œ ë°ì´í„°ìš© SPARQL ì¿¼ë¦¬ ìƒì„±"""
    print("ğŸ” ì‹¤ì œ ë°ì´í„°ìš© SPARQL ì¿¼ë¦¬ ìƒì„± ì¤‘...")
    
    if not rules:
        return None
    
    namespace = rules.get('namespace', 'http://samsung.com/project-logistics#')
    
    queries = f"""# HVDC ì‹¤ì œ ë°ì´í„° SPARQL ì¿¼ë¦¬ ëª¨ìŒ
# Generated: {datetime.now().isoformat()}

# 1. ë°ì´í„° ì†ŒìŠ¤ë³„ ì§‘ê³„
PREFIX ex: <{namespace}>
SELECT ?dataSource (COUNT(?event) AS ?eventCount) (SUM(?amount) AS ?totalAmount)
WHERE {{
    ?event rdf:type ex:TransportEvent ;
           ex:hasDataSource ?dataSource .
    OPTIONAL {{ ?event ex:hasAmount ?amount }}
}}
GROUP BY ?dataSource
ORDER BY DESC(?eventCount)

# 2. ì›”ë³„ íŠ¸ëœì­ì…˜ ë¶„ì„
PREFIX ex: <{namespace}>
SELECT ?month ?dataSource (COUNT(?event) AS ?eventCount)
WHERE {{
    ?event rdf:type ex:TransportEvent ;
           ex:hasDataSource ?dataSource ;
           ex:hasDate ?date .
    BIND(SUBSTR(STR(?date), 1, 7) AS ?month)
}}
GROUP BY ?month ?dataSource
ORDER BY ?month ?dataSource

# 3. ì°½ê³ ë³„ ì¬ê³  í˜„í™©
PREFIX ex: <{namespace}>
SELECT ?location (COUNT(?event) AS ?eventCount) (SUM(?qty) AS ?totalQty)
WHERE {{
    ?event rdf:type ex:TransportEvent ;
           ex:hasLocation ?location .
    OPTIONAL {{ ?event ex:hasQuantity ?qty }}
}}
GROUP BY ?location
ORDER BY DESC(?eventCount)

# 4. ë²¤ë”ë³„ ë¶„ì„ (HITACHI vs SIMENSE)
PREFIX ex: <{namespace}>
SELECT ?vendor (COUNT(?event) AS ?eventCount) (SUM(?amount) AS ?totalAmount)
WHERE {{
    ?event rdf:type ex:TransportEvent ;
           ex:hasVendor ?vendor .
    OPTIONAL {{ ?event ex:hasAmount ?amount }}
}}
GROUP BY ?vendor
ORDER BY DESC(?totalAmount)

# 5. ì¼€ì´ìŠ¤ë³„ ìƒì„¸ ì •ë³´
PREFIX ex: <{namespace}>
SELECT ?case ?dataSource ?location ?qty ?amount
WHERE {{
    ?event rdf:type ex:TransportEvent ;
           ex:hasCase ?case ;
           ex:hasDataSource ?dataSource .
    OPTIONAL {{ ?event ex:hasLocation ?location }}
    OPTIONAL {{ ?event ex:hasQuantity ?qty }}
    OPTIONAL {{ ?event ex:hasAmount ?amount }}
}}
ORDER BY ?case
LIMIT 20

# 6. í•˜ì—­ë¹„ ë¶„ì„
PREFIX ex: <{namespace}>
SELECT ?location (AVG(?handlingFee) AS ?avgHandlingFee) (SUM(?handlingFee) AS ?totalHandlingFee)
WHERE {{
    ?event rdf:type ex:TransportEvent ;
           ex:hasLocation ?location ;
           ex:hasHandlingFee ?handlingFee .
}}
GROUP BY ?location
ORDER BY DESC(?totalHandlingFee)
"""
    
    return queries

def save_real_data_outputs(ttl_content, sparql_queries):
    """ì‹¤ì œ ë°ì´í„° ê²°ê³¼ ì €ì¥"""
    print("\nğŸ’¾ ì‹¤ì œ ë°ì´í„° ê²°ê³¼ ì €ì¥ ì¤‘...")
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir = Path('rdf_output')
    output_dir.mkdir(exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    files_created = []
    
    # TTL íŒŒì¼ ì €ì¥
    if ttl_content:
        ttl_file = output_dir / f'hvdc_real_data_{timestamp}.ttl'
        with open(ttl_file, 'w', encoding='utf-8') as f:
            f.write(ttl_content)
        print(f"âœ… ì‹¤ì œ ë°ì´í„° RDF/TTL ì €ì¥: {ttl_file}")
        files_created.append(('RDF/TTL', ttl_file))
    
    # SPARQL ì¿¼ë¦¬ ì €ì¥
    if sparql_queries:
        sparql_file = output_dir / f'hvdc_real_queries_{timestamp}.sparql'
        with open(sparql_file, 'w', encoding='utf-8') as f:
            f.write(sparql_queries)
        print(f"âœ… ì‹¤ì œ ë°ì´í„° SPARQL ì¿¼ë¦¬ ì €ì¥: {sparql_file}")
        files_created.append(('SPARQL', sparql_file))
    
    return files_created

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ HVDC ì‹¤ì œ ë°ì´í„° ì˜¨í†¨ë¡œì§€ ë§¤í•‘")
    print("=" * 60)
    
    # 1ë‹¨ê³„: ì‹¤ì œ ë°ì´í„° ë¡œë“œ
    loaded_data = load_real_data()
    
    if not loaded_data:
        print("âŒ ë°ì´í„° íŒŒì¼ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # 2ë‹¨ê³„: ë°ì´í„° êµ¬ì¡° ë¶„ì„
    analyze_data_structure(loaded_data)
    
    # 3ë‹¨ê³„: ë§¤í•‘ ê·œì¹™ ë¡œë“œ
    rules = load_mapping_rules()
    
    # 4ë‹¨ê³„: RDF ë³€í™˜
    ttl_content = convert_to_rdf(loaded_data, rules)
    
    # 5ë‹¨ê³„: SPARQL ì¿¼ë¦¬ ìƒì„±
    sparql_queries = generate_real_data_sparql(rules)
    
    # 6ë‹¨ê³„: ê²°ê³¼ ì €ì¥
    if ttl_content and sparql_queries:
        files_created = save_real_data_outputs(ttl_content, sparql_queries)
        
        print("\nğŸ‰ ì‹¤ì œ ë°ì´í„° ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ì™„ë£Œ!")
        print("=" * 60)
        print("ğŸ“Š ì²˜ë¦¬ í†µê³„:")
        for name, df in loaded_data.items():
            print(f"   â€¢ {name}: {len(df)}í–‰ â†’ ìƒìœ„ 100í–‰ ë³€í™˜")
        
        print("\nğŸ“ ìƒì„±ëœ íŒŒì¼:")
        for file_type, file_path in files_created:
            print(f"   â€¢ {file_type}: {file_path}")
        
        print("\nğŸ”§ ì¶”ì²œ ëª…ë ¹ì–´:")
        print("   /cmd_real_data_query [ì‹¤ì œ ë°ì´í„° ì¿¼ë¦¬ ì‹¤í–‰]")
        print("   /cmd_data_validation [ë°ì´í„° ê²€ì¦]")
        print("   /cmd_warehouse_analysis [ì°½ê³  ë¶„ì„]")
    else:
        print("\nâŒ ì‹¤ì œ ë°ì´í„° ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ì‹¤íŒ¨")

if __name__ == "__main__":
    main() 