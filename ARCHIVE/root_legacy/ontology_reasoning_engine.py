#!/usr/bin/env python3
"""
HVDC ì˜¨í†¨ë¡œì§€ ì¶”ë¡  ì—”ì§„ (ì—…ê·¸ë ˆì´ë“œ v1.1)
/cmd_ontology_reasoning ëª…ë ¹ì–´ êµ¬í˜„
AI ê¸°ë°˜ íŒ¨í„´ ë°œê²¬ ë° ìë™ ì¶”ë¡  (ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ê³ ë„í™”)
"""

import pandas as pd
import json
from datetime import datetime, timedelta
from pathlib import Path
import logging
import time
import numpy as np
from collections import defaultdict, Counter
import re

# [UPDATE v1.1] ë¨¸ì‹ ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€
try:
    from sklearn.experimental import enable_iterative_imputer
    from sklearn.impute import IterativeImputer
    from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
    from sklearn.tree import DecisionTreeClassifier, export_text
    from sklearn.preprocessing import LabelEncoder
    ML_AVAILABLE = True
    print("âœ… ë¨¸ì‹ ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    print(f"âš ï¸ ë¨¸ì‹ ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ: {e}")
    print("ğŸ’¡ pip install scikit-learnìœ¼ë¡œ ì„¤ì¹˜í•˜ë©´ ML ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    ML_AVAILABLE = False

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class HVDCOntologyReasoner:
    """HVDC ì˜¨í†¨ë¡œì§€ ì¶”ë¡  ì—”ì§„ (ì—…ê·¸ë ˆì´ë“œ v1.1)"""
    
    def __init__(self, config_path='config.json'):
        self.data = {}
        self.rules = {}
        self.inferences = []
        self.patterns = {}
        self.anomalies = []
        self.config = self._load_config(config_path)
        self.ml_available = ML_AVAILABLE

    # [UPDATE v1.1] ì„¤ì • íŒŒì¼ ë¡œë“œ ê¸°ëŠ¥ ì¶”ê°€
    def _load_config(self, config_path):
        """ì„¤ì • íŒŒì¼(ë°ì´í„° ê²½ë¡œ ë“±)ì„ ë¡œë“œí•˜ì—¬ ìœ ì—°ì„± ì¦ëŒ€"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                print(f"âœ… ì„¤ì • íŒŒì¼ ë¡œë“œ ì„±ê³µ: {config_path}")
                return json.load(f)
        except FileNotFoundError:
            print(f"âš ï¸ ì„¤ì • íŒŒì¼({config_path})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ìœ¼ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
            return {
                "data_files": {
                    'HITACHI': 'data/HVDC WAREHOUSE_HITACHI(HE).xlsx',
                    'SIMENSE': 'data/HVDC WAREHOUSE_SIMENSE(SIM).xlsx',
                    'INVOICE': 'data/HVDC WAREHOUSE_INVOICE.xlsx'
                },
                "mapping_rules_file": "mapping_rules_v2.6.json"
            }

    def load_data_and_rules(self):
        """ë°ì´í„° ë° ê·œì¹™ ë¡œë“œ (ì„¤ì • íŒŒì¼ ê¸°ë°˜ìœ¼ë¡œ ë³€ê²½)"""
        print("ğŸ§  ì˜¨í†¨ë¡œì§€ ì¶”ë¡  ì—”ì§„ ì´ˆê¸°í™” ì¤‘...")
        
        # ì„¤ì • íŒŒì¼ì—ì„œ ë°ì´í„° íŒŒì¼ ê²½ë¡œ ë¡œë“œ
        data_files = self.config.get("data_files", {})
        
        for name, file_path in data_files.items():
            try:
                df = pd.read_excel(file_path)
                # [UPDATE v1.1] ë°ì´í„° íƒ€ì… ìë™ ë³€í™˜ ë° ì •ë¦¬
                df = self._preprocess_dataframe(df)
                self.data[name] = df
                print(f"âœ… {name}: {len(df):,}í–‰ ë¡œë“œ")
            except Exception as e:
                print(f"âŒ {name} ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # ë§¤í•‘ ê·œì¹™ ë¡œë“œ
        try:
            mapping_rules_file = self.config.get("mapping_rules_file", "mapping_rules_v2.6.json")
            if Path(mapping_rules_file).exists():
                with open(mapping_rules_file, 'r', encoding='utf-8') as f:
                    self.rules = json.load(f)
                print("âœ… ë§¤í•‘ ê·œì¹™ ë¡œë“œ ì™„ë£Œ")
            else:
                print(f"âš ï¸ ë§¤í•‘ ê·œì¹™ íŒŒì¼({mapping_rules_file})ì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ê·œì¹™ ì‚¬ìš©.")
                self.rules = self._create_default_rules()
        except Exception as e:
            print(f"âŒ ë§¤í•‘ ê·œì¹™ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.rules = self._create_default_rules()

    def _create_default_rules(self):
        """ê¸°ë³¸ ë§¤í•‘ ê·œì¹™ ìƒì„±"""
        return {
            "common_patterns": {
                "cbm_keywords": ["CBM", "Volume", "ë¶€í”¼"],
                "weight_keywords": ["Weight", "G.W", "ë¬´ê²Œ", "KG"],
                "location_keywords": ["Location", "ìœ„ì¹˜", "DSV", "AGI", "MIR"]
            }
        }

    # [UPDATE v1.1] ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜ ì¶”ê°€
    def _preprocess_dataframe(self, df):
        """ë°ì´í„°í”„ë ˆì„ì˜ ê¸°ë³¸ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        # ë‚ ì§œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ ì‹œë„
        for col in df.columns:
            if 'date' in col.lower() or 'month' in col.lower():
                try:
                    df[col] = pd.to_datetime(df[col], errors='coerce')
                except:
                    pass
        
        # ìˆ«ì í˜•ì‹ì—ì„œ ì‰¼í‘œ ì œê±° ë° ë³€í™˜
        for col in df.select_dtypes(include=['object']).columns:
            try:
                if df[col].dtype == 'object' and df[col].str.contains(',', na=False).any():
                    df[col] = pd.to_numeric(df[col].str.replace(',', '', regex=False), errors='ignore')
            except:
                pass
        
        return df

    def analyze_data_relationships(self):
        """ë°ì´í„° ê´€ê³„ ë¶„ì„ ë° ì¶”ë¡ """
        print("\nğŸ” ë°ì´í„° ê´€ê³„ ë¶„ì„ ë° ì¶”ë¡  ìˆ˜í–‰ ì¤‘...")
        
        relationships = {}
        
        for source_name, df in self.data.items():
            print(f"\nğŸ“Š {source_name} ë°ì´í„° ê´€ê³„ ë¶„ì„:")
            
            # 1. ì»¬ëŸ¼ ê°„ ìƒê´€ê´€ê³„ ë¶„ì„
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            if len(numeric_cols) > 1:
                try:
                    correlation_matrix = df[numeric_cols].corr()
                    high_correlations = []
                    
                    for i in range(len(correlation_matrix.columns)):
                        for j in range(i+1, len(correlation_matrix.columns)):
                            corr_value = correlation_matrix.iloc[i, j]
                            if abs(corr_value) > 0.7 and not np.isnan(corr_value):
                                high_correlations.append({
                                    'col1': correlation_matrix.columns[i],
                                    'col2': correlation_matrix.columns[j],
                                    'correlation': round(corr_value, 3)
                                })
                    
                    relationships[f'{source_name}_correlations'] = high_correlations
                    print(f"  ğŸ“ˆ ê°•í•œ ìƒê´€ê´€ê³„ ë°œê²¬: {len(high_correlations)}ê°œ")
                except Exception as e:
                    print(f"  âš ï¸ ìƒê´€ê´€ê³„ ë¶„ì„ ì˜¤ë¥˜: {e}")
            
            # 2. ë²”ì£¼í˜• ë°ì´í„° íŒ¨í„´ ë¶„ì„
            categorical_patterns = {}
            text_cols = df.select_dtypes(include=['object']).columns
            
            for col in text_cols[:5]:  # ìƒìœ„ 5ê°œ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ë§Œ ë¶„ì„
                if col in df.columns:
                    try:
                        value_counts = df[col].value_counts()
                        if 1 < len(value_counts) < 50:  # ê³ ìœ ê°’ì´ 1ê°œ ì´ˆê³¼, 50ê°œ ë¯¸ë§Œì¸ ê²½ìš°ë§Œ
                            categorical_patterns[col] = {
                                'unique_count': len(value_counts),
                                'top_values': value_counts.head(5).to_dict(),
                                'distribution': 'uniform' if value_counts.std() < value_counts.mean() * 0.5 else 'skewed'
                            }
                    except Exception as e:
                        print(f"    âš ï¸ {col} íŒ¨í„´ ë¶„ì„ ì˜¤ë¥˜: {e}")
            
            relationships[f'{source_name}_patterns'] = categorical_patterns
            print(f"  ğŸ“Š ë²”ì£¼í˜• íŒ¨í„´ ë¶„ì„: {len(categorical_patterns)}ê°œ ì»¬ëŸ¼")
        
        self.patterns['relationships'] = relationships
        return relationships

    # [MAJOR UPDATE v1.1] ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ì¶”ë¡ 
    def infer_business_rules(self):
        """ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ìë™ ì¶”ë¡  (Decision Tree ê¸°ë°˜)"""
        print("\nğŸ¯ ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ìë™ ì¶”ë¡  ì¤‘ (ML)...")
        
        business_rules = []

        if not self.ml_available:
            print("  âš ï¸ ML ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ì–´ ê¸°ë³¸ ê·œì¹™ ì¶”ë¡ ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return self._infer_basic_business_rules()

        # HITACHI ë°ì´í„°ì—ì„œ Location ì˜ˆì¸¡ ëª¨ë¸
        if 'HITACHI' in self.data:
            df = self.data['HITACHI'].copy()
            
            # ê°€ëŠ¥í•œ featureì™€ target ì¡°í•© ì‹œë„
            possible_features = ['CBM', 'Pkg', 'G.W(KG)', 'N.W(kgs)', 'L(CM)', 'W(CM)', 'H(CM)']
            possible_targets = ['Location', 'HVDC CODE 1', 'HVDC CODE 2']
            
            for target_col in possible_targets:
                if target_col not in df.columns:
                    continue
                    
                available_features = [f for f in possible_features if f in df.columns]
                if len(available_features) < 2:
                    continue
                
                try:
                    # ë¶„ì„ì— í•„ìš”í•œ ë°ì´í„°ë§Œ í•„í„°ë§ ë° ê²°ì¸¡ì¹˜ ì œê±°
                    df_clean = df[[target_col] + available_features].dropna()

                    if len(df_clean) > 50 and df_clean[target_col].nunique() > 1:
                        le = LabelEncoder()
                        X = df_clean[available_features]
                        y = le.fit_transform(df_clean[target_col].astype(str))

                        # ì˜ì‚¬ê²°ì •ë‚˜ë¬´ ëª¨ë¸ í•™ìŠµ
                        tree_model = DecisionTreeClassifier(
                            max_depth=4, 
                            min_samples_leaf=10, 
                            random_state=42
                        )
                        tree_model.fit(X, y)

                        # í•™ìŠµëœ ê·œì¹™ í…ìŠ¤íŠ¸ë¡œ ì¶”ì¶œ
                        tree_rules = export_text(
                            tree_model, 
                            feature_names=available_features, 
                            class_names=le.classes_.astype(str)
                        )
                        
                        accuracy = tree_model.score(X, y)
                        
                        business_rules.append({
                            'type': 'ml_inferred_rule',
                            'rule': f"'{target_col}' ì˜ˆì¸¡ ëª¨ë¸ (Decision Tree)",
                            'inference': f"{', '.join(available_features)} ê°’ì— ë”°ë¼ {target_col}ì´ ê²°ì •ë˜ëŠ” íŒ¨í„´ ë°œê²¬.",
                            'details': tree_rules.split('\n')[:10],  # ìƒìœ„ 10ê°œ ê·œì¹™ë§Œ í‘œì‹œ
                            'confidence': round(accuracy, 3),
                            'features_used': available_features,
                            'target': target_col
                        })
                        print(f"  âœ… {target_col} ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ. ì •í™•ë„: {accuracy:.2%}")
                        
                except Exception as e:
                    print(f"  âŒ {target_col} ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨: {e}")

        # INVOICE ë°ì´í„°ì—ì„œ ê¸ˆì•¡ ì˜ˆì¸¡ ëª¨ë¸
        if 'INVOICE' in self.data:
            df = self.data['INVOICE'].copy()
            
            if 'Amount' in df.columns and 'Weight (kg)' in df.columns:
                try:
                    df_clean = df[['Amount', 'Weight (kg)', 'CBM']].dropna()
                    
                    if len(df_clean) > 30:
                        X = df_clean[['Weight (kg)', 'CBM']]
                        y = df_clean['Amount']
                        
                        # íšŒê·€ ëª¨ë¸ í•™ìŠµ
                        rf_model = RandomForestRegressor(n_estimators=10, random_state=42)
                        rf_model.fit(X, y)
                        
                        score = rf_model.score(X, y)
                        feature_importance = dict(zip(X.columns, rf_model.feature_importances_))
                        
                        business_rules.append({
                            'type': 'ml_regression_rule',
                            'rule': 'Amount ì˜ˆì¸¡ ëª¨ë¸ (Random Forest)',
                            'inference': f"Weightì™€ CBMìœ¼ë¡œ Amountë¥¼ {score:.2%} ì •í™•ë„ë¡œ ì˜ˆì¸¡ ê°€ëŠ¥",
                            'feature_importance': {k: round(v, 3) for k, v in feature_importance.items()},
                            'confidence': round(score, 3)
                        })
                        print(f"  âœ… Amount ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ. RÂ² ì ìˆ˜: {score:.2%}")
                        
                except Exception as e:
                    print(f"  âŒ Amount ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨: {e}")

        self.inferences.extend(business_rules)
        print(f"âœ… {len(business_rules)}ê°œ ML ê¸°ë°˜ ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ì¶”ë¡  ì™„ë£Œ")
        return business_rules

    def _infer_basic_business_rules(self):
        """ê¸°ë³¸ í†µê³„ ê¸°ë°˜ ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ì¶”ë¡  (ML ì—†ì´)"""
        business_rules = []
        
        for source, df in self.data.items():
            # ê¸°ë³¸ í†µê³„ ê¸°ë°˜ ê·œì¹™
            if 'CBM' in df.columns and df['CBM'].notna().sum() > 10:
                avg_cbm = df['CBM'].mean()
                business_rules.append({
                    'type': 'statistical_rule',
                    'rule': f'{source} í‰ê·  CBM íŒ¨í„´',
                    'inference': f'{source}ì˜ í‰ê·  CBM: {avg_cbm:.2f}',
                    'confidence': 0.8
                })
        
        return business_rules

    def detect_anomalies(self):
        """ì´ìƒì¹˜ ë° ë¶ˆì¼ì¹˜ íƒì§€"""
        print("\nğŸš¨ ì´ìƒì¹˜ ë° ë°ì´í„° ë¶ˆì¼ì¹˜ íƒì§€ ì¤‘...")
        
        anomalies = []
        
        for source, df in self.data.items():
            print(f"ğŸ“Š {source} ì´ìƒì¹˜ ë¶„ì„:")
            
            # 1. ìˆ«ìí˜• ë°ì´í„° ì´ìƒì¹˜ (IQR ë°©ë²•)
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            for col in numeric_cols:
                if col in df.columns and df[col].notna().sum() > 10:
                    try:
                        Q1 = df[col].quantile(0.25)
                        Q3 = df[col].quantile(0.75)
                        IQR = Q3 - Q1
                        
                        if IQR > 0:
                            lower_bound = Q1 - 1.5 * IQR
                            upper_bound = Q3 + 1.5 * IQR
                            
                            outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
                            if len(outliers) > 0:
                                anomalies.append({
                                    'type': 'numeric_outlier',
                                    'source': source,
                                    'column': col,
                                    'outlier_count': len(outliers),
                                    'percentage': round(len(outliers) / len(df) * 100, 2),
                                    'bounds': f'[{lower_bound:.2f}, {upper_bound:.2f}]',
                                    'severity': 'high' if len(outliers) / len(df) > 0.05 else 'medium'
                                })
                    except Exception as e:
                        print(f"    âš ï¸ {col} ì´ìƒì¹˜ ë¶„ì„ ì˜¤ë¥˜: {e}")
            
            # 2. ê²°ì¸¡ê°’ íŒ¨í„´ ë¶„ì„
            missing_analysis = df.isnull().sum()
            high_missing = missing_analysis[missing_analysis > len(df) * 0.5]
            
            for col in high_missing.index:
                anomalies.append({
                    'type': 'high_missing_rate',
                    'source': source,
                    'column': col,
                    'missing_count': int(high_missing[col]),
                    'percentage': round(high_missing[col] / len(df) * 100, 2),
                    'severity': 'high' if high_missing[col] / len(df) > 0.8 else 'medium'
                })
        
        self.anomalies = anomalies
        print(f"âœ… {len(anomalies)}ê°œ ì´ìƒì¹˜/ë¶ˆì¼ì¹˜ íƒì§€ ì™„ë£Œ")
        return anomalies

    def generate_ontology_rules(self):
        """ì˜¨í†¨ë¡œì§€ ê·œì¹™ ìë™ ìƒì„±"""
        print("\nğŸ“œ ì˜¨í†¨ë¡œì§€ ê·œì¹™ ìë™ ìƒì„± ì¤‘...")
        
        ontology_rules = []
        
        # í´ë˜ìŠ¤ ê³„ì¸µ ê·œì¹™
        class_hierarchy_rules = [
            {
                'rule': 'IndoorWarehouse âŠ† Warehouse',
                'description': 'ì‹¤ë‚´ ì°½ê³ ëŠ” ì°½ê³ ì˜ í•˜ìœ„ í´ë˜ìŠ¤'
            },
            {
                'rule': 'OutdoorWarehouse âŠ† Warehouse', 
                'description': 'ì‹¤ì™¸ ì°½ê³ ëŠ” ì°½ê³ ì˜ í•˜ìœ„ í´ë˜ìŠ¤'
            },
            {
                'rule': 'Site âŠ† Warehouse',
                'description': 'í˜„ì¥ì€ ì°½ê³ ì˜ í•˜ìœ„ í´ë˜ìŠ¤'
            }
        ]
        
        # ì†ì„± ì œì•½ ê·œì¹™
        property_constraint_rules = [
            {
                'rule': 'hasAmount: TransportEvent â†’ xsd:decimal',
                'description': 'hasAmount ì†ì„±ì€ TransportEventì—ì„œ decimal ê°’ìœ¼ë¡œ'
            },
            {
                'rule': 'hasCBM: TransportEvent â†’ xsd:decimal',
                'description': 'hasCBM ì†ì„±ì€ TransportEventì—ì„œ decimal ê°’ìœ¼ë¡œ'
            },
            {
                'rule': 'hasPackageCount: TransportEvent â†’ xsd:integer',
                'description': 'hasPackageCount ì†ì„±ì€ TransportEventì—ì„œ integer ê°’ìœ¼ë¡œ'
            }
        ]
        
        # ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ê·œì¹™
        business_logic_rules = [
            {
                'rule': 'TransportEvent(x) âˆ§ hasAmount(x, amt) âˆ§ amt > 100000 â†’ HighValueCargo(x)',
                'description': 'ê¸ˆì•¡ì´ 100,000 ì´ˆê³¼ì¸ ì´ë²¤íŠ¸ëŠ” ê³ ê°€ í™”ë¬¼ë¡œ ë¶„ë¥˜'
            },
            {
                'rule': 'TransportEvent(x) âˆ§ hasCBM(x, cbm) âˆ§ cbm > 50 â†’ LargeCargo(x)',
                'description': 'CBMì´ 50 ì´ˆê³¼ì¸ ì´ë²¤íŠ¸ëŠ” ëŒ€í˜• í™”ë¬¼ë¡œ ë¶„ë¥˜'
            },
            {
                'rule': 'TransportEvent(x) âˆ§ hasLocation(x, loc) âˆ§ IndoorWarehouse(loc) â†’ hasStorageType(x, "indoor")',
                'description': 'ì‹¤ë‚´ ì°½ê³  ìœ„ì¹˜ì˜ ì´ë²¤íŠ¸ëŠ” ì‹¤ë‚´ ì €ì¥ íƒ€ì…'
            }
        ]
        
        # ëª¨ë“  ê·œì¹™ í†µí•©
        all_rules = class_hierarchy_rules + property_constraint_rules + business_logic_rules
        
        for rule_set in [class_hierarchy_rules, property_constraint_rules, business_logic_rules]:
            for rule in rule_set:
                rule_type = 'class_hierarchy' if rule in class_hierarchy_rules else \
                           'property_constraint' if rule in property_constraint_rules else \
                           'business_logic'
                
                ontology_rules.append({
                    'type': rule_type,
                    'rule': rule['rule'],
                    'description': rule['description']
                })
        
        print(f"âœ… {len(ontology_rules)}ê°œ ì˜¨í†¨ë¡œì§€ ê·œì¹™ ìƒì„± ì™„ë£Œ")
        return ontology_rules

    # [MAJOR UPDATE v1.1] ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ê²°ì¸¡ê°’ ì˜ˆì¸¡
    def predict_missing_values(self):
        """ê²°ì¸¡ê°’ ì˜ˆì¸¡ ë° ì¶”ë¡  (IterativeImputer ê¸°ë°˜)"""
        print("\nğŸ”® ê²°ì¸¡ê°’ ì˜ˆì¸¡ ë° ì¶”ë¡  ì¤‘ (ML)...")
        
        predictions = []
        
        if not self.ml_available:
            print("  âš ï¸ ML ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ì–´ ê¸°ë³¸ í†µê³„ ê¸°ë°˜ ì˜ˆì¸¡ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return self._predict_missing_basic()
        
        for source, df in self.data.items():
            print(f"ğŸ“Š {source} ê²°ì¸¡ê°’ ë¶„ì„:")
            
            # ìˆ«ìí˜• ë°ì´í„°ì— ëŒ€í•´ì„œë§Œ ì˜ˆì¸¡ ìˆ˜í–‰
            numeric_df = df.select_dtypes(include=[np.number])
            missing_cols = numeric_df.columns[numeric_df.isnull().any()].tolist()

            if not missing_cols:
                print(f"  âœ… {source}: ê²°ì¸¡ê°’ ì—†ìŒ")
                continue

            if len(numeric_df.dropna()) < 10:
                print(f"  âš ï¸ {source}: í•™ìŠµ ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ ML ì˜ˆì¸¡ ìŠ¤í‚µ.")
                continue

            try:
                # IterativeImputer: ë‹¤ë¥¸ ëª¨ë“  ì»¬ëŸ¼ì„ ì‚¬ìš©í•˜ì—¬ ê²°ì¸¡ê°’ì„ ì˜ˆì¸¡í•˜ëŠ” ì •êµí•œ ë°©ì‹
                imputer = IterativeImputer(
                    estimator=RandomForestRegressor(n_estimators=5, random_state=42),
                    max_iter=5, 
                    random_state=42,
                    tol=0.01
                )
                
                imputed_data = imputer.fit_transform(numeric_df)
                imputed_df = pd.DataFrame(imputed_data, columns=numeric_df.columns, index=numeric_df.index)
                
                print(f"  âœ… ML ê¸°ë°˜ ê²°ì¸¡ì¹˜ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ ë° ì ìš© ì™„ë£Œ.")

                for col in missing_cols:
                    missing_indices = df[col][df[col].isnull()].index
                    if not missing_indices.empty:
                        # ê²°ì¸¡ì´ ë°œìƒí–ˆë˜ ìœ„ì¹˜ì˜ ì˜ˆì¸¡ê°’ ì¶”ì¶œ
                        predicted_values = imputed_df.loc[missing_indices, col]
                        
                        predictions.append({
                            'source': source,
                            'column': col,
                            'missing_count': len(missing_indices),
                            'missing_percentage': round(len(missing_indices) / len(df) * 100, 2),
                            'predicted_value_example': round(predicted_values.iloc[0], 3) if not predicted_values.empty else 'N/A',
                            'predicted_mean': round(predicted_values.mean(), 3) if not predicted_values.empty else 'N/A',
                            'method': 'IterativeImputer (RandomForest)',
                            'confidence': 0.85
                        })

            except Exception as e:
                print(f"  âŒ {source}: ML ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ - {e}")

        print(f"âœ… {len(predictions)}ê°œ ì»¬ëŸ¼ì— ëŒ€í•œ ML ê²°ì¸¡ê°’ ì˜ˆì¸¡ ì™„ë£Œ")
        return predictions

    def _predict_missing_basic(self):
        """ê¸°ë³¸ í†µê³„ ê¸°ë°˜ ê²°ì¸¡ê°’ ì˜ˆì¸¡ (ML ì—†ì´)"""
        predictions = []
        
        for source, df in self.data.items():
            missing_analysis = df.isnull().sum()
            missing_cols = missing_analysis[missing_analysis > 0].index
            
            for col in missing_cols:
                if df[col].dtype in ['int64', 'float64']:
                    mean_val = df[col].mean()
                    predictions.append({
                        'source': source,
                        'column': col,
                        'missing_count': int(missing_analysis[col]),
                        'missing_percentage': round(missing_analysis[col] / len(df) * 100, 2),
                        'predicted_value_example': round(mean_val, 3) if not pd.isna(mean_val) else 'N/A',
                        'method': 'Mean Imputation',
                        'confidence': 0.6
                    })
        
        return predictions

    def save_reasoning_results(self, relationships, business_rules, anomalies, ontology_rules, predictions):
        """ì˜¨í†¨ë¡œì§€ ì¶”ë¡  ê²°ê³¼ ì €ì¥"""
        print("\nğŸ’¾ ì˜¨í†¨ë¡œì§€ ì¶”ë¡  ê²°ê³¼ ì €ì¥ ì¤‘...")
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = Path('reasoning_output')
        output_dir.mkdir(exist_ok=True)
        
        # ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±
        reasoning_report = {
            'metadata': {
                'generated_at': datetime.now().isoformat(),
                'engine_version': '1.1-ML' if self.ml_available else '1.1-Basic',
                'ml_enabled': self.ml_available,
                'total_records': sum(len(df) for df in self.data.values()),
                'data_sources': list(self.data.keys())
            },
            'data_relationships': relationships,
            'inferred_business_rules': business_rules,
            'anomalies_detected': anomalies,
            'ontology_rules': ontology_rules,
            'missing_value_predictions': predictions,
            'summary': {
                'relationships_found': len(relationships),
                'business_rules_inferred': len(business_rules),
                'anomalies_detected': len(anomalies),
                'ontology_rules_generated': len(ontology_rules),
                'missing_predictions': len(predictions)
            }
        }
        
        # JSON ë¦¬í¬íŠ¸ ì €ì¥
        json_file = output_dir / f'hvdc_reasoning_report_{timestamp}.json'
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(reasoning_report, f, ensure_ascii=False, indent=2, default=str)
        
        # ë§ˆí¬ë‹¤ìš´ ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±
        md_file = output_dir / f'hvdc_reasoning_summary_{timestamp}.md'
        self._generate_markdown_summary(reasoning_report, md_file)
        
        # SPARQL ì¿¼ë¦¬ ìƒì„±
        sparql_file = output_dir / f'hvdc_reasoning_queries_{timestamp}.sparql'
        self._generate_sparql_queries(ontology_rules, sparql_file)
        
        print(f"  ğŸ“Š JSON ë¦¬í¬íŠ¸: {json_file.name} ({json_file.stat().st_size:,} bytes)")
        print(f"  ğŸ“ ìš”ì•½ ë¦¬í¬íŠ¸: {md_file.name} ({md_file.stat().st_size:,} bytes)")
        print(f"  ğŸ” SPARQL ì¿¼ë¦¬: {sparql_file.name} ({sparql_file.stat().st_size:,} bytes)")
        
        return [json_file, md_file, sparql_file]

    def _generate_markdown_summary(self, report, output_file):
        """ë§ˆí¬ë‹¤ìš´ ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±"""
        content = f"""# HVDC ì˜¨í†¨ë¡œì§€ ì¶”ë¡  ë¦¬í¬íŠ¸

## ğŸ“Š ì‹¤í–‰ ì •ë³´
- **ìƒì„± ì¼ì‹œ**: {report['metadata']['generated_at']}
- **ì—”ì§„ ë²„ì „**: {report['metadata']['engine_version']}
- **ML í™œì„±í™”**: {'âœ… ì˜ˆ' if report['metadata']['ml_enabled'] else 'âŒ ì•„ë‹ˆì˜¤'}
- **ì´ ë ˆì½”ë“œ**: {report['metadata']['total_records']:,}ê°œ
- **ë°ì´í„° ì†ŒìŠ¤**: {', '.join(report['metadata']['data_sources'])}

## ğŸ¯ ì¶”ë¡  ê²°ê³¼ ìš”ì•½
- **ë°ì´í„° ê´€ê³„**: {report['summary']['relationships_found']}ê°œ ë°œê²¬
- **ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™**: {report['summary']['business_rules_inferred']}ê°œ ì¶”ë¡ 
- **ì´ìƒì¹˜ íƒì§€**: {report['summary']['anomalies_detected']}ê°œ ë°œê²¬
- **ì˜¨í†¨ë¡œì§€ ê·œì¹™**: {report['summary']['ontology_rules_generated']}ê°œ ìƒì„±
- **ê²°ì¸¡ê°’ ì˜ˆì¸¡**: {report['summary']['missing_predictions']}ê°œ ì»¬ëŸ¼

## ğŸ¤– ML ê¸°ë°˜ ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™

"""
        
        for rule in report['inferred_business_rules']:
            if rule['type'] == 'ml_inferred_rule':
                content += f"### {rule['rule']}\n"
                content += f"- **ì¶”ë¡ **: {rule['inference']}\n"
                content += f"- **ì‹ ë¢°ë„**: {rule['confidence']:.2%}\n"
                if 'features_used' in rule:
                    content += f"- **ì‚¬ìš© íŠ¹ì„±**: {', '.join(rule['features_used'])}\n"
                content += "\n"
        
        content += "\n## ğŸš¨ ì´ìƒì¹˜ íƒì§€ ê²°ê³¼\n\n"
        
        high_severity = [a for a in report['anomalies_detected'] if a.get('severity') == 'high']
        for anomaly in high_severity[:5]:  # ìƒìœ„ 5ê°œë§Œ
            if 'outlier_count' in anomaly:
                content += f"- **{anomaly['source']}.{anomaly['column']}**: {anomaly['outlier_count']}ê°œ ì´ìƒì¹˜ ({anomaly['percentage']:.1f}%)\n"
            elif 'missing_count' in anomaly:
                content += f"- **{anomaly['source']}.{anomaly['column']}**: {anomaly['missing_count']}ê°œ ê²°ì¸¡ê°’ ({anomaly['percentage']:.1f}%)\n"
            else:
                content += f"- **{anomaly['source']}.{anomaly.get('column', 'N/A')}**: {anomaly['type']} íƒì§€\n"
        
        content += "\n## ğŸ”® ê²°ì¸¡ê°’ ì˜ˆì¸¡\n\n"
        
        for pred in report['missing_value_predictions'][:10]:  # ìƒìœ„ 10ê°œë§Œ
            content += f"- **{pred['source']}.{pred['column']}**: {pred['missing_percentage']:.1f}% ê²°ì¸¡ â†’ ì˜ˆì¸¡ê°’ ì˜ˆì‹œ: {pred['predicted_value_example']}\n"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(content)

    def _generate_sparql_queries(self, ontology_rules, output_file):
        """SPARQL ì¿¼ë¦¬ ìƒì„±"""
        queries = []
        
        # ê³ ê°€ í™”ë¬¼ ìë™ ë¶„ë¥˜ ì¿¼ë¦¬
        queries.append("""
# ê³ ê°€ í™”ë¬¼ ìë™ ë¶„ë¥˜
PREFIX ex: <http://samsung.com/project-logistics#>
CONSTRUCT {
    ?event a ex:HighValueCargo .
} WHERE {
    ?event a ex:TransportEvent ;
           ex:hasAmount ?amount .
    FILTER(?amount > 100000)
}
""")
        
        # ëŒ€í˜• í™”ë¬¼ ìë™ ë¶„ë¥˜ ì¿¼ë¦¬
        queries.append("""
# ëŒ€í˜• í™”ë¬¼ ìë™ ë¶„ë¥˜
PREFIX ex: <http://samsung.com/project-logistics#>
CONSTRUCT {
    ?event a ex:LargeCargo .
} WHERE {
    ?event a ex:TransportEvent ;
           ex:hasCBM ?cbm .
    FILTER(?cbm > 50)
}
""")
        
        # ì°½ê³  íƒ€ì…ë³„ ì´ë²¤íŠ¸ ì¡°íšŒ
        queries.append("""
# ì°½ê³  íƒ€ì…ë³„ ì´ë²¤íŠ¸ ì¡°íšŒ
PREFIX ex: <http://samsung.com/project-logistics#>
SELECT ?warehouse ?warehouseType ?eventCount WHERE {
    {
        SELECT ?warehouse ?warehouseType (COUNT(?event) AS ?eventCount) WHERE {
            ?event a ex:TransportEvent ;
                   ex:hasLocation ?warehouse .
            ?warehouse a ?warehouseType .
            FILTER(?warehouseType IN (ex:IndoorWarehouse, ex:OutdoorWarehouse, ex:Site))
        }
        GROUP BY ?warehouse ?warehouseType
    }
}
ORDER BY DESC(?eventCount)
""")
        
        content = "# HVDC ì˜¨í†¨ë¡œì§€ ì¶”ë¡  SPARQL ì¿¼ë¦¬\n"
        content += f"# ìƒì„± ì¼ì‹œ: {datetime.now().isoformat()}\n\n"
        content += "\n".join(queries)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(content)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ§  /cmd_ontology_reasoning ì‹¤í–‰ (v1.1-ML)")
    print("=" * 70)
    print("ğŸ¤– HVDC ì˜¨í†¨ë¡œì§€ AI ì¶”ë¡  ì—”ì§„ (ë¨¸ì‹ ëŸ¬ë‹ ì—…ê·¸ë ˆì´ë“œ)")
    print("=" * 70)
    
    start_time = time.time()
    
    # ì¶”ë¡  ì—”ì§„ ì´ˆê¸°í™”
    reasoner = HVDCOntologyReasoner(config_path='config.json')
    
    # 1ë‹¨ê³„: ë°ì´í„° ë° ê·œì¹™ ë¡œë“œ
    reasoner.load_data_and_rules()
    
    if not reasoner.data:
        print("âŒ ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # 2ë‹¨ê³„: ë°ì´í„° ê´€ê³„ ë¶„ì„
    relationships = reasoner.analyze_data_relationships()
    
    # 3ë‹¨ê³„: ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ì¶”ë¡  (ML)
    business_rules = reasoner.infer_business_rules()
    
    # 4ë‹¨ê³„: ì´ìƒì¹˜ íƒì§€
    anomalies = reasoner.detect_anomalies()
    
    # 5ë‹¨ê³„: ì˜¨í†¨ë¡œì§€ ê·œì¹™ ìƒì„±
    ontology_rules = reasoner.generate_ontology_rules()
    
    # 6ë‹¨ê³„: ê²°ì¸¡ê°’ ì˜ˆì¸¡ (ML)
    predictions = reasoner.predict_missing_values()
    
    # 7ë‹¨ê³„: ê²°ê³¼ ì €ì¥
    result_files = reasoner.save_reasoning_results(
        relationships, business_rules, anomalies, ontology_rules, predictions
    )
    
    total_time = time.time() - start_time
    total_records = sum(len(df) for df in reasoner.data.values())
    
    print("\nğŸ‰ ì˜¨í†¨ë¡œì§€ ì¶”ë¡  ì™„ë£Œ!")
    print("=" * 70)
    print(f"ğŸ“Š ì²˜ë¦¬ ê²°ê³¼:")
    print(f"  â€¢ ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ")
    print(f"  â€¢ ë¶„ì„ ë ˆì½”ë“œ: {total_records:,}ê°œ")
    print(f"  â€¢ ë°ì´í„° ê´€ê³„: {len(relationships)}ê°œ ë°œê²¬")
    print(f"  â€¢ ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™: {len(business_rules)}ê°œ ì¶”ë¡ ")
    print(f"  â€¢ ì´ìƒì¹˜ íƒì§€: {len(anomalies)}ê°œ")
    print(f"  â€¢ ì˜¨í†¨ë¡œì§€ ê·œì¹™: {len(ontology_rules)}ê°œ ìƒì„±")
    print(f"  â€¢ ê²°ì¸¡ê°’ ì˜ˆì¸¡: {len(predictions)}ê°œ")
    
    if reasoner.ml_available:
        print(f"  â€¢ ML ê¸°ë°˜ ê·œì¹™: {len([r for r in business_rules if 'ml_' in r['type']])}ê°œ ì¶”ë¡ ")
        print(f"  â€¢ ML ê¸°ë°˜ ì˜ˆì¸¡: {len([p for p in predictions if 'RandomForest' in p.get('method', '')])}ê°œ ì»¬ëŸ¼")
    
    print("\nğŸ“ ìƒì„±ëœ íŒŒì¼:")
    for file_path in result_files:
        print(f"  â€¢ {file_path.name}")

    print("\nğŸ”§ ì¶”ì²œ ëª…ë ¹ì–´:")
    print("  /validate_ontology [ì˜¨í†¨ë¡œì§€ ê²€ì¦ ì‹¤í–‰]")
    print("  /predictive_analytics [ì˜ˆì¸¡ ë¶„ì„ ì‹¤í–‰]")
    print("  /visualize_data [ë°ì´í„° ì‹œê°í™” ì‹¤í–‰]")

if __name__ == "__main__":
    main() 