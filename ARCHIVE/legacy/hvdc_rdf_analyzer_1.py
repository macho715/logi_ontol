#!/usr/bin/env python3
"""
HVDC RDF Analyzer - Excel to RDF Conversion and SPARQL Analysis
í†µí•© Excel â†’ RDF ë³€í™˜ ë° SPARQL ë¶„ì„ ì‹œìŠ¤í…œ

Features:
- Excel íŒŒì¼ â†’ RDF ë³€í™˜
- SPARQL ì¿¼ë¦¬ ì‹¤í–‰
- ì°½ê³ ë³„ CBM ë¶„ì„
- ë¬¼ë¥˜ ë°ì´í„° ì˜ë¯¸ë¡ ì  ë¶„ì„
"""

import pandas as pd
import numpy as np
from rdflib import Graph, Namespace, URIRef, Literal, RDF, RDFS, OWL, XSD
from rdflib.plugins.sparql import prepareQuery
import json
import os
from pathlib import Path
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì •ì˜
EX = Namespace("http://samsung.com/project-logistics#")
ns = {
    "ex": "http://samsung.com/project-logistics#",
    "rdf": "http://www.w3.org/1999/02/22-rdf-syntax-ns#",
    "rdfs": "http://www.w3.org/2000/01/rdf-schema#",
    "owl": "http://www.w3.org/2002/07/owl#",
    "xsd": "http://www.w3.org/2001/XMLSchema#"
}

# í•„ë“œ ë§¤í•‘ ê·œì¹™
FIELD_MAPPINGS = {
    "Case No.": "hasCase",
    "no.": "hasSequenceNumber",
    "Shipment Invoice No.": "hasShipmentInvoiceNumber",
    "HVDC CODE": "hasHVDCCode",
    "HVDC CODE 1": "hasHVDCCode1",
    "HVDC CODE 2": "hasHVDCCode2",
    "HVDC CODE 3": "hasHVDCCode3",
    "HVDC CODE 4": "hasHVDCCode4",
    "HVDC CODE 5": "hasHVDCCode5",
    "EQ No": "hasEquipmentNumber",
    "Pkg": "hasPackageCount",
    "Storage": "hasStorageType",
    "Description": "hasDescription",
    "L(CM)": "hasLength",
    "W(CM)": "hasWidth", 
    "H(CM)": "hasHeight",
    "CBM": "hasCubicMeter",
    "N.W(kgs)": "hasNetWeight",
    "G.W(kgs)": "hasGrossWeight",
    "Stack": "hasStackInfo",
    "ETA": "hasETA",
    "ETD": "hasETD",
    "Date": "hasDate",
    "Operation Month": "hasOperationMonth",
    "DHL Warehouse": "hasWarehouse",
    "DSV Indoor": "hasWarehouse",
    "DSV Al Markaz": "hasWarehouse",
    "DSV Outdoor": "hasWarehouse",
    "AAA  Storage": "hasWarehouse",
    "Hauler Indoor": "hasWarehouse",
    "DSV MZP": "hasWarehouse",
    "MOSB": "hasWarehouse",
    "Shifting": "hasWarehouse",
    "DAS": "hasSite",
    "AGI": "hasSite",
    "SHU": "hasSite",
    "MIR": "hasSite",
    "Logistics Flow Code": "hasLogisticsFlowCode",
    "Status": "hasStatus",
    "wh handling": "hasWHHandling"
}

class HVDCRDFConverter:
    """HVDC Excel to RDF Converter with SPARQL Analysis"""
    
    def __init__(self):
        self.graph = Graph()
        self.setup_namespaces()
        self.setup_ontology_schema()
        
    def setup_namespaces(self):
        """ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì„¤ì •"""
        for prefix, uri in ns.items():
            self.graph.bind(prefix, Namespace(uri))
    
    def setup_ontology_schema(self):
        """ê¸°ë³¸ ì˜¨í†¨ë¡œì§€ ìŠ¤í‚¤ë§ˆ ì„¤ì •"""
        # ê¸°ë³¸ í´ë˜ìŠ¤ ì •ì˜
        self.graph.add((EX.TransportEvent, RDF.type, OWL.Class))
        self.graph.add((EX.HitachiCargo, RDFS.subClassOf, EX.TransportEvent))
        self.graph.add((EX.SiemensCargo, RDFS.subClassOf, EX.TransportEvent))
        self.graph.add((EX.Warehouse, RDF.type, OWL.Class))
        self.graph.add((EX.Site, RDF.type, OWL.Class))
        
        # ê¸°ë³¸ ì†ì„± ì •ì˜
        for field, prop in FIELD_MAPPINGS.items():
            prop_uri = EX[prop]
            self.graph.add((prop_uri, RDF.type, OWL.DatatypeProperty))
            self.graph.add((prop_uri, RDFS.domain, EX.TransportEvent))
    
    def preprocess_dataframe(self, df, source_name=""):
        """ë°ì´í„°í”„ë ˆì„ ì „ì²˜ë¦¬"""
        print(f"ğŸ”„ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘: {source_name}")
        
        # ê¸°ë³¸ ì „ì²˜ë¦¬
        df = df.copy()
        
        # ë‚ ì§œ ì»¬ëŸ¼ ì²˜ë¦¬
        date_columns = ['ETA', 'ETD', 'Date']
        for col in date_columns:
            if col in df.columns:
                df[col] = pd.to_datetime(df[col], errors='coerce')
        
        # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ì²˜ë¦¬
        numeric_columns = ['CBM', 'N.W(kgs)', 'G.W(kgs)', 'L(CM)', 'W(CM)', 'H(CM)', 'Pkg']
        for col in numeric_columns:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
        
        # CBM ì–‘ìˆ˜ ë³´ì •
        if 'CBM' in df.columns:
            cbm_mean = df['CBM'].mean()
            df.loc[df['CBM'] <= 0, 'CBM'] = cbm_mean
            print(f"   CBM ë³´ì •: â‰¤0 ê°’ â†’ í‰ê· ê°’ {cbm_mean:.2f} ëŒ€ì²´")
        
        # íŒ¨í‚¤ì§€ ìˆ˜ ë³´ì •
        if 'Pkg' in df.columns:
            df['Pkg'] = df['Pkg'].fillna(1)
        
        # ë²¤ë” í•„í„°ë§ (HE, SIMë§Œ ìœ ì§€)
        if 'HVDC CODE 3' in df.columns:
            initial_count = len(df)
            df = df[df['HVDC CODE 3'].isin(['HE', 'SIM'])].copy()
            print(f"   ë²¤ë” í•„í„°ë§: {initial_count} â†’ {len(df)} ë ˆì½”ë“œ")
        
        # ì¤‘ë³µ ì œê±°
        if 'Case No.' in df.columns:
            initial_count = len(df)
            df = df.drop_duplicates(subset=['Case No.'], keep='first')
            removed = initial_count - len(df)
            if removed > 0:
                print(f"   ì¤‘ë³µ ì œê±°: {removed}ê°œ ì¤‘ë³µ ì¼€ì´ìŠ¤ ì œê±°")
        
        # ë°ì´í„° ì†ŒìŠ¤ ì¶”ê°€
        df['data_source'] = source_name
        
        print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: {len(df)} ë ˆì½”ë“œ")
        return df
    
    def convert_excel_to_rdf(self, excel_file, sheet_name='Case List'):
        """Excel íŒŒì¼ì„ RDFë¡œ ë³€í™˜"""
        print(f"ğŸš€ Excel â†’ RDF ë³€í™˜ ì‹œì‘: {excel_file}")
        
        # Excel íŒŒì¼ ì½ê¸°
        if not Path(excel_file).exists():
            # data ë””ë ‰í† ë¦¬ì—ì„œ ì°¾ê¸°
            data_path = Path("data") / excel_file
            if data_path.exists():
                excel_file = str(data_path)
            else:
                raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {excel_file}")
        
        try:
            df = pd.read_excel(excel_file, sheet_name=sheet_name)
            print(f"ğŸ“Š ë°ì´í„° ë¡œë“œ: {len(df)} ë ˆì½”ë“œ")
        except Exception as e:
            print(f"âŒ Excel íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
            return None
        
        # ë°ì´í„° ì „ì²˜ë¦¬
        df = self.preprocess_dataframe(df, Path(excel_file).stem)
        
        # RDF ë³€í™˜
        self.create_rdf_from_dataframe(df, Path(excel_file).stem)
        
        print(f"âœ… RDF ë³€í™˜ ì™„ë£Œ: {len(self.graph)} íŠ¸ë¦¬í”Œ ìƒì„±")
        return self.graph
    
    def create_rdf_from_dataframe(self, df, source_name):
        """DataFrameì„ RDFë¡œ ë³€í™˜"""
        print(f"ğŸ”— RDF ê·¸ë˜í”„ ìƒì„± ì¤‘: {source_name}")
        
        warehouse_columns = ['DHL Warehouse', 'DSV Indoor', 'DSV Al Markaz', 'DSV Outdoor', 
                           'AAA  Storage', 'Hauler Indoor', 'DSV MZP', 'MOSB', 'Shifting']
        site_columns = ['DAS', 'AGI', 'SHU', 'MIR']
        
        for idx, row in df.iterrows():
            # TransportEvent URI ìƒì„±
            case_no = str(row.get('Case No.', f"case_{idx+1}")).replace(' ', '_')
            event_uri = EX[f"TransportEvent_{case_no}"]
            
            # ê¸°ë³¸ í´ë˜ìŠ¤ ì¶”ê°€
            self.graph.add((event_uri, RDF.type, EX.TransportEvent))
            
            # ë²¤ë”ë³„ í™”ë¬¼ í´ë˜ìŠ¤ ì¶”ê°€
            if 'HVDC CODE 3' in row and pd.notna(row['HVDC CODE 3']):
                vendor = str(row['HVDC CODE 3']).strip().upper()
                if vendor == 'HE':
                    self.graph.add((event_uri, RDF.type, EX.HitachiCargo))
                elif vendor == 'SIM':
                    self.graph.add((event_uri, RDF.type, EX.SiemensCargo))
            
            # ê° ì»¬ëŸ¼ì„ RDF ì†ì„±ìœ¼ë¡œ ë³€í™˜
            for col, value in row.items():
                if pd.isna(value) or col not in FIELD_MAPPINGS:
                    continue
                
                property_uri = EX[FIELD_MAPPINGS[col]]
                
                # ë°ì´í„° íƒ€ì…ì— ë”°ë¥¸ Literal ìƒì„±
                if isinstance(value, (int, np.integer)):
                    literal = Literal(int(value), datatype=XSD.integer)
                elif isinstance(value, (float, np.floating)):
                    literal = Literal(float(value), datatype=XSD.decimal)
                elif isinstance(value, datetime):
                    literal = Literal(value.date(), datatype=XSD.date)
                elif isinstance(value, pd.Timestamp):
                    literal = Literal(value.date(), datatype=XSD.date)
                else:
                    literal = Literal(str(value))
                
                self.graph.add((event_uri, property_uri, literal))
            
            # ì°½ê³  ì •ë³´ ì¶”ê°€ (ë‚ ì§œê°€ ìˆëŠ” ì°½ê³ ë“¤)
            warehouses = []
            for warehouse_col in warehouse_columns:
                if warehouse_col in row and pd.notna(row[warehouse_col]):
                    warehouses.append(warehouse_col)
            
            # í˜„ì¥ ì •ë³´ ì¶”ê°€ (ë‚ ì§œê°€ ìˆëŠ” í˜„ì¥ë“¤)
            sites = []
            for site_col in site_columns:
                if site_col in row and pd.notna(row[site_col]):
                    sites.append(site_col)
            
            # ì£¼ìš” ì°½ê³  ì •ë³´ ì¶”ê°€
            if warehouses:
                primary_warehouse = warehouses[0]  # ì²« ë²ˆì§¸ ì°½ê³ ë¥¼ ì£¼ìš” ì°½ê³ ë¡œ
                self.graph.add((event_uri, EX.hasWarehouse, Literal(primary_warehouse)))
            
            # ì£¼ìš” í˜„ì¥ ì •ë³´ ì¶”ê°€
            if sites:
                primary_site = sites[0]  # ì²« ë²ˆì§¸ í˜„ì¥ì„ ì£¼ìš” í˜„ì¥ìœ¼ë¡œ
                self.graph.add((event_uri, EX.hasSite, Literal(primary_site)))
    
    def query(self, sparql_query):
        """SPARQL ì¿¼ë¦¬ ì‹¤í–‰"""
        try:
            # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì ‘ë‘ì‚¬ ì¶”ê°€
            full_query = f"""
            PREFIX ex: <{EX}>
            PREFIX rdf: <{RDF}>
            PREFIX rdfs: <{RDFS}>
            PREFIX owl: <{OWL}>
            PREFIX xsd: <{XSD}>
            
            {sparql_query}
            """
            
            results = self.graph.query(full_query)
            return results
        except Exception as e:
            print(f"âŒ SPARQL ì¿¼ë¦¬ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return None
    
    def analyze_warehouse_cbm(self):
        """ì°½ê³ ë³„ CBM ë¶„ì„"""
        print("ğŸ“Š ì°½ê³ ë³„ CBM ë¶„ì„ ì‹œì‘")
        
        sparql_query = """
        SELECT ?warehouse (AVG(?cbm) as ?avg_cbm) (COUNT(?event) as ?count) (SUM(?cbm) as ?total_cbm) WHERE {
            ?event rdf:type ex:TransportEvent .
            ?event ex:hasWarehouse ?warehouse .
            ?event ex:hasCubicMeter ?cbm .
            FILTER(?cbm > 0)
        } GROUP BY ?warehouse
        ORDER BY DESC(?avg_cbm)
        """
        
        results = self.query(sparql_query)
        
        if results:
            print("\nğŸ“ˆ ì°½ê³ ë³„ CBM ë¶„ì„ ê²°ê³¼:")
            print("-" * 70)
            print(f"{'ì°½ê³ ëª…':<20} {'í‰ê·  CBM':<12} {'ì´ CBM':<12} {'ê±´ìˆ˜':<8}")
            print("-" * 70)
            
            for row in results:
                warehouse = str(row.warehouse)
                avg_cbm = float(row.avg_cbm)
                total_cbm = float(row.total_cbm) if row.total_cbm else 0
                count = int(row.count)
                
                print(f"{warehouse:<20} {avg_cbm:<12.2f} {total_cbm:<12.2f} {count:<8}")
        
        return results
    
    def analyze_vendor_distribution(self):
        """ë²¤ë”ë³„ ë¶„í¬ ë¶„ì„"""
        print("\nğŸ“Š ë²¤ë”ë³„ ë¶„í¬ ë¶„ì„ ì‹œì‘")
        
        sparql_query = """
        SELECT ?vendor (COUNT(?event) as ?count) (AVG(?cbm) as ?avg_cbm) WHERE {
            ?event rdf:type ex:TransportEvent .
            ?event ex:hasHVDCCode3 ?vendor .
            ?event ex:hasCubicMeter ?cbm .
            FILTER(?cbm > 0)
        } GROUP BY ?vendor
        ORDER BY DESC(?count)
        """
        
        results = self.query(sparql_query)
        
        if results:
            print("\nğŸ“ˆ ë²¤ë”ë³„ ë¶„í¬ ë¶„ì„ ê²°ê³¼:")
            print("-" * 50)
            print(f"{'ë²¤ë”':<10} {'ê±´ìˆ˜':<8} {'í‰ê·  CBM':<12}")
            print("-" * 50)
            
            for row in results:
                vendor = str(row.vendor)
                count = int(row.count)
                avg_cbm = float(row.avg_cbm) if row.avg_cbm else 0
                
                print(f"{vendor:<10} {count:<8} {avg_cbm:<12.2f}")
        
        return results
    
    def analyze_large_cargo(self, cbm_threshold=50):
        """ëŒ€í˜• í™”ë¬¼ ë¶„ì„"""
        print(f"\nğŸ“Š ëŒ€í˜• í™”ë¬¼ ë¶„ì„ (CBM > {cbm_threshold})")
        
        sparql_query = f"""
        SELECT ?event ?case ?cbm ?warehouse ?vendor WHERE {{
            ?event rdf:type ex:TransportEvent .
            ?event ex:hasCase ?case .
            ?event ex:hasCubicMeter ?cbm .
            ?event ex:hasWarehouse ?warehouse .
            ?event ex:hasHVDCCode3 ?vendor .
            FILTER(?cbm > {cbm_threshold})
        }} ORDER BY DESC(?cbm)
        """
        
        results = self.query(sparql_query)
        
        if results:
            print(f"\nğŸ“ˆ ëŒ€í˜• í™”ë¬¼ ë¶„ì„ ê²°ê³¼ (CBM > {cbm_threshold}):")
            print("-" * 80)
            print(f"{'ì¼€ì´ìŠ¤ ë²ˆí˜¸':<20} {'CBM':<8} {'ì°½ê³ ':<15} {'ë²¤ë”':<8}")
            print("-" * 80)
            
            for row in results:
                case = str(row.case)
                cbm = float(row.cbm)
                warehouse = str(row.warehouse)
                vendor = str(row.vendor)
                
                print(f"{case:<20} {cbm:<8.2f} {warehouse:<15} {vendor:<8}")
        
        return results
    
    def comprehensive_analysis(self):
        """ì¢…í•© ë¶„ì„"""
        print("\nğŸ” HVDC ë°ì´í„° ì¢…í•© ë¶„ì„")
        print("=" * 60)
        
        # ê¸°ë³¸ í†µê³„
        total_events = len(list(self.graph.subjects(RDF.type, EX.TransportEvent)))
        hitachi_events = len(list(self.graph.subjects(RDF.type, EX.HitachiCargo)))
        siemens_events = len(list(self.graph.subjects(RDF.type, EX.SiemensCargo)))
        
        print(f"ğŸ“Š ê¸°ë³¸ í†µê³„:")
        print(f"   ì „ì²´ ì´ë²¤íŠ¸: {total_events:,}ê°œ")
        print(f"   Hitachi í™”ë¬¼: {hitachi_events:,}ê°œ")
        print(f"   Siemens í™”ë¬¼: {siemens_events:,}ê°œ")
        
        # ì°½ê³ ë³„ CBM ë¶„ì„
        warehouse_results = self.analyze_warehouse_cbm()
        
        # ë²¤ë”ë³„ ë¶„í¬ ë¶„ì„
        vendor_results = self.analyze_vendor_distribution()
        
        # ëŒ€í˜• í™”ë¬¼ ë¶„ì„
        large_cargo_results = self.analyze_large_cargo(50)
        
        return {
            'total_events': total_events,
            'hitachi_events': hitachi_events,
            'siemens_events': siemens_events,
            'warehouse_analysis': warehouse_results,
            'vendor_analysis': vendor_results,
            'large_cargo_analysis': large_cargo_results
        }
    
    def save_rdf(self, output_file):
        """RDF íŒŒì¼ ì €ì¥"""
        try:
            self.graph.serialize(destination=output_file, format='turtle')
            print(f"âœ… RDF íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_file}")
            return True
        except Exception as e:
            print(f"âŒ RDF íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ HVDC RDF Analyzer ì‹œì‘")
    print("=" * 50)
    
    # RDF ë³€í™˜ê¸° ì´ˆê¸°í™”
    converter = HVDCRDFConverter()
    
    # ì‚¬ìš©ì ìš”ì²­ ì½”ë“œ êµ¬í˜„
    print("\nğŸ“ ì‚¬ìš©ì ìš”ì²­ ì½”ë“œ ì‹¤í–‰:")
    print("# Excel íŒŒì¼ â†’ RDF ë³€í™˜ â†’ SPARQL ë¶„ì„")
    
    try:
        # Excel íŒŒì¼ â†’ RDF ë³€í™˜
        rdf_graph = converter.convert_excel_to_rdf("HVDC WAREHOUSE_HITACHI(HE).xlsx")
        
        if rdf_graph:
            # SPARQL ì¿¼ë¦¬ ì‹¤í–‰
            print("\nğŸ” SPARQL ì¿¼ë¦¬ ì‹¤í–‰:")
            results = converter.query("""
                SELECT ?warehouse (AVG(?cbm) as ?avg_cbm) WHERE {
                    ?event ex:hasWarehouse ?warehouse .
                    ?event ex:hasCubicMeter ?cbm .
                } GROUP BY ?warehouse
            """)
            
            if results:
                print("\nğŸ“Š ì°½ê³ ë³„ í‰ê·  CBM ê²°ê³¼:")
                print("-" * 40)
                for row in results:
                    warehouse = str(row.warehouse)
                    avg_cbm = float(row.avg_cbm) if row.avg_cbm else 0
                    print(f"{warehouse}: {avg_cbm:.2f} CBM")
            
            # ì¶”ê°€ ë¶„ì„
            print("\nğŸ” ì¶”ê°€ ë¶„ì„ ìˆ˜í–‰:")
            analysis_results = converter.comprehensive_analysis()
            
            # RDF íŒŒì¼ ì €ì¥
            output_file = f"rdf_output/hvdc_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.ttl"
            Path(output_file).parent.mkdir(exist_ok=True)
            converter.save_rdf(output_file)
            
            print(f"\nâœ… ë¶„ì„ ì™„ë£Œ! RDF íŒŒì¼: {output_file}")
            
        else:
            print("âŒ RDF ë³€í™˜ ì‹¤íŒ¨")
            
    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        # SIMENSE íŒŒì¼ë¡œ ëŒ€ì²´ ì‹œë„
        try:
            print("\nğŸ”„ SIMENSE íŒŒì¼ë¡œ ì¬ì‹œë„...")
            rdf_graph = converter.convert_excel_to_rdf("HVDC WAREHOUSE_SIMENSE(SIM).xlsx")
            
            if rdf_graph:
                analysis_results = converter.comprehensive_analysis()
                print("âœ… SIMENSE íŒŒì¼ ë¶„ì„ ì™„ë£Œ")
                
        except Exception as e2:
            print(f"âŒ ì¬ì‹œë„ ì‹¤íŒ¨: {e2}")


if __name__ == "__main__":
    main() 