#!/usr/bin/env python3
"""
HVDC ì „ì²´ ë°ì´í„° ì˜¨í†¨ë¡œì§€ ë§¤í•‘ (8,000+í–‰)
/cmd_full_data_mapping ëª…ë ¹ì–´ êµ¬í˜„
"""

import pandas as pd
import json
from datetime import datetime
from pathlib import Path
import logging
import time
from tqdm import tqdm

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def load_full_data():
    """ì „ì²´ ì‹¤ì œ ë°ì´í„° íŒŒì¼ ë¡œë“œ"""
    print("ğŸš€ /cmd_full_data_mapping ì‹¤í–‰ ì¤‘...")
    print("ğŸ“Š ì „ì²´ ì‹¤ì œ ë°ì´í„° íŒŒì¼ ë¡œë“œ ì¤‘...")
    
    data_files = {
        'HITACHI': 'data/HVDC WAREHOUSE_HITACHI(HE).xlsx',
        'SIMENSE': 'data/HVDC WAREHOUSE_SIMENSE(SIM).xlsx',
        'INVOICE': 'data/HVDC WAREHOUSE_INVOICE.xlsx'
    }
    
    loaded_data = {}
    total_rows = 0
    
    for name, file_path in data_files.items():
        try:
            print(f"ğŸ“‹ {name} íŒŒì¼ ë¡œë“œ ì¤‘: {file_path}")
            start_time = time.time()
            df = pd.read_excel(file_path)
            load_time = time.time() - start_time
            
            loaded_data[name] = df
            total_rows += len(df)
            
            print(f"âœ… {name}: {len(df):,}í–‰ ë¡œë“œ ì™„ë£Œ ({load_time:.2f}ì´ˆ)")
            print(f"   ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {df.memory_usage(deep=True).sum() / 1024 / 1024:.2f}MB")
            
        except Exception as e:
            print(f"âŒ {name} íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            continue
    
    print(f"\nğŸ“ˆ ì „ì²´ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {total_rows:,}í–‰")
    return loaded_data

def load_mapping_rules():
    """ë§¤í•‘ ê·œì¹™ ë¡œë“œ"""
    try:
        with open('mapping_rules_v2.6.json', 'r', encoding='utf-8') as f:
            rules = json.load(f)
        print("âœ… ë§¤í•‘ ê·œì¹™ ë¡œë“œ ì„±ê³µ")
        return rules
    except FileNotFoundError:
        print("âŒ mapping_rules_v2.6.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None

def analyze_full_data_structure(loaded_data):
    """ì „ì²´ ë°ì´í„° êµ¬ì¡° ìƒì„¸ ë¶„ì„"""
    print("\nğŸ” ì „ì²´ ë°ì´í„° êµ¬ì¡° ìƒì„¸ ë¶„ì„")
    print("=" * 60)
    
    total_rows = 0
    total_cols = 0
    
    for name, df in loaded_data.items():
        print(f"\nğŸ“‹ {name} ë°ì´í„° ìƒì„¸ ë¶„ì„:")
        print(f"   ğŸ“Š í–‰ ìˆ˜: {len(df):,}")
        print(f"   ğŸ“Š ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}")
        print(f"   ğŸ“Š ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {df.memory_usage(deep=True).sum() / 1024 / 1024:.2f}MB")
        
        # ë°ì´í„° íƒ€ì… ë¶„ì„
        numeric_cols = df.select_dtypes(include=['number']).columns
        date_cols = df.select_dtypes(include=['datetime']).columns
        text_cols = df.select_dtypes(include=['object']).columns
        
        print(f"   ğŸ“Š ìˆ«ìí˜• ì»¬ëŸ¼: {len(numeric_cols)}ê°œ")
        print(f"   ğŸ“Š ë‚ ì§œí˜• ì»¬ëŸ¼: {len(date_cols)}ê°œ")
        print(f"   ğŸ“Š í…ìŠ¤íŠ¸í˜• ì»¬ëŸ¼: {len(text_cols)}ê°œ")
        
        # ê²°ì¸¡ê°’ ë¶„ì„
        missing_data = df.isnull().sum()
        missing_cols = missing_data[missing_data > 0]
        print(f"   ğŸ“Š ê²°ì¸¡ê°’ ìˆëŠ” ì»¬ëŸ¼: {len(missing_cols)}ê°œ")
        
        # ê³ ìœ ê°’ ë¶„ì„
        unique_counts = df.nunique()
        high_cardinality = unique_counts[unique_counts > 1000]
        print(f"   ğŸ“Š ê³ ìœ ê°’ 1000+ ì»¬ëŸ¼: {len(high_cardinality)}ê°œ")
        
        total_rows += len(df)
        total_cols += len(df.columns)
        
        print("-" * 40)
    
    print(f"\nğŸ“ˆ ì „ì²´ í†µê³„:")
    print(f"   ğŸ“Š ì´ í–‰ ìˆ˜: {total_rows:,}")
    print(f"   ğŸ“Š ì´ ì»¬ëŸ¼ ìˆ˜: {total_cols}")
    print(f"   ğŸ“Š í‰ê·  í–‰/íŒŒì¼: {total_rows/len(loaded_data):,.0f}")

def map_columns_to_ontology_full(df, rules, data_source):
    """ì „ì²´ ë°ì´í„°ìš© ì»¬ëŸ¼ ì˜¨í†¨ë¡œì§€ ë§¤í•‘"""
    if not rules:
        return {}
    
    field_map = rules.get('field_map', {})
    
    # ì»¬ëŸ¼ëª… ë§¤í•‘ (ì •í™• ë§¤ì¹­ + ìœ ì‚¬ ë§¤ì¹­)
    mapped_columns = {}
    unmapped_columns = []
    
    for col in df.columns:
        mapped = False
        
        # 1. ì •í™•í•œ ë§¤ì¹­
        if col in field_map:
            mapped_columns[col] = field_map[col]
            mapped = True
        else:
            # 2. ìœ ì‚¬ ë§¤ì¹­ (ëŒ€ì†Œë¬¸ì, ê³µë°±, íŠ¹ìˆ˜ë¬¸ì ë¬´ì‹œ)
            col_normalized = col.lower().replace(' ', '').replace('_', '').replace('(', '').replace(')', '').replace('.', '')
            
            for excel_col, rdf_prop in field_map.items():
                excel_normalized = excel_col.lower().replace(' ', '').replace('_', '').replace('(', '').replace(')', '').replace('.', '')
                
                if col_normalized == excel_normalized:
                    mapped_columns[col] = rdf_prop
                    mapped = True
                    break
                
                # 3. ë¶€ë¶„ ë§¤ì¹­ (í¬í•¨ ê´€ê³„)
                if col_normalized in excel_normalized or excel_normalized in col_normalized:
                    if len(col_normalized) > 3 and len(excel_normalized) > 3:  # ë„ˆë¬´ ì§§ì€ ë§¤ì¹­ ë°©ì§€
                        mapped_columns[col] = rdf_prop
                        mapped = True
                        break
        
        if not mapped:
            unmapped_columns.append(col)
    
    print(f"ğŸ“‹ {data_source} ì»¬ëŸ¼ ë§¤í•‘ ê²°ê³¼:")
    print(f"   âœ… ë§¤í•‘ ì„±ê³µ: {len(mapped_columns)}ê°œ")
    print(f"   âŒ ë§¤í•‘ ì‹¤íŒ¨: {len(unmapped_columns)}ê°œ")
    
    # ë§¤í•‘ëœ ì»¬ëŸ¼ í‘œì‹œ
    for original, mapped in mapped_columns.items():
        print(f"   {original} â†’ {mapped}")
    
    # ë§¤í•‘ë˜ì§€ ì•Šì€ ì¤‘ìš” ì»¬ëŸ¼ í‘œì‹œ
    if unmapped_columns:
        print(f"   ğŸ“ ë§¤í•‘ë˜ì§€ ì•Šì€ ì»¬ëŸ¼ (ì²˜ìŒ 10ê°œ):")
        for col in unmapped_columns[:10]:
            print(f"      â€¢ {col}")
    
    return mapped_columns

def convert_full_data_to_rdf(loaded_data, rules, batch_size=1000):
    """ì „ì²´ ë°ì´í„°ë¥¼ ë°°ì¹˜ ì²˜ë¦¬ë¡œ RDF ë³€í™˜"""
    print(f"\nğŸ”— ì „ì²´ ë°ì´í„°ë¥¼ RDFë¡œ ë³€í™˜ ì¤‘... (ë°°ì¹˜ í¬ê¸°: {batch_size})")
    
    if not rules:
        print("âŒ ë§¤í•‘ ê·œì¹™ì´ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    namespace = rules.get('namespace', 'http://samsung.com/project-logistics#')
    
    # TTL í—¤ë”
    ttl_content = f"""# HVDC Full Data Ontology RDF
# Generated: {datetime.now().isoformat()}
# Source: Complete HITACHI, SIMENSE, INVOICE Excel files
# Total Records: {sum(len(df) for df in loaded_data.values()):,}

@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .
@prefix owl: <http://www.w3.org/2002/07/owl#> .
@prefix ex: <{namespace}> .

# Ontology Declaration
ex: a owl:Ontology ;
    rdfs:label "HVDC Full Data Warehouse Ontology" ;
    rdfs:comment "Complete warehouse data from HITACHI ({len(loaded_data.get('HITACHI', pd.DataFrame())):,} records), SIMENSE ({len(loaded_data.get('SIMENSE', pd.DataFrame())):,} records), INVOICE ({len(loaded_data.get('INVOICE', pd.DataFrame())):,} records)" ;
    owl:versionInfo "2.6" ;
    owl:versionIRI <{namespace}v2.6> .

"""
    
    event_counter = 1
    total_events = sum(len(df) for df in loaded_data.values())
    
    # ê° ë°ì´í„°ì…‹ ì²˜ë¦¬
    for data_source, df in loaded_data.items():
        print(f"ğŸ”„ {data_source} ì „ì²´ ë°ì´í„° ì²˜ë¦¬ ì¤‘... ({len(df):,}í–‰)")
        
        # ì»¬ëŸ¼ ë§¤í•‘
        column_mapping = map_columns_to_ontology_full(df, rules, data_source)
        
        # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í™•ë³´
        num_batches = (len(df) + batch_size - 1) // batch_size
        
        with tqdm(total=len(df), desc=f"{data_source} ì²˜ë¦¬", unit="í–‰") as pbar:
            for batch_idx in range(num_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, len(df))
                batch_df = df.iloc[start_idx:end_idx]
                
                # ë°°ì¹˜ ë‚´ ê° í–‰ ì²˜ë¦¬
                for idx, row in batch_df.iterrows():
                    event_id = f"TransportEvent_{event_counter:06d}"
                    ttl_content += f"ex:{event_id} a ex:TransportEvent ;\n"
                    ttl_content += f"    ex:hasDataSource \"{data_source}\" ;\n"
                    ttl_content += f"    ex:hasRowIndex \"{idx}\"^^xsd:integer ;\n"
                    
                    # ë§¤í•‘ëœ ì»¬ëŸ¼ë“¤ ì²˜ë¦¬
                    for original_col, rdf_prop in column_mapping.items():
                        if original_col in row and pd.notna(row[original_col]):
                            value = row[original_col]
                            
                            # ë°ì´í„° íƒ€ì…ë³„ ì •í™•í•œ ì²˜ë¦¬
                            if rdf_prop in ['hasQuantity', 'hasPackageCount', 'has20FTContainer', 'has40FTContainer', 'has20FRContainer', 'has40FRContainer', 'hasContainerStuffing', 'hasContainerUnstuffing']:
                                try:
                                    int_value = int(float(str(value)))
                                    ttl_content += f'    ex:{rdf_prop} "{int_value}"^^xsd:integer ;\n'
                                except:
                                    continue
                            elif rdf_prop in ['hasAmount', 'hasTotalAmount', 'hasHandlingFee', 'hasCBM', 'hasWeight', 'hasHandlingIn', 'hasHandlingOut', 'hasUnstuffing', 'hasStuffing', 'hasForklift', 'hasCrane']:
                                try:
                                    float_value = float(str(value))
                                    ttl_content += f'    ex:{rdf_prop} "{float_value}"^^xsd:decimal ;\n'
                                except:
                                    continue
                            elif rdf_prop in ['hasDate', 'hasStartDate', 'hasFinishDate', 'hasOperationMonth']:
                                try:
                                    if isinstance(value, pd.Timestamp):
                                        date_str = value.strftime('%Y-%m-%d')
                                    elif pd.isna(pd.to_datetime(str(value), errors='coerce')):
                                        continue
                                    else:
                                        date_str = pd.to_datetime(str(value)).strftime('%Y-%m-%d')
                                    ttl_content += f'    ex:{rdf_prop} "{date_str}"^^xsd:date ;\n'
                                except:
                                    continue
                            else:
                                # ë¬¸ìì—´ ì²˜ë¦¬ (íŠ¹ìˆ˜ë¬¸ì ì´ìŠ¤ì¼€ì´í”„)
                                clean_value = str(value).replace('"', '\\"').replace('\n', ' ').replace('\r', ' ').strip()
                                if clean_value and clean_value != 'nan':
                                    # ë„ˆë¬´ ê¸´ ë¬¸ìì—´ ì œí•œ (1000ì)
                                    if len(clean_value) > 1000:
                                        clean_value = clean_value[:1000] + "..."
                                    ttl_content += f'    ex:{rdf_prop} "{clean_value}" ;\n'
                    
                    ttl_content = ttl_content.rstrip(';\n') + ' .\n\n'
                    event_counter += 1
                    pbar.update(1)
                
                # ë©”ëª¨ë¦¬ ê´€ë¦¬: ë§¤ 1000ê°œ ì´ë²¤íŠ¸ë§ˆë‹¤ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
                if event_counter % 1000 == 0:
                    import gc
                    gc.collect()
    
    # ì°½ê³  ë° ì‚¬ì´íŠ¸ ì¸ìŠ¤í„´ìŠ¤ ì¶”ê°€
    warehouse_classification = rules.get('warehouse_classification', {})
    ttl_content += "\n# Warehouse and Site Instances\n"
    
    for storage_type, warehouses in warehouse_classification.items():
        for warehouse in warehouses:
            warehouse_id = warehouse.replace(' ', '_').replace('(', '').replace(')', '').replace('-', '_')
            
            if storage_type == 'Indoor':
                class_type = 'IndoorWarehouse'
            elif storage_type == 'Outdoor':
                class_type = 'OutdoorWarehouse'
            elif storage_type == 'Site':
                class_type = 'Site'
            elif storage_type == 'dangerous_cargo':
                class_type = 'DangerousCargoWarehouse'
            else:
                class_type = 'Warehouse'
            
            ttl_content += f"""ex:{warehouse_id} a ex:{class_type} ;
    rdfs:label "{warehouse}" ;
    ex:hasStorageType "{storage_type}" ;
    ex:hasCapacity "unknown"^^xsd:string .

"""
    
    print(f"âœ… ì „ì²´ RDF ë³€í™˜ ì™„ë£Œ: {event_counter-1:,}ê°œ ì´ë²¤íŠ¸")
    return ttl_content

def generate_full_data_sparql(rules, loaded_data):
    """ì „ì²´ ë°ì´í„°ìš© ê³ ê¸‰ SPARQL ì¿¼ë¦¬ ìƒì„±"""
    print("ğŸ” ì „ì²´ ë°ì´í„°ìš© ê³ ê¸‰ SPARQL ì¿¼ë¦¬ ìƒì„± ì¤‘...")
    
    if not rules:
        return None
    
    namespace = rules.get('namespace', 'http://samsung.com/project-logistics#')
    total_records = sum(len(df) for df in loaded_data.values())
    
    queries = f"""# HVDC ì „ì²´ ë°ì´í„° ê³ ê¸‰ SPARQL ì¿¼ë¦¬ ëª¨ìŒ
# Generated: {datetime.now().isoformat()}
# Total Records: {total_records:,}

# 1. ì „ì²´ ë°ì´í„° í†µê³„ ìš”ì•½
PREFIX ex: <{namespace}>
SELECT 
    (COUNT(?event) AS ?totalEvents)
    (COUNT(DISTINCT ?dataSource) AS ?dataSources)
    (SUM(?amount) AS ?totalAmount)
    (AVG(?amount) AS ?avgAmount)
    (SUM(?qty) AS ?totalQuantity)
    (SUM(?cbm) AS ?totalCBM)
WHERE {{
    ?event rdf:type ex:TransportEvent .
    OPTIONAL {{ ?event ex:hasAmount ?amount }}
    OPTIONAL {{ ?event ex:hasQuantity ?qty }}
    OPTIONAL {{ ?event ex:hasCBM ?cbm }}
    OPTIONAL {{ ?event ex:hasDataSource ?dataSource }}
}}

# 2. ë°ì´í„° ì†ŒìŠ¤ë³„ ìƒì„¸ ë¶„ì„
PREFIX ex: <{namespace}>
SELECT ?dataSource 
    (COUNT(?event) AS ?eventCount) 
    (SUM(?amount) AS ?totalAmount)
    (AVG(?amount) AS ?avgAmount)
    (SUM(?cbm) AS ?totalCBM)
    (AVG(?cbm) AS ?avgCBM)
    (SUM(?weight) AS ?totalWeight)
WHERE {{
    ?event rdf:type ex:TransportEvent ;
           ex:hasDataSource ?dataSource .
    OPTIONAL {{ ?event ex:hasAmount ?amount }}
    OPTIONAL {{ ?event ex:hasCBM ?cbm }}
    OPTIONAL {{ ?event ex:hasWeight ?weight }}
}}
GROUP BY ?dataSource
ORDER BY DESC(?eventCount)

# 3. ì›”ë³„ íŠ¸ë Œë“œ ë¶„ì„ (Operation Month ê¸°ì¤€)
PREFIX ex: <{namespace}>
SELECT ?month ?dataSource 
    (COUNT(?event) AS ?eventCount)
    (SUM(?amount) AS ?totalAmount)
    (SUM(?handlingFee) AS ?totalHandlingFee)
WHERE {{
    ?event rdf:type ex:TransportEvent ;
           ex:hasDataSource ?dataSource ;
           ex:hasOperationMonth ?month .
    OPTIONAL {{ ?event ex:hasAmount ?amount }}
    OPTIONAL {{ ?event ex:hasHandlingFee ?handlingFee }}
}}
GROUP BY ?month ?dataSource
ORDER BY ?month ?dataSource

# 4. ì»¨í…Œì´ë„ˆ íƒ€ì…ë³„ ë¶„ì„
PREFIX ex: <{namespace}>
SELECT 
    (SUM(?container20) AS ?total20FT)
    (SUM(?container40) AS ?total40FT)
    (SUM(?container20FR) AS ?total20FR)
    (SUM(?container40FR) AS ?total40FR)
    (COUNT(?event) AS ?eventsWithContainers)
WHERE {{
    ?event rdf:type ex:TransportEvent .
    OPTIONAL {{ ?event ex:has20FTContainer ?container20 }}
    OPTIONAL {{ ?event ex:has40FTContainer ?container40 }}
    OPTIONAL {{ ?event ex:has20FRContainer ?container20FR }}
    OPTIONAL {{ ?event ex:has40FRContainer ?container40FR }}
    FILTER(BOUND(?container20) || BOUND(?container40) || BOUND(?container20FR) || BOUND(?container40FR))
}}

# 5. í•˜ì—­ ì‘ì—… ë¶„ì„
PREFIX ex: <{namespace}>
SELECT 
    (COUNT(?event) AS ?handlingEvents)
    (SUM(?handlingIn) AS ?totalHandlingIn)
    (SUM(?handlingOut) AS ?totalHandlingOut)
    (SUM(?unstuffing) AS ?totalUnstuffing)
    (SUM(?stuffing) AS ?totalStuffing)
    (SUM(?forklift) AS ?totalForklift)
    (SUM(?crane) AS ?totalCrane)
WHERE {{
    ?event rdf:type ex:TransportEvent .
    OPTIONAL {{ ?event ex:hasHandlingIn ?handlingIn }}
    OPTIONAL {{ ?event ex:hasHandlingOut ?handlingOut }}
    OPTIONAL {{ ?event ex:hasUnstuffing ?unstuffing }}
    OPTIONAL {{ ?event ex:hasStuffing ?stuffing }}
    OPTIONAL {{ ?event ex:hasForklift ?forklift }}
    OPTIONAL {{ ?event ex:hasCrane ?crane }}
}}

# 6. ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„ (INVOICE ë°ì´í„°)
PREFIX ex: <{namespace}>
SELECT ?category 
    (COUNT(?event) AS ?eventCount)
    (SUM(?amount) AS ?totalAmount)
    (AVG(?amount) AS ?avgAmount)
WHERE {{
    ?event rdf:type ex:TransportEvent ;
           ex:hasCategory ?category ;
           ex:hasAmount ?amount .
}}
GROUP BY ?category
ORDER BY DESC(?totalAmount)

# 7. CBM ë° ë¬´ê²Œ ê¸°ì¤€ ëŒ€í˜• í™”ë¬¼ ë¶„ì„
PREFIX ex: <{namespace}>
SELECT ?dataSource
    (COUNT(?event) AS ?largeCargoCount)
    (SUM(?cbm) AS ?totalCBM)
    (AVG(?cbm) AS ?avgCBM)
    (SUM(?weight) AS ?totalWeight)
    (AVG(?weight) AS ?avgWeight)
WHERE {{
    ?event rdf:type ex:TransportEvent ;
           ex:hasDataSource ?dataSource ;
           ex:hasCBM ?cbm .
    OPTIONAL {{ ?event ex:hasWeight ?weight }}
    FILTER(?cbm > 10.0)  # 10 CBM ì´ìƒ ëŒ€í˜• í™”ë¬¼
}}
GROUP BY ?dataSource
ORDER BY DESC(?totalCBM)

# 8. íŒ¨í‚¤ì§€ ìˆ˜ëŸ‰ ë¶„ì„
PREFIX ex: <{namespace}>
SELECT ?dataSource
    (COUNT(?event) AS ?eventCount)
    (SUM(?packages) AS ?totalPackages)
    (AVG(?packages) AS ?avgPackages)
    (MAX(?packages) AS ?maxPackages)
WHERE {{
    ?event rdf:type ex:TransportEvent ;
           ex:hasDataSource ?dataSource ;
           ex:hasPackageCount ?packages .
}}
GROUP BY ?dataSource
ORDER BY DESC(?totalPackages)

# 9. ë°ì´í„° í’ˆì§ˆ ë¶„ì„ (ê²°ì¸¡ê°’ ì²´í¬)
PREFIX ex: <{namespace}>
SELECT ?dataSource
    (COUNT(?event) AS ?totalEvents)
    (COUNT(?amount) AS ?eventsWithAmount)
    (COUNT(?cbm) AS ?eventsWithCBM)
    (COUNT(?weight) AS ?eventsWithWeight)
    (COUNT(?packages) AS ?eventsWithPackages)
WHERE {{
    ?event rdf:type ex:TransportEvent ;
           ex:hasDataSource ?dataSource .
    OPTIONAL {{ ?event ex:hasAmount ?amount }}
    OPTIONAL {{ ?event ex:hasCBM ?cbm }}
    OPTIONAL {{ ?event ex:hasWeight ?weight }}
    OPTIONAL {{ ?event ex:hasPackageCount ?packages }}
}}
GROUP BY ?dataSource
ORDER BY ?dataSource

# 10. ë³µí•© ë¶„ì„: ì›”ë³„ + ì¹´í…Œê³ ë¦¬ë³„ + ë°ì´í„°ì†ŒìŠ¤ë³„
PREFIX ex: <{namespace}>
SELECT ?month ?category ?dataSource
    (COUNT(?event) AS ?eventCount)
    (SUM(?amount) AS ?totalAmount)
    (SUM(?handlingFee) AS ?totalHandlingFee)
WHERE {{
    ?event rdf:type ex:TransportEvent ;
           ex:hasDataSource ?dataSource .
    OPTIONAL {{ ?event ex:hasOperationMonth ?month }}
    OPTIONAL {{ ?event ex:hasCategory ?category }}
    OPTIONAL {{ ?event ex:hasAmount ?amount }}
    OPTIONAL {{ ?event ex:hasHandlingFee ?handlingFee }}
    FILTER(BOUND(?month) && BOUND(?category))
}}
GROUP BY ?month ?category ?dataSource
ORDER BY ?month ?category ?dataSource
"""
    
    return queries

def save_full_data_outputs(ttl_content, sparql_queries, loaded_data):
    """ì „ì²´ ë°ì´í„° ê²°ê³¼ ì €ì¥"""
    print("\nğŸ’¾ ì „ì²´ ë°ì´í„° ê²°ê³¼ ì €ì¥ ì¤‘...")
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir = Path('rdf_output')
    output_dir.mkdir(exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    total_records = sum(len(df) for df in loaded_data.values())
    
    files_created = []
    
    # TTL íŒŒì¼ ì €ì¥ (ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²˜ë¦¬)
    if ttl_content:
        ttl_file = output_dir / f'hvdc_full_data_{total_records}records_{timestamp}.ttl'
        
        print(f"ğŸ“ ëŒ€ìš©ëŸ‰ TTL íŒŒì¼ ì €ì¥ ì¤‘... (ì˜ˆìƒ í¬ê¸°: {len(ttl_content) / 1024 / 1024:.1f}MB)")
        
        with open(ttl_file, 'w', encoding='utf-8') as f:
            f.write(ttl_content)
        
        file_size_mb = ttl_file.stat().st_size / 1024 / 1024
        print(f"âœ… ì „ì²´ ë°ì´í„° RDF/TTL ì €ì¥: {ttl_file}")
        print(f"   ğŸ“Š íŒŒì¼ í¬ê¸°: {file_size_mb:.2f}MB")
        files_created.append(('RDF/TTL', ttl_file, f"{file_size_mb:.2f}MB"))
    
    # SPARQL ì¿¼ë¦¬ ì €ì¥
    if sparql_queries:
        sparql_file = output_dir / f'hvdc_full_queries_{total_records}records_{timestamp}.sparql'
        with open(sparql_file, 'w', encoding='utf-8') as f:
            f.write(sparql_queries)
        print(f"âœ… ì „ì²´ ë°ì´í„° SPARQL ì¿¼ë¦¬ ì €ì¥: {sparql_file}")
        files_created.append(('SPARQL', sparql_file, "ê³ ê¸‰ 10ê°œ ì¿¼ë¦¬"))
    
    # í†µê³„ ìš”ì•½ íŒŒì¼ ìƒì„±
    stats_file = output_dir / f'hvdc_full_stats_{timestamp}.md'
    stats_content = f"""# HVDC ì „ì²´ ë°ì´í„° ì˜¨í†¨ë¡œì§€ ë§¤í•‘ í†µê³„

## ğŸ“Š ì²˜ë¦¬ í†µê³„
- **ì²˜ë¦¬ ì¼ì‹œ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **ì´ ë ˆì½”ë“œ ìˆ˜**: {total_records:,}ê°œ
- **ë°ì´í„° ì†ŒìŠ¤**: {len(loaded_data)}ê°œ

## ğŸ“‹ ë°ì´í„° ì†ŒìŠ¤ë³„ ìƒì„¸
"""
    
    for name, df in loaded_data.items():
        stats_content += f"""
### {name}
- **ë ˆì½”ë“œ ìˆ˜**: {len(df):,}ê°œ
- **ì»¬ëŸ¼ ìˆ˜**: {len(df.columns)}ê°œ
- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: {df.memory_usage(deep=True).sum() / 1024 / 1024:.2f}MB
"""
    
    stats_content += f"""
## ğŸ“ ìƒì„±ëœ íŒŒì¼
"""
    
    for file_type, file_path, size_info in files_created:
        stats_content += f"- **{file_type}**: `{file_path.name}` ({size_info})\n"
    
    with open(stats_file, 'w', encoding='utf-8') as f:
        f.write(stats_content)
    
    files_created.append(('í†µê³„', stats_file, "ìƒì„¸ í†µê³„"))
    
    return files_created

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ /cmd_full_data_mapping ì‹¤í–‰")
    print("=" * 70)
    print("ğŸ“ˆ HVDC ì „ì²´ ë°ì´í„° ì˜¨í†¨ë¡œì§€ ë§¤í•‘ (8,000+í–‰)")
    print("=" * 70)
    
    start_time = time.time()
    
    # 1ë‹¨ê³„: ì „ì²´ ë°ì´í„° ë¡œë“œ
    loaded_data = load_full_data()
    
    if not loaded_data:
        print("âŒ ë°ì´í„° íŒŒì¼ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # 2ë‹¨ê³„: ë°ì´í„° êµ¬ì¡° ë¶„ì„
    analyze_full_data_structure(loaded_data)
    
    # 3ë‹¨ê³„: ë§¤í•‘ ê·œì¹™ ë¡œë“œ
    rules = load_mapping_rules()
    
    # 4ë‹¨ê³„: ì „ì²´ ë°ì´í„° RDF ë³€í™˜
    ttl_content = convert_full_data_to_rdf(loaded_data, rules, batch_size=500)
    
    # 5ë‹¨ê³„: ê³ ê¸‰ SPARQL ì¿¼ë¦¬ ìƒì„±
    sparql_queries = generate_full_data_sparql(rules, loaded_data)
    
    # 6ë‹¨ê³„: ê²°ê³¼ ì €ì¥
    if ttl_content and sparql_queries:
        files_created = save_full_data_outputs(ttl_content, sparql_queries, loaded_data)
        
        total_time = time.time() - start_time
        total_records = sum(len(df) for df in loaded_data.values())
        
        print("\nğŸ‰ ì „ì²´ ë°ì´í„° ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ì™„ë£Œ!")
        print("=" * 70)
        print("ğŸ“Š ìµœì¢… í†µê³„:")
        print(f"   â€¢ ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ")
        print(f"   â€¢ ì´ ë ˆì½”ë“œ ìˆ˜: {total_records:,}ê°œ")
        print(f"   â€¢ ì²˜ë¦¬ ì†ë„: {total_records/total_time:.0f}ê°œ/ì´ˆ")
        
        for name, df in loaded_data.items():
            print(f"   â€¢ {name}: {len(df):,}í–‰ â†’ ì™„ì „ ë³€í™˜")
        
        print("\nğŸ“ ìƒì„±ëœ íŒŒì¼:")
        for file_type, file_path, size_info in files_created:
            print(f"   â€¢ {file_type}: {file_path.name} ({size_info})")
        
        print("\nğŸ”§ ì¶”ì²œ ëª…ë ¹ì–´:")
        print("   /cmd_ontology_query [ì „ì²´ ë°ì´í„° ì¿¼ë¦¬ ì‹¤í–‰]")
        print("   /cmd_data_validation [ë°ì´í„° í’ˆì§ˆ ê²€ì¦]")
        print("   /cmd_warehouse_analysis [ì°½ê³  ë¶„ì„]")
        print("   /cmd_performance_analysis [ì„±ëŠ¥ ë¶„ì„]")
        print("   /cmd_export_excel [Excel ë¦¬í¬íŠ¸ ìƒì„±]")
    else:
        print("\nâŒ ì „ì²´ ë°ì´í„° ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ì‹¤íŒ¨")

if __name__ == "__main__":
    main() 